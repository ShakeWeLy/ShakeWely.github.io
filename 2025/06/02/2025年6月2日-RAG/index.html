<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>2025年6月2日 RAG | Weakliy_Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="https:&#x2F;&#x2F;github.com&#x2F;infiniflow&#x2F;ragflow   llamaindex  安装 1234git clone https:&#x2F;&#x2F;github.com&#x2F;run-llama&#x2F;llama_index.gitpip intall poetrypip install -e llama-index-corepip install -e . ollama本地 1pip install">
<meta property="og:type" content="article">
<meta property="og:title" content="2025年6月2日 RAG">
<meta property="og:url" content="https://shakewely.github.io/2025/06/02/2025%E5%B9%B46%E6%9C%882%E6%97%A5-RAG/index.html">
<meta property="og:site_name" content="Weakliy_Blog">
<meta property="og:description" content="https:&#x2F;&#x2F;github.com&#x2F;infiniflow&#x2F;ragflow   llamaindex  安装 1234git clone https:&#x2F;&#x2F;github.com&#x2F;run-llama&#x2F;llama_index.gitpip intall poetrypip install -e llama-index-corepip install -e . ollama本地 1pip install">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-06-02T00:57:10.000Z">
<meta property="article:modified_time" content="2025-06-04T03:52:21.611Z">
<meta property="article:author" content="Weakliy">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Weakliy_Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

  
<link rel="stylesheet" href="/plugin/bganimation/bg.css">

  

  <link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css" rel="stylesheet" type="text/css">
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <div class="outer">
        <div class="widget-wrap mobile-header">
  <h3 class="widget-title"></h3>
  <div class="widget">
    <img class="avatar" src="https://raw.githubusercontent.com/ShakeWeLy/Weakliy.github.io/main/%E5%A4%B4%E5%83%8F/mmexport1683194148817.png">
    <h2 class="author">Weakliy</h2>
    <h3 class="description"></h3>
    <div class="count-box">
      <a href="/archives"><div><strong>118</strong><br>文章</div></a>
      <a href="/categories"><div><strong>0</strong><br>分类</div></a>
      <a href="/tags"><div><strong>34</strong><br>标签</div></a>
    </div>
    <ul class="blog-link">
     
          <a href="/" title="Home">
            <li>主页</li>
          </a>
        
          <a href="/archives" title="Archives">
            <li>归档</li>
          </a>
        
          <a href="/categories" title="Categories">
            <li>分类</li>
          </a>
        
          <a href="/tags" title="Tags">
            <li>标签</li>
          </a>
        
    </ul>
  </div>
</div>

        <section id="main"><article id="post-2025年6月2日-RAG" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2025/06/02/2025%E5%B9%B46%E6%9C%882%E6%97%A5-RAG/" class="article-date">
  <time class="post-time" datetime="2025-06-02T00:57:10.000Z" itemprop="datePublished">
    <span class="post-month">6月</span><br/>
    <span class="post-day">02</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      2025年6月2日 RAG
    </h1>
  

        <div>
          
          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <blockquote>
<p><a target="_blank" rel="noopener" href="https://github.com/infiniflow/ragflow">https://github.com/infiniflow/ragflow</a></p>
</blockquote>
<h1 id="llamaindex"><a class="markdownIt-Anchor" href="#llamaindex"></a> llamaindex</h1>
<h3 id="安装"><a class="markdownIt-Anchor" href="#安装"></a> 安装</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/run-llama/llama_index.git</span><br><span class="line">pip intall poetry</span><br><span class="line">pip install -e llama-index-core</span><br><span class="line">pip install -e .</span><br></pre></td></tr></table></figure>
<p><strong>ollama</strong>本地</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install llama-index-llms-ollama llama-index-embeddings-huggingface</span><br></pre></td></tr></table></figure>
<h1 id="agentic-applications"><a class="markdownIt-Anchor" href="#agentic-applications"></a> Agentic Applications</h1>
<p>当 LLM 在应用程序中使用时，它通常用于做出决策、采取行动和/或与世界交互。这是<strong>代理应用程序</strong>的核心定义。</p>
<p>虽然代理应用程序的定义很宽泛，但定义代理应用程序的几个关键特征是：</p>
<ul>
<li><strong>LLM 增强</strong>：LLM 通过工具（即代码中的任意可调用函数）、内存和/或动态提示进行增强。</li>
<li><strong>Prompt Chaining</strong>：使用多个相互构建的 LLM 调用，一个 LLM 调用的输出用作下一个 LLM 调用的输入。</li>
<li><strong>路由</strong>：LLM 用于将应用程序路由到应用程序中的下一个适当步骤或状态。</li>
<li><strong>并行度</strong>：应用程序可以并行执行多个步骤或作。</li>
<li><strong>编排</strong>：LLM 的层次结构用于编排较低级别的作和 LLM。</li>
<li><strong>反射</strong>：LLM 用于反射和验证先前步骤或 LLM 调用的输出，可用于指导应用程序进入下一个适当的步骤或状态。</li>
</ul>
<h1 id="rag"><a class="markdownIt-Anchor" href="#rag"></a> RAG</h1>
<h2 id="1-介绍"><a class="markdownIt-Anchor" href="#1-介绍"></a> 1 介绍</h2>
<blockquote>
<h4 id="是什么rag"><a class="markdownIt-Anchor" href="#是什么rag"></a> 是什么RAG:</h4>
<p><strong>引入外部知识库</strong>，使模型在回答问题前先去“查资料”</p>
</blockquote>
<blockquote>
<h4 id="rag-的流程一般如下"><a class="markdownIt-Anchor" href="#rag-的流程一般如下"></a> RAG 的流程一般如下：</h4>
<ol>
<li><strong>用户提问</strong><br>
👉 输入一个自然语言问题，如：“介绍一下 RAG 是怎么工作的？”</li>
<li><strong>检索（Retrieval）</strong><br>
👉 使用向量搜索引擎（如 FAISS、Weaviate、Milvus）从外部知识库中检索相关文档段落。</li>
<li><strong>增强上下文（Augmentation）</strong><br>
👉 将这些检索到的文档作为“额外上下文”拼接到用户的问题后面。</li>
<li><strong>生成（Generation）</strong><br>
👉 使用语言模型（如 GPT、LLaMA）基于这个增强上下文进行回答。</li>
<li><strong>输出答案</strong><br>
👉 返回包含事实依据的自然语言回答。</li>
</ol>
</blockquote>
<blockquote>
<h4 id="技术组件工具链"><a class="markdownIt-Anchor" href="#技术组件工具链"></a> 🔧 技术组件（工具链）:</h4>
<ul>
<li><strong>Embedding 模型</strong>：将文本转为向量（如 <code>text-embedding-3-small</code>）</li>
<li><strong>向量数据库</strong>：存储与检索文档（如 FAISS、Chroma、Weaviate）</li>
<li><strong>语言模型</strong>：生成答案（如 OpenAI GPT, Mistral, Claude）</li>
<li><strong>框架支持</strong>：LangChain、LlamaIndex、Haystack 等</li>
</ul>
</blockquote>
<h2 id="2-rag_test"><a class="markdownIt-Anchor" href="#2-rag_test"></a> 2 RAG_Test</h2>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">处理文档时出错: No files found in f:\.Work\test_llm\data</span><br><span class="line">使用这个.&quot;F:\data\paul_graham_essay.txt&quot;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>(llama_new) PS F:.Work\test_llm\llama_index&gt; &amp; D:/Pydata/anacondai/envs/llama_new/python.exe f:/.Work/test_llm/test_ollama_rag.py<br>
INFO:<strong>main</strong>:数据文件路径: F:\data\paul_graham_essay.txt<br>
INFO:<strong>main</strong>:数据目录: F:\data<br>
INFO:<strong>main</strong>:找到数据文件: F:\data\paul_graham_essay.txt<br>
INFO:<strong>main</strong>:数据目录中的文本文件列表: [‘F:\data\paul_graham_essay.txt’]<br>
INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: BAAI/bge-base-en-v1.5<br>
INFO:sentence_transformers.SentenceTransformer:2 prompts are loaded, with the keys: [‘query’, ‘text’]<br>
INFO:<strong>main</strong>:开始加载文档…<br>
INFO:<strong>main</strong>:成功加载 1 个文档<br>
INFO:<strong>main</strong>:开始创建索引…<br>
INFO:<strong>main</strong>:索引创建完成<br>
INFO:<strong>main</strong>:查询引擎创建完成<br>
INFO:<strong>main</strong>:开始运行查询…<br>
INFO:httpx:HTTP Request: POST <a target="_blank" rel="noopener" href="http://localhost:11434/api/chat">http://localhost:11434/api/chat</a> “HTTP/1.1 200 OK”<br>
INFO:httpx:HTTP Request: POST <a target="_blank" rel="noopener" href="http://localhost:11434/api/chat">http://localhost:11434/api/chat</a> “HTTP/1.1 200 OK”<br>
INFO:httpx:HTTP Request: POST <a target="_blank" rel="noopener" href="http://localhost:11434/api/chat">http://localhost:11434/api/chat</a> “HTTP/1.1 200 OK”</p>
<p>查询结果:<br>
The output from the tool calls will be used to format an answer to the original user question.</p>
<p>The author took classes in fundamental subjects like drawing, color, and design as part of a foundation program. They were counted as a transfer sophomore at RISD.</p>
<p>The result of 7 * 8 is 56.<br>
INFO:<strong>main</strong>:查询完成</p>
</blockquote>
<h1 id="reader"><a class="markdownIt-Anchor" href="#reader"></a> READER</h1>
<h2 id="1-介绍-2"><a class="markdownIt-Anchor" href="#1-介绍-2"></a> 1 介绍</h2>
<blockquote>
<h4 id="什么是-llamaindex-readers"><a class="markdownIt-Anchor" href="#什么是-llamaindex-readers"></a> 什么是 LlamaIndex Readers:</h4>
<p><strong>LlamaIndex Readers</strong> 是 LlamaIndex 框架中的数据加载模块，用于从各种数据源（如网页、文件、数据库、API 等）提取和加载数据，转化为可用于索引和检索的格式。它们是 LlamaIndex 数据管道的核心组件，支持 <strong>RAG（Retrieval-Augmented Generation）</strong> 系统通过外部数据增强语言模型的能力。</p>
</blockquote>
<blockquote>
<h4 id="llamaindex-readers-的流程一般如下"><a class="markdownIt-Anchor" href="#llamaindex-readers-的流程一般如下"></a> LlamaIndex Readers 的流程一般如下：</h4>
<ol>
<li><strong>数据源选择</strong> 👉 用户指定数据源（如 PDF 文件、网页、数据库、社交媒体等）。</li>
<li><strong>数据加载</strong> 👉 使用特定的 Reader（如 PDFReader、SimpleWebPageReader）从数据源提取原始内容（如文本、表格、元数据）。</li>
<li><strong>数据解析</strong> 👉 将提取的内容解析为结构化或非结构化数据，通常以 Document 对象的形式输出，包含文本和元数据。</li>
<li><strong>索引准备</strong> 👉 加载的数据可直接用于 LlamaIndex 的索引模块（如 VectorStoreIndex），将文本转为向量存储到向量数据库。</li>
<li><strong>检索与生成</strong> 👉 在 RAG 流程中，加载的数据作为知识库内容，供检索和生成答案使用。</li>
</ol>
</blockquote>
<blockquote>
<h4 id="技术组件工具链-2"><a class="markdownIt-Anchor" href="#技术组件工具链-2"></a> 🔧 技术组件（工具链）:</h4>
<ul>
<li><strong>数据源支持</strong>：支持多种数据源，包括文件（PDF、CSV、DOCX）、网页（HTML、RSS）、数据库（MongoDB、Snowflake）、社交媒体（Twitter、Slack）、云服务（Google Drive、S3）等。</li>
<li><strong>核心库</strong>：依赖 unstructured、BeautifulSoup、PyMuPDF 等库来解析不同格式的数据。</li>
<li><strong>集成框架</strong>：与 LlamaIndex 的索引和查询模块无缝集成，支持向量数据库（如 FAISS、Weaviate、Chroma）以及语言模型（OpenAI GPT、HuggingFace 模型等）。</li>
<li><strong>扩展性</strong>：提供大量社区贡献的 Reader（如 WikipediaReader、YoutubeTranscriptReader），支持特定场景的数据加载。</li>
</ul>
</blockquote>
<h2 id="2-reader_use"><a class="markdownIt-Anchor" href="#2-reader_use"></a> 2 Reader_use</h2>
<blockquote>
<ol>
<li>文件加载器
<ul>
<li><strong>PDFReader</strong> (ravi03071991): 解析 PDF 文件，提取文本和元数据，支持简单文本和复杂表格。</li>
<li><strong>DocxReader</strong> (thejessezhang, Verified): 解析 Microsoft Word (.docx) 文件，提取格式化文本。</li>
<li><strong>CSVReader</strong> (llama-index): 加载 CSV 文件，适合结构化数据处理。</li>
<li><strong>PyMuPDFReader</strong> (iamarunbrahma): 使用 PyMuPDF 高效解析 PDF，适合大规模文档处理。</li>
<li><strong>SmartPDFLoader</strong> (ansukla): 智能解析 PDF，提取复杂结构如表格和图像。</li>
</ul>
</li>
<li>网页加载器
<ul>
<li><strong>SimpleWebPageReader</strong> (thejessezhang, Verified): 加载网页内容，适合简单 HTML 页面。</li>
<li><strong>BeautifulSoupWebReader</strong> (thejessezhang, Verified): 使用 BeautifulSoup 解析网页，提取结构化内容。</li>
<li><strong>FireCrawlWebReader</strong> (llama-index): 通过 FireCrawl 工具高效抓取动态网页。</li>
<li><strong>TrafilaturaWebReader</strong> (na): 使用 Trafilatura 提取网页核心内容，去除无关元素。</li>
</ul>
</li>
<li>社交媒体与通信加载器
<ul>
<li><strong>TwitterTweetReader</strong> (ravi03071991): 加载 Twitter/X 推文及元数据。</li>
<li><strong>SlackReader</strong> (jerryjliu, Verified): 从 Slack 频道提取消息和文件。</li>
<li><strong>YoutubeTranscriptReader</strong> (ravi03071991): 提取 YouTube 视频字幕或转录内容。</li>
<li><strong>GmailReader</strong> (llama-index): 加载 Gmail 邮件内容及附件。</li>
</ul>
</li>
<li>数据库加载器
<ul>
<li><strong>SimpleMongoReader</strong> (jerryjliu, Verified): 从 MongoDB 数据库加载数据。</li>
<li><strong>QdrantReader</strong> (kacperlukawski): 从 Qdrant 向量数据库加载向量数据。</li>
<li><strong>DatabaseReader</strong> (kevinqz): 通用 SQL 数据库加载器。</li>
</ul>
</li>
<li>学术与研究加载器
<ul>
<li><strong>WikipediaReader</strong> (jerryjliu, Verified): 从维基百科加载文章内容。</li>
<li><strong>ArxivReader</strong> (thejessezhang, Verified): 从 arXiv 加载学术论文。</li>
<li><strong>SemanticScholarReader</strong> (shauryr): 从 Semantic Scholar 提取研究数据。</li>
</ul>
</li>
<li>云服务与生产力工具加载器
<ul>
<li><strong>NotionPageReader</strong> (jerryjliu, Verified): 从 Notion 页面和数据库加载内容。</li>
<li><strong>GoogleDocsReader</strong> (jerryjliu, Verified): 加载 Google Docs 文档。</li>
<li><strong>S3Reader</strong> (thejessezhang, Verified): 从 AWS S3 存储桶加载文件。</li>
</ul>
</li>
</ol>
</blockquote>
<blockquote>
<p>config_sentence_transformers.json: 100%|█████████████████████████████████████████████████████████████████████████████████████| 124/124 [00:00&lt;00:00, 18.8kB/s]<br>
<a target="_blank" rel="noopener" href="http://README.md">README.md</a>: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 30.4k/30.4k [00:00&lt;00:00, 2.40MB/s]<br>
sentence_bert_config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 52.0/52.0 [00:00&lt;00:00, 11.3kB/s]<br>
config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.00k/1.00k [00:00&lt;00:00, 332kB/s]<br>
pytorch_model.bin: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1.30G/1.30G [00:16&lt;00:00, 77.5MB/s]<br>
tokenizer_config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 394/394 [00:00&lt;00:00, 69.1kB/s]<br>
vocab.txt: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 110k/110k [00:00&lt;00:00, 5.40MB/s]<br>
tokenizer.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 439k/439k [00:00&lt;00:00, 1.05MB/s]<br>
special_tokens_map.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:00&lt;00:00, 27.4kB/s]<br>
config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 191/191 [00:00&lt;00:00, 39.4kB/s]<br>
2025-06-04 08:28:35,589 - INFO - 2 prompts are loaded, with the keys: [‘query’, ‘text’]<br>
2025-06-04 08:28:35,593 - INFO - 开始加载文档…</p>
</blockquote>
<h1 id="embeddings"><a class="markdownIt-Anchor" href="#embeddings"></a> Embeddings</h1>
<h2 id="1-介绍-3"><a class="markdownIt-Anchor" href="#1-介绍-3"></a> 1 介绍</h2>
<blockquote>
<h4 id="什么是-llamaindex-embeddings"><a class="markdownIt-Anchor" href="#什么是-llamaindex-embeddings"></a> 什么是 LlamaIndex Embeddings:</h4>
<p><strong>LlamaIndex Embeddings</strong> 是 LlamaIndex 框架中的嵌入模型模块，用于将文本、图像或其他数据转换为高维向量表示（embeddings），以支持语义搜索、检索增强生成（RAG）等应用。这些嵌入模型将输入数据映射到向量空间，便于存储到向量数据库（如 FAISS、Weaviate）并进行相似性检索。</p>
</blockquote>
<blockquote>
<h4 id="llamaindex-embeddings-的流程一般如下"><a class="markdownIt-Anchor" href="#llamaindex-embeddings-的流程一般如下"></a> LlamaIndex Embeddings 的流程一般如下：</h4>
<ol>
<li><strong>输入数据</strong> 👉 用户提供文本、图像或其他数据（如文档、查询）。</li>
<li><strong>嵌入生成</strong> 👉 使用嵌入模型（如 OpenAIEmbedding、HuggingFaceEmbedding）将数据转换为固定维度的向量。</li>
<li><strong>存储向量</strong> 👉 将生成的向量存储到向量数据库，供后续检索使用。</li>
<li><strong>语义检索</strong> 👉 在 RAG 流程中，基于查询生成嵌入向量，与存储的向量进行相似性匹配，检索相关内容。</li>
<li><strong>生成答案</strong> 👉 结合检索到的内容，使用语言模型生成最终答案。</li>
</ol>
</blockquote>
<blockquote>
<h4 id="技术组件工具链-3"><a class="markdownIt-Anchor" href="#技术组件工具链-3"></a> 🔧 技术组件（工具链）:</h4>
<ul>
<li><strong>嵌入模型</strong>：支持多种嵌入模型，包括开源模型（如 HuggingFace、Ollama）、云服务（如 OpenAI、AWS Bedrock、Google）、专用模型（如 CLIP、MistralAI）。</li>
<li><strong>向量数据库</strong>：与 FAISS、Chroma、Weaviate 等向量存储集成，用于高效存储和检索嵌入向量。</li>
<li><strong>框架支持</strong>：无缝集成 LlamaIndex 的索引和查询模块，适用于 RAG、语义搜索和知识库构建。</li>
<li><strong>扩展性</strong>：支持多种云服务和本地模型，覆盖广泛的部署场景。</li>
</ul>
</blockquote>
<h2 id="2-主要-embeddings-分类与功能"><a class="markdownIt-Anchor" href="#2-主要-embeddings-分类与功能"></a> 2 <strong>主要 Embeddings 分类与功能</strong></h2>
<blockquote>
<p>以下是一些典型 LlamaIndex Embeddings 的介绍（基于您提供的列表）：</p>
<ol>
<li>
<p>云服务嵌入模型</p>
<ul>
<li><strong>BedrockEmbedding</strong> (llama-index): 使用 AWS Bedrock 平台的嵌入模型，支持多种预训练模型。<br>
下载量：309,490 | 1 个月前更新</li>
<li><strong>OpenAIEmbedding</strong> (llama-index): 基于 OpenAI 的嵌入模型（如 text-embedding-3-small），适合高质量文本嵌入。<br>
下载量：0 | 1 个月前更新</li>
</ul>
</li>
<li>
<p>开源与本地嵌入模型</p>
<ul>
<li><strong>HuggingFaceEmbedding</strong> (llama-index): 使用 HuggingFace 模型（如 sentence-transformers）生成嵌入，支持本地运行。<br>
下载量：0 | 25 天前更新</li>
<li><strong>OllamaEmbedding</strong> (llama-index): 基于 Ollama 框架的本地嵌入模型，适合隐私敏感场景。<br>
下载量：0 | 1 个月前更新</li>
<li><strong>ClipEmbedding</strong> (llama-index): 使用 CLIP 模型生成文本和图像的多模态嵌入。<br>
下载量：8,031 | 1 个月前更新</li>
<li><strong>FastEmbedEmbedding</strong> (llama-index): 轻量级嵌入模型，优化速度和资源占用。<br>
下载量：0 | 22 天前更新</li>
</ul>
</li>
<li>
<p><strong>适配器与增强嵌入模型</strong></p>
<ul>
<li>
<p><strong>AdapterEmbeddingModel</strong> (llama-index): 通用适配器模型，允许在预训练嵌入模型上添加微调层。<br>
下载量：8,925 | 1 个月前更新</p>
</li>
<li>
<p><strong>LinearAdapterEmbeddingModel</strong> (llama-index): 使用线性适配器增强嵌入模型，适合特定任务优化。</p>
<p>下载量：8,925 | 1 个月前更新</p>
</li>
</ul>
</li>
<li>
<p>特定平台与服务嵌入模型</p>
<ul>
<li><strong>DatabricksEmbedding</strong> (enrico-stauss): 基于 Databricks 平台的嵌入模型，适合大数据环境。<br>
下载量：2,679 | 1 个月前更新</li>
</ul>
</li>
<li>
<p>其他嵌入模型</p>
<ul>
<li><strong>DashScopeEmbedding</strong> (liuyhwangyh): 基于阿里云 DashScope 的嵌入模型，适合中文场景。<br>
下载量：0 | 1 个月前更新</li>
</ul>
</li>
</ol>
</blockquote>
<h3 id="21-适配器与增强adapterembeddingmodel-linearadapterembeddingmodel介绍"><a class="markdownIt-Anchor" href="#21-适配器与增强adapterembeddingmodel-linearadapterembeddingmodel介绍"></a> 2.1 适配器与增强（AdapterEmbeddingModel &amp; LinearAdapterEmbeddingModel）介绍</h3>
<blockquote>
<h4 id="什么是适配器与增强"><a class="markdownIt-Anchor" href="#什么是适配器与增强"></a> 什么是适配器与增强:</h4>
<p>在 LlamaIndex 框架中，<strong>适配器与增强</strong>（Adapter Embedding Models）是指通过在预训练嵌入模型（如 HuggingFace 的 sentence-transformers 或 OpenAI 的 text-embedding-3-small）的基础上添加轻量级适配器层（Adapter Layers）来优化或定制嵌入生成的技术。这些适配器通过微调或增强现有模型，使其更适合特定任务或领域，而无需从头训练整个模型。<strong>AdapterEmbeddingModel</strong> 和 <strong>LinearAdapterEmbeddingModel</strong> 是 LlamaIndex 提供的两种适配器嵌入模型，用于增强嵌入向量的性能。</p>
</blockquote>
<blockquote>
<h4 id="适配器与增强的流程一般如下"><a class="markdownIt-Anchor" href="#适配器与增强的流程一般如下"></a> 适配器与增强的流程一般如下：</h4>
<ol>
<li><strong>选择基础嵌入模型</strong> 👉 选用预训练嵌入模型（如 HuggingFaceEmbedding 或 OpenAIEmbedding）作为基础。</li>
<li><strong>添加适配器层</strong> 👉 在基础模型的输出层后添加轻量级神经网络层（如线性层或其他结构），用于调整嵌入向量。</li>
<li><strong>微调适配器</strong> 👉 对适配器层进行微调，优化特定任务（如领域特定语义搜索、文本分类），保持基础模型参数不变。</li>
<li><strong>生成嵌入</strong> 👉 使用增强后的模型将文本或其他数据转换为向量，适用于 RAG、语义检索等场景。</li>
<li><strong>存储与检索</strong> 👉 将生成的嵌入向量存储到向量数据库（如 FAISS、Weaviate），用于后续查询和生成。</li>
</ol>
</blockquote>
<blockquote>
<h4 id="技术组件工具链-4"><a class="markdownIt-Anchor" href="#技术组件工具链-4"></a> 🔧 技术组件（工具链）:</h4>
<ul>
<li><strong>基础嵌入模型</strong>：依赖现有嵌入模型（如 HuggingFaceEmbedding、OpenAIEmbedding）提供初始向量表示。</li>
<li><strong>适配器层</strong>：轻量级神经网络模块（如线性层、Transformer 层），用于调整嵌入向量。</li>
<li><strong>向量数据库</strong>：与 FAISS、Chroma、Weaviate 等集成，存储增强后的嵌入向量。</li>
<li><strong>框架支持</strong>：LlamaIndex 提供统一接口，简化适配器模型的集成和使用。</li>
</ul>
</blockquote>
<p><strong>实现</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.embeddings.adapter <span class="keyword">import</span> AdapterEmbeddingModel</span><br><span class="line">base_model = HuggingFaceEmbedding(model_name=<span class="string">&quot;sentence-transformers/all-MiniLM-L6-v2&quot;</span>)</span><br><span class="line">adapter_model = AdapterEmbeddingModel(base_model, adapter_config=&#123;<span class="string">&quot;type&quot;</span>: <span class="string">&quot;linear&quot;</span>&#125;)</span><br><span class="line">embeddings = adapter_model.get_text_embedding(<span class="string">&quot;Hello, world!&quot;</span>)</span><br></pre></td></tr></table></figure>
<h1 id="llamaindex-indexes"><a class="markdownIt-Anchor" href="#llamaindex-indexes"></a> LlamaIndex Indexes</h1>
<h2 id="1-介绍-4"><a class="markdownIt-Anchor" href="#1-介绍-4"></a> 1 介绍</h2>
<blockquote>
<h4 id="什么是-llamaindex-indexes"><a class="markdownIt-Anchor" href="#什么是-llamaindex-indexes"></a> 什么是 LlamaIndex Indexes:</h4>
<p><strong>LlamaIndex Indexes</strong> 是 LlamaIndex 框架中的核心模块，用于组织、存储和检索数据，以便在 <strong>RAG（Retrieval-Augmented Generation）</strong> 或其他数据驱动任务中高效访问外部知识。索引将原始数据（如文档、文本）转换为结构化表示（通常是向量或关键词形式），支持快速检索和语义搜索。<strong>VectorStoreIndex</strong> 是其中最常用的索引类型，基于向量嵌入存储数据，广泛用于语义相似性检索。</p>
</blockquote>
<blockquote>
<h4 id="llamaindex-indexes-的流程一般如下"><a class="markdownIt-Anchor" href="#llamaindex-indexes-的流程一般如下"></a> LlamaIndex Indexes 的流程一般如下：</h4>
<ol>
<li><strong>数据加载</strong> 👉 使用 LlamaIndex Readers（如 PDFReader、SimpleWebPageReader）加载原始数据（如文档、网页）。</li>
<li><strong>数据预处理</strong> 👉 将数据分块（chunking）、提取元数据，并使用嵌入模型（如 OpenAIEmbedding）生成向量表示。</li>
<li><strong>索引构建</strong> 👉 将向量或关键词存储到索引结构（如向量数据库、关键词表），支持高效查询。</li>
<li><strong>查询执行</strong> 👉 用户输入查询，索引将查询转为嵌入向量，检索最相关的文档或数据片段。</li>
<li><strong>上下文提供</strong> 👉 检索结果作为上下文，供语言模型（LLM）生成答案或进一步处理。</li>
</ol>
</blockquote>
<blockquote>
<h4 id="技术组件工具链-5"><a class="markdownIt-Anchor" href="#技术组件工具链-5"></a> 🔧 技术组件（工具链）:</h4>
<ul>
<li><strong>嵌入模型</strong>：将文本转为向量（如 HuggingFaceEmbedding、OpenAIEmbedding）。</li>
<li><strong>向量数据库</strong>：存储嵌入向量（如 FAISS、Weaviate、Pinecone）。</li>
<li><strong>存储后端</strong>：支持多种存储方式，包括本地文件、云服务（如 AWS、Google）、数据库（如 Postgres）。</li>
<li><strong>框架支持</strong>：与 LlamaIndex 的 Readers、Agents 和 LLMs 无缝集成，构建 RAG 管道。</li>
</ul>
</blockquote>
<h1 id="llamaindex-retrievers检索器"><a class="markdownIt-Anchor" href="#llamaindex-retrievers检索器"></a> LlamaIndex Retrievers/检索器</h1>
<h2 id="1-介绍-5"><a class="markdownIt-Anchor" href="#1-介绍-5"></a> 1 介绍</h2>
<blockquote>
<h4 id="什么是-llamaindex-retrievers"><a class="markdownIt-Anchor" href="#什么是-llamaindex-retrievers"></a> 什么是 LlamaIndex Retrievers:</h4>
<p><strong>LlamaIndex Retrievers</strong> 是 LlamaIndex 框架中的检索模块，用于从索引（如 VectorStoreIndex）或外部数据源（如数据库、知识库）中提取与用户查询最相关的文档或数据片段。它们是 <strong>RAG（Retrieval-Augmented Generation）</strong> 流程中的核心组件，通过语义搜索、关键词匹配或其他检索策略，提供高质量的外部上下文，供语言模型（LLM）生成答案。Retrievers 通常与索引和嵌入模型（如 OpenAIEmbedding）配合使用，以实现高效的语义或关键词检索。</p>
</blockquote>
<blockquote>
<h4 id="llamaindex-retrievers-的流程一般如下"><a class="markdownIt-Anchor" href="#llamaindex-retrievers-的流程一般如下"></a> LlamaIndex Retrievers 的流程一般如下：</h4>
<ol>
<li><strong>接收用户查询</strong> 👉 用户输入自然语言查询（如“什么是 AI？”）。</li>
<li><strong>查询转换</strong> 👉 将查询转为嵌入向量（语义检索）或关键词（关键词检索），具体取决于 Retriever 类型。</li>
<li><strong>检索数据</strong> 👉 从索引或外部数据源（如向量数据库、知识库）中查找最相关的文档或数据片段。</li>
<li><strong>后处理（可选）</strong> 👉 结合 Post Processors（如 CohereRerank）对检索结果进行排序、过滤或优化。</li>
<li><strong>返回结果</strong> 👉 输出相关文档或数据片段，作为上下文供 LLM 生成答案。</li>
</ol>
</blockquote>
<blockquote>
<h4 id="技术组件工具链-6"><a class="markdownIt-Anchor" href="#技术组件工具链-6"></a> 🔧 技术组件（工具链）:</h4>
<ul>
<li><strong>检索策略</strong>：支持语义检索（基于嵌入向量）、关键词检索（如 BM25）、混合检索等。</li>
<li><strong>嵌入模型</strong>：用于生成查询和文档的向量表示（如 HuggingFaceEmbedding）。</li>
<li><strong>索引支持</strong>：与 LlamaIndex 的索引模块（如 VectorStoreIndex）或外部数据源（如 AWS Kendra）集成。</li>
<li><strong>框架支持</strong>：与 LlamaIndex 的 Agents、Post Processors 和 LLMs 无缝集成，构建 RAG 管道。</li>
</ul>
</blockquote>
<hr>
<h2 id="2-具体-retrievers-介绍"><a class="markdownIt-Anchor" href="#2-具体-retrievers-介绍"></a> 2 <strong>具体 Retrievers 介绍</strong></h2>
<blockquote>
<ol>
<li>BM25Retriever (llama-index)
<ul>
<li><strong>功能</strong>：基于 BM25 算法的关键词检索器，优化文本的关键词匹配。</li>
<li>特点：
<ul>
<li>使用经典信息检索算法 BM25，基于词频和文档长度计算相关性。</li>
<li>不依赖嵌入模型，适合轻量级、快速的关键词检索。</li>
</ul>
</li>
</ul>
</li>
<li>PathwayRetriever (llama-index)
<ul>
<li><strong>功能</strong>：基于 Pathway 框架的检索器，优化语义搜索和复杂查询处理。</li>
<li>特点：
<ul>
<li>支持语义检索，结合嵌入模型和向量索引。</li>
<li>适合处理复杂查询和多模态数据。</li>
</ul>
</li>
</ul>
</li>
</ol>
</blockquote>
<h2 id="3-分解复杂查询"><a class="markdownIt-Anchor" href="#3-分解复杂查询"></a> 3 分解复杂查询</h2>
<h1 id="reranking"><a class="markdownIt-Anchor" href="#reranking"></a> Reranking</h1>
<h2 id="1-介绍-6"><a class="markdownIt-Anchor" href="#1-介绍-6"></a> 1 介绍</h2>
<blockquote>
<h4 id="什么是-reranking"><a class="markdownIt-Anchor" href="#什么是-reranking"></a> 什么是 Reranking:</h4>
<p><strong>Reranking</strong>（重新排序）是 RAG（Retrieval-Augmented Generation）流程中的一种后处理技术，用于优化初始检索结果的相关性。它通过`<strong>对检索到的文档或数据片段重新评分和排序</strong>，确保返回的结果更符合用户查询的语义或上下文需求。Reranking 通常在初始检索（Retrieval）阶段之后，通过更精细的模型（如嵌入模型或 LLM）或算法（如 ColBERT、Cohere）来提升结果质量。</p>
</blockquote>
<blockquote>
<h4 id="reranking-的流程一般如下"><a class="markdownIt-Anchor" href="#reranking-的流程一般如下"></a> Reranking 的流程一般如下：</h4>
<ol>
<li><strong>初始检索</strong> 👉 使用 Retriever（如 BM25Retriever 或 VectorStoreIndex）从索引或数据源中获取一组候选文档（通常 top-k 个）。</li>
<li><strong>重新评分</strong> 👉 应用 reranking 模型（如 CohereRerank、ColbertRerank）或算法，基于查询与文档的语义相关性重新计算分数。</li>
<li><strong>排序优化</strong> 👉 根据新分数对文档重新排序，优先保留最相关的文档。</li>
<li><strong>过滤（可选）</strong> 👉 移除低相关性文档或限制返回数量（如 top-5）。</li>
<li><strong>返回结果</strong> 👉 输出优化后的文档列表，供 RAG 的生成阶段或直接使用。</li>
</ol>
</blockquote>
<blockquote>
<h4 id="技术组件工具链-7"><a class="markdownIt-Anchor" href="#技术组件工具链-7"></a> 🔧 技术组件（工具链）:</h4>
<ul>
<li><strong>Reranking 模型</strong>：包括嵌入模型（如 SentenceTransformerRerank）、专用 reranking 模型（如 ColbertRerank）、LLM-based reranking（如 RankGPTRerank）或云服务（如 CohereRerank）。</li>
<li><strong>初始检索</strong>：依赖 LlamaIndex Retrievers（如 BM25Retriever、PathwayRetriever）提供候选文档。</li>
<li><strong>索引支持</strong>：与 LlamaIndex 的索引（如 VectorStoreIndex）和嵌入模型（如 OpenAIEmbedding）集成。</li>
<li><strong>框架支持</strong>：作为 LlamaIndex Post Processors 的一部分，与查询引擎和 Agents 无缝协作。</li>
</ul>
</blockquote>
<h1 id="storage"><a class="markdownIt-Anchor" href="#storage"></a> Storage</h1>
<h1 id="llamaindex-agents"><a class="markdownIt-Anchor" href="#llamaindex-agents"></a> LlamaIndex Agents</h1>
<h2 id="1-介绍-7"><a class="markdownIt-Anchor" href="#1-介绍-7"></a> 1 介绍</h2>
<blockquote>
<h4 id="什么是-llamaindex-agents"><a class="markdownIt-Anchor" href="#什么是-llamaindex-agents"></a> 什么是 LlamaIndex Agents:</h4>
<p><strong>LlamaIndex Agents</strong> 是 LlamaIndex 框架中的智能代理模块，设计用于结合语言模型、工具和外部数据源（如知识库、API）来执行复杂任务。这些代理能够自主推理、调用工具、检索数据并生成响应，广泛应用于 <strong>RAG（Retrieval-Augmented Generation）</strong>、任务自动化和多步骤推理场景。它们通过与 LlamaIndex 的索引和嵌入模块集成，提供智能化的数据处理和交互能力。</p>
</blockquote>
<blockquote>
<h4 id="llamaindex-agents-的流程一般如下"><a class="markdownIt-Anchor" href="#llamaindex-agents-的流程一般如下"></a> LlamaIndex Agents 的流程一般如下：</h4>
<ol>
<li><strong>接收任务</strong> 👉 用户输入任务或查询（如“分析文档并回答问题”或“调用 API 获取数据”）。</li>
<li><strong>任务分解与规划</strong> 👉 代理根据任务分解为子步骤，可能涉及检索数据、调用工具或推理。</li>
<li><strong>数据检索</strong> 👉 使用 LlamaIndex 的索引模块（如 VectorStoreIndex）从知识库检索相关数据。</li>
<li><strong>工具调用</strong> 👉 调用外部工具（如 API、计算函数）或内部工具（如数据查询）以补充信息。</li>
<li><strong>生成响应</strong> 👉 结合检索数据和工具输出，使用语言模型生成最终答案。</li>
</ol>
</blockquote>
<blockquote>
<h4 id="技术组件工具链-8"><a class="markdownIt-Anchor" href="#技术组件工具链-8"></a> 🔧 技术组件（工具链）:</h4>
<ul>
<li><strong>语言模型</strong>：支持 OpenAI、Mistral、HuggingFace 等模型，用于推理和生成。</li>
<li><strong>索引与嵌入</strong>：与 LlamaIndex 的嵌入模型（如 OpenAIEmbedding）和向量数据库（如 FAISS、Weaviate）集成。</li>
<li><strong>工具集成</strong>：支持外部工具（如 API、计算函数）和内部工具（如查询引擎）。</li>
<li><strong>框架支持</strong>：LlamaIndex 提供统一接口，简化代理的配置和使用。</li>
</ul>
</blockquote>
<p>2</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">index = VectorStoreIndex.from_documents(</span><br><span class="line">    documents,</span><br><span class="line">    embed_model=Settings.embed_model,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="2-详细流程"><a class="markdownIt-Anchor" href="#2-详细流程"></a> 2 详细流程</h2>
<p>LlamaIndex Agents 的运作可以分为以下核心步骤，适用于大多数代理（如 OpenAIAgent、ContextRetrieverOpenAIAgent 等）：</p>
<p><strong>接收用户输入</strong></p>
<ul>
<li>用户通过自然语言提出任务或查询（如“分析文档并回答问题”或“调用 API 获取天气数据”）。</li>
<li>代理解析输入，识别任务目标和所需资源。</li>
</ul>
<p><strong>任务分解与规划</strong></p>
<ul>
<li>代理分析任务复杂度，决定是否需要分解为子任务。</li>
<li>使用语言模型（LLM）进行推理，生成执行计划（如“先检索文档，再调用工具”）。</li>
<li>高级代理（如 LLMCompilerAgentWorker）可能使用编译式推理或抽象链（Chain-of-Abstraction）优化规划。</li>
</ul>
<p><strong>数据检索（Retrieval）</strong></p>
<ul>
<li>若任务需要外部知识，代理调用 LlamaIndex 的索引模块（如 VectorStoreIndex）。</li>
<li>将用户查询转为嵌入向量（使用嵌入模型，如 OpenAIEmbedding）。</li>
<li>在向量数据库（如 FAISS、Weaviate）中检索与查询语义最相关的文档或数据片段。</li>
</ul>
<p><strong>工具调用（Tool Interaction）</strong></p>
<ul>
<li>代理根据任务需求调用外部工具（如 API、计算函数）或内部工具（如查询引擎）。</li>
<li>工具调用可能涉及动态函数选择（如 FnRetrieverOpenAIAgent）或多步骤工具交互（如 ToolInteractiveReflectionAgentWorker）。</li>
<li>工具输出作为额外上下文，补充检索到的数据。</li>
</ul>
<p><strong>推理与上下文增强（Augmentation）</strong></p>
<ul>
<li>代理整合检索到的文档、工具输出和用户输入，形成增强上下文。</li>
<li>高级代理（如 IntrospectiveAgentWorker、SelfReflectionAgentWorker）可能进行自我反思，评估上下文质量并优化推理。</li>
</ul>
<p><strong>生成响应（Generation）</strong></p>
<ul>
<li>使用语言模型（LLM，如 OpenAI GPT-4、Mistral）基于增强上下文生成最终答案。</li>
<li>输出可以是自然语言回答、结构化数据或其他形式，视任务而定。</li>
</ul>
<p><strong>反馈与迭代（可选）</strong></p>
<ul>
<li>部分代理（如 SelfReflectionAgentWorker）会对输出进行反思，检查是否需要重新检索或调整工具调用。</li>
<li>迭代优化直到满足任务要求或达到预设条件。</li>
</ul>
<blockquote>
<h3 id="运作机制"><a class="markdownIt-Anchor" href="#运作机制"></a> <strong>运作机制</strong></h3>
<ul>
<li>核心组件：
<ul>
<li><strong>语言模型（LLM）</strong>：负责任务规划、推理和生成（如 OpenAI GPT、HuggingFace 模型）。</li>
<li><strong>嵌入模型</strong>：将文本转为向量，用于检索（如 HuggingFaceEmbedding）。</li>
<li><strong>索引模块</strong>：管理知识库，基于向量数据库检索数据（如 VectorStoreIndex）。</li>
<li><strong>工具模块</strong>：支持外部 API、函数或内部查询引擎，扩展代理能力。</li>
</ul>
</li>
<li>工作原理：
<ul>
<li>代理通过 LLM 理解用户意图，结合索引模块检索相关数据，使用工具补充信息。</li>
<li>代理动态决定执行路径，可能循环调用检索和工具，直到任务完成。</li>
<li>反射机制（reflection）或编译式推理（如 LLMCompilerAgentWorker）优化复杂任务的执行效率。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">List</span>, <span class="type">Dict</span>, <span class="type">Any</span></span><br><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> VectorStoreIndex, FunctionTool</span><br><span class="line"><span class="keyword">from</span> llama_index.core.llms <span class="keyword">import</span> LLM</span><br><span class="line"><span class="keyword">from</span> llama_index.core.embeddings <span class="keyword">import</span> BaseEmbedding</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LlamaIndexAgent</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, llm: LLM, index: VectorStoreIndex, tools: <span class="type">List</span>[FunctionTool], embedding_model: BaseEmbedding</span>):</span><br><span class="line">        self.llm = llm  <span class="comment"># 语言模型</span></span><br><span class="line">        self.index = index  <span class="comment"># 向量索引</span></span><br><span class="line">        self.tools = tools  <span class="comment"># 工具列表</span></span><br><span class="line">        self.embedding_model = embedding_model  <span class="comment"># 嵌入模型</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">chat</span>(<span class="params">self, user_query: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        <span class="comment"># 1. 解析用户输入</span></span><br><span class="line">        query = user_query.strip()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2. 任务分解与规划</span></span><br><span class="line">        plan = self.llm.generate_plan(query)  <span class="comment"># 使用 LLM 推理生成执行计划</span></span><br><span class="line">        sub_tasks = plan.get_sub_tasks()  <span class="comment"># 假设计划分解为子任务</span></span><br><span class="line"></span><br><span class="line">        context = []</span><br><span class="line">        <span class="keyword">for</span> task <span class="keyword">in</span> sub_tasks:</span><br><span class="line">            <span class="comment"># 3. 数据检索</span></span><br><span class="line">            <span class="keyword">if</span> task.requires_retrieval:</span><br><span class="line">                query_embedding = self.embedding_model.get_text_embedding(task.query)</span><br><span class="line">                retrieved_docs = self.index.query(query_embedding, top_k=<span class="number">5</span>)</span><br><span class="line">                context.extend(retrieved_docs)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 4. 工具调用</span></span><br><span class="line">            <span class="keyword">if</span> task.requires_tool:</span><br><span class="line">                tool = self.select_tool(task)  <span class="comment"># 动态选择工具</span></span><br><span class="line">                tool_output = tool.execute(task.tool_input)</span><br><span class="line">                context.append(tool_output)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 5. 上下文增强</span></span><br><span class="line">        augmented_context = self.combine_context(query, context)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 6. 反思（可选）</span></span><br><span class="line">        <span class="keyword">if</span> self.has_reflection:</span><br><span class="line">            augmented_context = self.reflect_and_refine(augmented_context)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 7. 生成响应</span></span><br><span class="line">        response = self.llm.generate_response(augmented_context)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 8. 反馈与迭代（可选）</span></span><br><span class="line">        <span class="keyword">if</span> self.needs_iteration(response):</span><br><span class="line">            <span class="keyword">return</span> self.chat(query)  <span class="comment"># 重新执行以优化结果</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> response</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">select_tool</span>(<span class="params">self, task: <span class="type">Dict</span></span>) -&gt; FunctionTool:</span><br><span class="line">        <span class="comment"># 动态选择适合任务的工具</span></span><br><span class="line">        <span class="keyword">for</span> tool <span class="keyword">in</span> self.tools:</span><br><span class="line">            <span class="keyword">if</span> tool.name == task.tool_name:</span><br><span class="line">                <span class="keyword">return</span> tool</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;No suitable tool found&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">combine_context</span>(<span class="params">self, query: <span class="built_in">str</span>, context: <span class="type">List</span>[<span class="type">Any</span>]</span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        <span class="comment"># 合并查询和上下文</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">f&quot;Query: <span class="subst">&#123;query&#125;</span>\nContext: <span class="subst">&#123;<span class="string">&#x27;&#x27;</span>.join(<span class="built_in">str</span>(c) <span class="keyword">for</span> c <span class="keyword">in</span> context)&#125;</span>&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reflect_and_refine</span>(<span class="params">self, context: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        <span class="comment"># 反思并优化上下文（适用于 IntrospectiveAgentWorker 等）</span></span><br><span class="line">        reflection = self.llm.evaluate_context(context)</span><br><span class="line">        <span class="keyword">return</span> reflection.refined_context</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">needs_iteration</span>(<span class="params">self, response: <span class="built_in">str</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        <span class="comment"># 判断是否需要迭代优化</span></span><br><span class="line">        <span class="keyword">return</span> self.llm.evaluate_response(response).needs_refinement</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例使用</span></span><br><span class="line">llm = OpenAI(model=<span class="string">&quot;gpt-4&quot;</span>)</span><br><span class="line">index = VectorStoreIndex.from_documents(documents)</span><br><span class="line">tools = [FunctionTool.from_defaults(fn=<span class="keyword">lambda</span> x: x * <span class="number">2</span>, name=<span class="string">&quot;double&quot;</span>)]</span><br><span class="line">embedding_model = OpenAIEmbedding(model=<span class="string">&quot;text-embedding-3-small&quot;</span>)</span><br><span class="line">agent = LlamaIndexAgent(llm, index, tools, embedding_model)</span><br><span class="line"></span><br><span class="line">response = agent.chat(<span class="string">&quot;Double the number 42 and summarize this document.&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(response)</span><br></pre></td></tr></table></figure>
</blockquote>
<h2 id="3-主要-agents-介绍"><a class="markdownIt-Anchor" href="#3-主要-agents-介绍"></a> 3 <strong>主要 Agents 介绍</strong></h2>
<blockquote>
<ol>
<li><strong>ContextRetrieverOpenAIAgent</strong>
<ul>
<li><strong>功能</strong>：基于 OpenAI 语言模型的代理，结合上下文检索功能，优化 RAG 场景下的数据查询和响应生成。</li>
<li><strong>特点</strong>：
<ul>
<li>利用 OpenAI 模型进行推理，结合 LlamaIndex 的索引模块检索上下文。</li>
<li>适合需要强语义理解的任务，如问答、对话系统。</li>
<li>强调上下文相关性，确保检索到的信息高度匹配用户查询。</li>
</ul>
</li>
</ul>
</li>
<li><strong>FnRetrieverOpenAIAgent</strong>
<ul>
<li><strong>功能</strong>：基于 OpenAI 模型的代理，专注于通过函数调用（Function Calling）增强数据检索能力。</li>
<li><strong>特点</strong>：
<ul>
<li>支持动态函数调用，允许代理根据任务选择合适的检索工具或 API。</li>
<li>结合 LlamaIndex 的检索模块，优化复杂查询的处理。</li>
<li>适合需要结合外部工具和数据源的任务。</li>
</ul>
</li>
</ul>
</li>
<li><strong>LATSAgentWorker</strong>
<ul>
<li><strong>功能</strong>：基于 LATS（Learning-Augmented Task Solver）框架的代理，专注于任务分解和多步骤推理。</li>
<li><strong>特点</strong>：
<ul>
<li>通过学习增强任务解决能力，自动规划和执行复杂任务。</li>
<li>结合 LlamaIndex 的检索和工具调用功能，处理多源数据。</li>
<li>适合需要分步推理和动态规划的任务。</li>
</ul>
</li>
</ul>
</li>
</ol>
</blockquote>
<h1 id="其他处理"><a class="markdownIt-Anchor" href="#其他处理"></a> 其他处理</h1>
<h3 id="output-parsers"><a class="markdownIt-Anchor" href="#output-parsers"></a> Output Parsers</h3>
<blockquote>
<p>解析语言模型（LLM）的原始输出，将其从自由文本转换为结构化数据（如 JSON、列表、表格）或特定格式</p>
</blockquote>
<h3 id="post-processors"><a class="markdownIt-Anchor" href="#post-processors"></a> Post Processors</h3>
<blockquote>
<p>接收输入 👉 从索引（如 VectorStoreIndex）获取检索到的文档或从 LLM 获取生成结果。</p>
</blockquote>
<h3 id="agent-tools"><a class="markdownIt-Anchor" href="#agent-tools"></a> Agent Tools</h3>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://shakewely.github.io/2025/06/02/2025%E5%B9%B46%E6%9C%882%E6%97%A5-RAG/" data-id="cmbhxnxbe000ggov4ftob7p9q" class="article-share-link">分享</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2025/06/02/2025%E5%B9%B46%E6%9C%882%E6%97%A5-pandas/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">上一篇</strong>
      <div class="article-nav-title">
        
          2025年6月2日 pandas
        
      </div>
    </a>
  
  
    <a href="/2025/05/27/2025%E5%B9%B45%E6%9C%8827%E6%97%A5-opencv%E6%B1%87%E6%80%BB/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">下一篇</strong>
      <div class="article-nav-title">2025年5月27日 opencv汇总</div>
    </a>
  
</nav>

  
</article>



</section>
        
          <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title"></h3>
  <div class="widget">
    <h1 class="blog-title">Weakliy_Blog</h1>
    <h2 class="blog-subtitle"></h2>
    <ul class="blog-link">
     
          <a href="/" title="Home">
            <li>主页</li>
          </a>
        
          <a href="/archives" title="Archives">
            <li>归档</li>
          </a>
        
          <a href="/categories" title="Categories">
            <li>分类</li>
          </a>
        
          <a href="/tags" title="Tags">
            <li>标签</li>
          </a>
        
    </ul>
  </div>
</div>

  
    <div class="widget-wrap">
  <h3 class="widget-title"></h3>
  <div class="widget">
    <img class="avatar" src="https://raw.githubusercontent.com/ShakeWeLy/Weakliy.github.io/main/%E5%A4%B4%E5%83%8F/mmexport1683194148817.png">
    <h2 class="author">Weakliy</h2>
    <h3 class="description"></h3>
    <div class="count-box">
      <a href="/archives"><div><strong>118</strong><br>文章</div></a>
      <a href="/categories"><div><strong>0</strong><br>分类</div></a>
      <a href="/tags"><div><strong>34</strong><br>标签</div></a>
    </div>



    <div class="social-link">
      
        <a class="hvr-bounce-in" href="https://github.com/ShakeWeLy" target="_blank" title="Github">
          Github
        </a>
      
    </div>

    <div class="friend-link">
      <h2>联系我</h2>
      
        <a class="hvr-bounce-in" href="https://github.com/ShakeWeLy" target="_blank" title="ShanaMaid">
          ShanaMaid
        </a>
      
    </div>
  </div>
</div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy;2024 - 2025 Weakliy<br>
      由<a href="http://hexo.io/" target="_blank">Hexo</a>强力驱动 | 
      主题-<a target="_blank" rel="noopener" href="https://github.com/ShanaMaid/hexo-theme-shana">Shana</a>
      
    </div>
    
  </div>
</footer>
    </div>
    

<script src="//apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"></script>
<script src="//apps.bdimg.com/libs/wow/0.1.6/wow.min.js"></script>
<script>
new WOW().init();
</script>   


  
<link rel="stylesheet" href="/plugin/fancybox/jquery.fancybox.css">

  
<script src="/plugin/fancybox/jquery.fancybox.pack.js"></script>




  
<link rel="stylesheet" href="/plugin/galmenu/GalMenu.css">

  
<script src="/plugin/galmenu/GalMenu.js"></script>

  <div class="GalMenu GalDropDown">
      <div class="circle" id="gal">
        <div class="ring">
          
            <a href="/" title="" class="menuItem">首页</a>
          
            <a href="/tags" title="" class="menuItem">标签</a>
          
            <a href="/categories" title="" class="menuItem">分类</a>
          
            <a href="/archives" title="" class="menuItem">总览</a>
          
            <a href="/xxxxxxxxx" title="" class="menuItem">xxx</a>
          
            <a href="/xxxxxxx" title="" class="menuItem">xxxx</a>
          
        </div>
        
          <audio id="audio" src="#"></audio>
        
      </div> 
</div>
<div id="overlay" style="opacity: 1; cursor: pointer;"></div>
  <script type="text/javascript">var items = document.querySelectorAll('.menuItem');
    for (var i = 0,
    l = items.length; i < l; i++) {
      items[i].style.left = (50 - 35 * Math.cos( - 0.5 * Math.PI - 2 * (1 / l) * i * Math.PI)).toFixed(4) + "%";
      items[i].style.top = (50 + 35 * Math.sin( - 0.5 * Math.PI - 2 * (1 / l) * i * Math.PI)).toFixed(4) + "%"
    }</script>
<script type="text/javascript">
  $(document).ready(function() {
    $('body').GalMenu({
      'menu': 'GalDropDown'
    })
  });
</script>

  <section class="hidden-xs"> 
  <ul class="cb-slideshow"> 
    <li><span>苟利</span></li> 
    <li><span>国家</span></li> 
    <li><span>生死以</span></li> 
    <li><span>岂能</span></li> 
    <li><span>祸福</span></li> 
    <li><span>趋避之</span></li> 
  </ul>
</section>

<script src="/js/script.js"></script>




  </div>
</body>
</html>