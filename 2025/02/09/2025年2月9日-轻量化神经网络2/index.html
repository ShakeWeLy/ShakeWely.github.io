<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>2025年2月9日 轻量化神经网络2 | Weakliy_Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="LCNet  CPU端的最强轻量型架构    主要解决方案：  采用H-Swish作为激活函数，性能大幅提升，而推理速度几乎不变。 在网络合适的位置添加少量的SE模块可以进一步提升模型性能；实验表明：当把SE置于模型的尾部时，它具有更好的效果。因此，我们仅将SE模块添加到接近网络尾部的模块即可, 这种处理方式具有更好的精度-速度平衡。 根据MixNet的实验论证结果:在一定范围内大的卷积核可以提">
<meta property="og:type" content="article">
<meta property="og:title" content="2025年2月9日 轻量化神经网络2">
<meta property="og:url" content="https://shakewely.github.io/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/index.html">
<meta property="og:site_name" content="Weakliy_Blog">
<meta property="og:description" content="LCNet  CPU端的最强轻量型架构    主要解决方案：  采用H-Swish作为激活函数，性能大幅提升，而推理速度几乎不变。 在网络合适的位置添加少量的SE模块可以进一步提升模型性能；实验表明：当把SE置于模型的尾部时，它具有更好的效果。因此，我们仅将SE模块添加到接近网络尾部的模块即可, 这种处理方式具有更好的精度-速度平衡。 根据MixNet的实验论证结果:在一定范围内大的卷积核可以提">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://shakewely.github.io/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250219133119899.png">
<meta property="og:image" content="https://shakewely.github.io/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209162740618.png">
<meta property="og:image" content="https://shakewely.github.io/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209172824970.png">
<meta property="og:image" content="https://shakewely.github.io/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209172918099.png">
<meta property="og:image" content="https://shakewely.github.io/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209173848541.png">
<meta property="og:image" content="https://shakewely.github.io/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209174104028.png">
<meta property="og:image" content="https://shakewely.github.io/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209180216944.png">
<meta property="og:image" content="https://shakewely.github.io/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209174529781.png">
<meta property="og:image" content="https://pic2.zhimg.com/v2-7c42cff2fa3c346d2e41be95848fc619_1440w.jpg">
<meta property="og:image" content="https://shakewely.github.io/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209174719710.png">
<meta property="og:image" content="https://shakewely.github.io/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209183640934.png">
<meta property="og:image" content="https://shakewely.github.io/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250303211827687.png">
<meta property="og:image" content="https://shakewely.github.io/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250303211752599.png">
<meta property="og:image" content="https://shakewely.github.io/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209184403579.png">
<meta property="og:image" content="https://shakewely.github.io/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209184638406.png">
<meta property="og:image" content="https://pic4.zhimg.com/v2-31d05dfac01c6ae64bd5bf4e52ffc069_1440w.jpg">
<meta property="og:image" content="https://pic4.zhimg.com/v2-dc2339ac78de452e286317dd259070f5_1440w.jpg">
<meta property="og:image" content="https://pic3.zhimg.com/v2-104058d724746316b12298ec27fafe26_1440w.jpg">
<meta property="og:image" content="https://shakewely.github.io/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209190732084.png">
<meta property="og:image" content="https://shakewely.github.io/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209224653761.png">
<meta property="og:image" content="https://shakewely.github.io/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209223353104.png">
<meta property="og:image" content="https://shakewely.github.io/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209224903212.png">
<meta property="og:image" content="https://shakewely.github.io/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209224555819.png">
<meta property="og:image" content="https://pic4.zhimg.com/v2-faec285a27615d8829af43172d9d1385_1440w.jpg">
<meta property="og:image" content="https://pic4.zhimg.com/v2-a0f1319eae0571829c79b1a70d4fc1b9_1440w.jpg">
<meta property="og:image" content="https://pic2.zhimg.com/v2-032dfea7e52345c3f26a543e3dbcc6fd_1440w.jpg">
<meta property="og:image" content="https://pic1.zhimg.com/v2-4f1414b59ea6700387f0dea2dac0d878_1440w.jpg">
<meta property="og:image" content="https://pica.zhimg.com/v2-646a538d15c17d28145024953ff90eda_1440w.jpg">
<meta property="og:image" content="https://shakewely.github.io/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209230157789.png">
<meta property="og:image" content="https://shakewely.github.io/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209230342459.png">
<meta property="og:image" content="https://shakewely.github.io/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209231222947.png">
<meta property="og:image" content="https://shakewely.github.io/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250303184803366.png">
<meta property="og:image" content="https://shakewely.github.io/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250210155707027.png">
<meta property="og:image" content="https://shakewely.github.io/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250210155825206.png">
<meta property="og:image" content="https://shakewely.github.io/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250210160135377.png">
<meta property="og:image" content="https://shakewely.github.io/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250210160342687.png">
<meta property="article:published_time" content="2025-02-09T07:21:36.000Z">
<meta property="article:modified_time" content="2025-05-20T09:00:00.906Z">
<meta property="article:author" content="Weakliy">
<meta property="article:tag" content="轻量化神经网络、model">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://shakewely.github.io/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250219133119899.png">
  
    <link rel="alternate" href="/atom.xml" title="Weakliy_Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

  
<link rel="stylesheet" href="/plugin/bganimation/bg.css">

  

  <link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css" rel="stylesheet" type="text/css">
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <div class="outer">
        <div class="widget-wrap mobile-header">
  <h3 class="widget-title"></h3>
  <div class="widget">
    <img class="avatar" src="https://raw.githubusercontent.com/ShakeWeLy/Weakliy.github.io/main/%E5%A4%B4%E5%83%8F/mmexport1683194148817.png">
    <h2 class="author">Weakliy</h2>
    <h3 class="description"></h3>
    <div class="count-box">
      <a href="/archives"><div><strong>119</strong><br>文章</div></a>
      <a href="/categories"><div><strong>0</strong><br>分类</div></a>
      <a href="/tags"><div><strong>34</strong><br>标签</div></a>
    </div>
    <ul class="blog-link">
     
          <a href="/" title="Home">
            <li>主页</li>
          </a>
        
          <a href="/archives" title="Archives">
            <li>归档</li>
          </a>
        
          <a href="/categories" title="Categories">
            <li>分类</li>
          </a>
        
          <a href="/tags" title="Tags">
            <li>标签</li>
          </a>
        
    </ul>
  </div>
</div>

        <section id="main"><article id="post-2025年2月9日-轻量化神经网络2" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/" class="article-date">
  <time class="post-time" datetime="2025-02-09T07:21:36.000Z" itemprop="datePublished">
    <span class="post-month">2月</span><br/>
    <span class="post-day">09</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      2025年2月9日 轻量化神经网络2
    </h1>
  

        <div>
          
          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="lcnet"><a class="markdownIt-Anchor" href="#lcnet"></a> LCNet</h1>
<blockquote>
<p>CPU端的最强轻量型架构</p>
</blockquote>
<img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250219133119899.png" alt="image-20250219133119899" style="zoom:33%;">
<h3 id="主要解决方案"><a class="markdownIt-Anchor" href="#主要解决方案"></a> 主要解决方案：</h3>
<ol>
<li>采用H-Swish作为激活函数，性能大幅提升，而推理速度几乎不变。</li>
<li>在网络合适的位置添加少量的SE模块可以进一步提升模型性能；实验表明：当把SE置于模型的尾部时，它具有更好的效果。因此，我们仅将SE模块添加到接近网络尾部的模块即可, 这种处理方式具有更好的精度-速度平衡。</li>
<li>根据MixNet的实验论证结果:在一定范围内大的卷积核可以提升模型的性能，但是超过这个范围会有损模型的性能。本文通过实验总结了一些更大的卷积核在不同位置的作用，类似SE模块的位置，更大的卷积核在网络的中后部作用更明显。比如5x5卷积。</li>
<li>GAP后使用更大的1x1卷积层；在GoogLeNet之后，GAP（Global-Average-Pooling）后往往直接接分类层，但是在轻量级网络中，这样会导致GAP后提取的特征没有得到进一步的融合和加工。如果在此后使用一个更大的1x1卷积层（等同于FC层），GAP后的特征便不会直接经过分类层，而是先进行了融合，并将融合的特征进行分类。这样可以在不影响模型推理速度的同时大大提升准确率。</li>
<li>使用dropout技术可以进一步提升模型的精度。</li>
</ol>
<p><strong>低秩卷积的引入</strong>：</p>
<ul>
<li>低秩卷积的引入是 LCNet 的最大创新之一。通过矩阵低秩分解，LCNet 能够将卷积操作的计算复杂度降到最低，从而显著减少了卷积神经网络的计算负担和内存消耗。</li>
</ul>
<p><strong>高效的通道剪枝</strong>：</p>
<ul>
<li>LCNet 提出了通道剪枝的概念，即对卷积层中的不重要通道进行剪枝，减少冗余计算。这个过程通过分析每个通道的贡献，剔除那些对最终输出没有显著影响的通道，从而大幅减小模型的规模。</li>
</ul>
<p><strong>集成深度可分离卷积</strong>：</p>
<ul>
<li>深度可分离卷积已经被证明是提升计算效率的一种有效手段，LCNet 将其应用到各个卷积层中，通过拆解卷积操作来减少计算量和参数量，提高了模型的运行效率。</li>
</ul>
<p><strong>硬件友好设计</strong>：</p>
<ul>
<li>LCNet 强调了硬件友好的设计理念，特别是在移动设备和嵌入式设备上。网络架构通过采用低计算量的卷积和模块化设计，使得 LCNet 在计算资源受限的设备上表现更好。</li>
</ul>
<p><strong>优化的注意力机制</strong>：</p>
<ul>
<li>LCNet 在设计中还结合了高效的注意力机制，通过优化网络对输入信息的加权方式，使得网络能够更好地关注到重要特征，从而提升分类性能，同时保持较低的计算开销。</li>
</ul>
<h1 id="squeezenet"><a class="markdownIt-Anchor" href="#squeezenet"></a> SqueezeNet</h1>
<h2 id="fire-module"><a class="markdownIt-Anchor" href="#fire-module"></a> Fire Module</h2>
<p><strong>Fire Module</strong>：每个 Fire Module 由两个主要部分组成：</p>
<ul>
<li><strong>Squeeze层</strong>：一个1x1的卷积层，用来减少特征图的深度。</li>
<li><strong>Expand层</strong>：两个分支，一个是1x1卷积，另一个是3x3卷积，用来增加特征图的深度。</li>
</ul>
<p>通过这种结构，SqueezeNet能够有效地减少参数量，而不牺牲太多的准确度。</p>
<p><strong>减少全连接层的参数</strong>：SqueezeNet中没有传统的全连接层，而是使用全局平均池化（Global Average Pooling）来替代。这样大大减少了参数数量，且仍然保留了模型的表现力。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cat([x1, x2], 1)</span><br></pre></td></tr></table></figure>
<h1 id="mobilenethttpsblogcsdnnetqq_37555071articledetails108393809"><a class="markdownIt-Anchor" href="#mobilenethttpsblogcsdnnetqq_37555071articledetails108393809"></a> [MobileNet][<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_37555071/article/details/108393809">https://blog.csdn.net/qq_37555071/article/details/108393809</a>]</h1>
<h2 id="深度可分离卷积depthwise-separable-convolutions"><a class="markdownIt-Anchor" href="#深度可分离卷积depthwise-separable-convolutions"></a> 深度可分离卷积(Depthwise Separable Convolutions)</h2>
<p>（Depthwise Separable Convolutions）</p>
<p><strong>深度卷积（Depthwise Convolution）</strong>：对每个输入通道独立进行卷积操作，而不是像传统卷积那样对所有通道进行卷积。这大大减少了计算量。</p>
<p><strong>逐点卷积（Pointwise Convolution）</strong>：即1x1卷积，用于将深度卷积的输出进行线性组合，融合通道信息。</p>
<p><strong>宽度和分辨率的可调性</strong>： MobileNet引入了两个超参数，分别是<strong>宽度系数（Width Multiplier）**和**分辨率系数（Resolution Multiplier）</strong>，使得网络的大小和计算量可以根据实际需求进行调整。</p>
<ul>
<li><strong>宽度系数（α）</strong>：用于控制网络每一层的通道数。通过减小α的值，可以降低网络的复杂度和参数数量。</li>
<li><strong>分辨率系数（ρ）</strong>：用于控制输入图像的分辨率，通过减小分辨率来减少计算量。</li>
</ul>
<p><img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209162740618.png" alt="image-20250209162740618"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=<span class="number">3</span>, stride=stride, padding=<span class="number">1</span>, groups=in_channels, bias=<span class="literal">False</span>)</span><br><span class="line">// groups=in_channels</span><br></pre></td></tr></table></figure>
<h1 id="shufflenet"><a class="markdownIt-Anchor" href="#shufflenet"></a> ShuffleNet</h1>
<p><strong>深度可分离卷积（Depthwise Separable Convolution） 和</strong></p>
<h2 id="通道混洗channel-shuffle"><a class="markdownIt-Anchor" href="#通道混洗channel-shuffle"></a> <strong>通道混洗（Channel Shuffle）</strong></h2>
<p>通道混洗技术会在<strong>每个卷积层的输出中打乱通道的顺序</strong>，使得不同通道的特征能够进行更好的融合，从而提高模型的表达能力和准确率。</p>
<h2 id="分组卷积grouped-convolution"><a class="markdownIt-Anchor" href="#分组卷积grouped-convolution"></a> <strong>分组卷积（Grouped Convolution）</strong></h2>
<p>为了进一步提高网络的计算效率，ShuffleNet 采用了 <strong>分组卷积（Grouped Convolution）</strong>。分组卷积将输入通道分成多个组，每个组内的通道与一个独立的卷积核进行卷积，从而减少了卷积运算的计算量。</p>
<h1 id="googlenet"><a class="markdownIt-Anchor" href="#googlenet"></a> GoogLeNet</h1>
<h2 id="inception块"><a class="markdownIt-Anchor" href="#inception块"></a> Inception块</h2>
<p>各种模块全都要</p>
<img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209172824970.png" alt="image-20250209172824970" style="zoom:50%;">
<img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209172918099.png" alt="image-20250209172918099" style="zoom:67%;">
<h2 id="xception-块"><a class="markdownIt-Anchor" href="#xception-块"></a> <strong>Xception 块</strong></h2>
<p>极限情况</p>
<h1 id="efficientnet"><a class="markdownIt-Anchor" href="#efficientnet"></a> EfficientNet</h1>
<p><img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209173848541.png" alt="image-20250209173848541" style="zoom: 80%;"><img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209174104028.png" alt="image-20250209174104028" style="zoom: 67%;"></p>
<h2 id="原理"><a class="markdownIt-Anchor" href="#原理"></a> 原理</h2>
<h3 id="网络架构搜索nas"><a class="markdownIt-Anchor" href="#网络架构搜索nas"></a> <strong>网络架构搜索（NAS）</strong>：</h3>
<ul>
<li>EfficientNet 使用了 <strong>神经架构搜索</strong>（NAS）来自动化地找到最适合的卷积神经网络架构。在这个过程中，作者通过 NAS 来寻找一个 <strong>基础架构</strong>，该架构在计算效率和性能之间取得了最佳的平衡。</li>
</ul>
<h3 id="优化网络的宽度-深度和分辨率"><a class="markdownIt-Anchor" href="#优化网络的宽度-深度和分辨率"></a> <strong>优化网络的宽度、深度和分辨率</strong>：</h3>
<ul>
<li>EfficientNet 在进行网络扩展时，并不是简单地增加网络的层数或通道数，而是采用了更加 <strong>均衡的增长策略</strong>。通过在深度、宽度和输入图像分辨率上都进行扩展，模型能够获得更强的表达能力，同时保持较低的计算成本。</li>
</ul>
<h3 id="高效的卷积操作"><a class="markdownIt-Anchor" href="#高效的卷积操作"></a> <strong>高效的卷积操作</strong>：</h3>
<ul>
<li>EfficientNet 采用了高效的卷积操作，如 <strong>Depthwise Separable Convolution</strong>，以进一步减少计算量。</li>
</ul>
<p>通过结构搜索（NAS, Neural Architecture Search）和优化策略，在精度和计算效率之间找到最好的平衡。</p>
<h2 id="mbconvefficientnet块"><a class="markdownIt-Anchor" href="#mbconvefficientnet块"></a> MBConv/EfficientNet块</h2>
<p><img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209180216944.png" alt="image-20250209180216944"></p>
<p><img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209174529781.png" alt="image-20250209174529781"></p>
<h3 id="倒残差结构inverted-residuals"><a class="markdownIt-Anchor" href="#倒残差结构inverted-residuals"></a> 倒残差结构（Inverted Residuals)</h3>
<p><img src="https://pic2.zhimg.com/v2-7c42cff2fa3c346d2e41be95848fc619_1440w.jpg" alt="img"></p>
<blockquote>
<p>使用逐通道卷积和逐点卷积来提高计算效率。</p>
</blockquote>
<h2 id="注意力机制"><a class="markdownIt-Anchor" href="#注意力机制"></a> 注意力机制</h2>
<h3 id="se模块squeeze-and-excitation-block"><a class="markdownIt-Anchor" href="#se模块squeeze-and-excitation-block"></a> SE模块（Squeeze-and-Excitation Block）</h3>
<p>来源于人类视觉系统中的注意机制：大脑会根据不同的视觉刺激，<strong>自动聚焦于最重要的信息，并忽略不相关的部分</strong>。SE模块通过类似的机制，<strong>自动为每个通道分配不同的重要性</strong>，从而增强模型对重要特征的敏感度。</p>
<ul>
<li>
<p><strong>queeze-and-Excitation</strong>（SE）模块，提升了特征通道之间的依赖关系.仅在 <strong>通道维度</strong> 上进行加权</p>
</li>
<li>
<p>通过一个 <strong>“Squeeze”</strong> 操作来压缩空间维度信息，再通过 <strong>“Excitation”</strong> 操作生成通道权重</p>
</li>
</ul>
<p><strong>Squeeze（压缩）</strong>：</p>
<ul>
<li>输入是一个 <strong>H×W×C</strong> 的特征图，其中 H和 W 分别是空间维度的高度和宽度，C 是通道数。</li>
<li><strong>通过全局平均池化</strong>（Global Average Pooling）对每个通道进行压缩。即对每个通道的空间维度（H×W）求平均，得到一个 <strong>C</strong> 维的向量，表示每个通道的“全局特征”。</li>
</ul>
<p><strong>Excitation（激励）</strong>：</p>
<ul>
<li>对通道的全局特征向量进行<strong>两层全连接层操作</strong>，其中第二层是激活函数（通常是 <strong>Sigmoid</strong>），生成每个通道的 <strong>注意力系数</strong>。</li>
<li>第一个全连接层是 <strong>瓶颈层（bottleneck layer）</strong>，通常通过降低维度来减少计算量。然后通过第二个全连接层，将输出恢复到原始通道数，得到每个通道的权重。</li>
</ul>
<img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209174719710.png" alt="image-20250209174719710" style="zoom:67%;">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_make_se_block</span>(<span class="params">self, channels, se_ratio</span>):</span><br><span class="line">        reduction = <span class="built_in">int</span>(channels * se_ratio)</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">            nn.AdaptiveAvgPool2d(<span class="number">1</span>),</span><br><span class="line">            nn.Conv2d(channels, reduction, kernel_size=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(reduction, channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">            nn.Sigmoid()</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<h4 id="空间注意力机制"><a class="markdownIt-Anchor" href="#空间注意力机制"></a> 空间注意力机制</h4>
<p>对特征图的<strong>每个位置</strong>分配一个权重来决定哪些空间区域应该被网络更多关注</p>
<blockquote>
<p>人类的视觉注意力更多的是<strong>空间注意力</strong>，也就是对图片上不同区域赋予不同的权重</p>
</blockquote>
<img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209183640934.png" alt="image-20250209183640934" style="zoom:67%;">
<p><img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250303211827687.png" alt="image-20250303211827687"></p>
<h4 id="通道注意力机制"><a class="markdownIt-Anchor" href="#通道注意力机制"></a> 通道注意力机制</h4>
<p><img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250303211752599.png" alt="image-20250303211752599"></p>
<h4 id="cbam注意力"><a class="markdownIt-Anchor" href="#cbam注意力"></a> CBAM注意力</h4>
<p>CBAM 通过融合 <strong>通道注意力机制</strong> 和 <strong>空间注意力机制</strong></p>
<h4 id="视觉自注意力non-local"><a class="markdownIt-Anchor" href="#视觉自注意力non-local"></a> 视觉自注意力(Non Local))</h4>
<p>用来在不引入过多计算量的基础上提高CNN网络的<strong>远程依赖</strong></p>
<p><img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209184403579.png" alt="image-20250209184403579"></p>
<blockquote>
<p>我可能先识别到篮球，然后在其周围找到人和篮筐，并根据他们的位置我们才能判断这个图是不是表达人在灌篮这个动作。其中，篮球位置可以理解为<strong>查询点</strong>，周边（可能离得较远）的人和篮筐就是<strong>查询点对应的关联区域</strong>。</p>
</blockquote>
<p>查询点到关联区域的对应可以加深CNN网络对场景<strong>从局部到整体</strong>的理解，因此可以有效提高CNN网络在视觉任务的效率。</p>
<ul>
<li>
<p>该模块建立了图像中<strong>每个像素/区域之间的关联</strong>，有效提升了CNN网络的感受野</p>
</li>
<li>
<p>对每个空间位置生成与所有其他位置的<strong>相似度</strong>。</p>
</li>
<li>
<p>基于<strong>相似度进行加权</strong>，使得每个位置可以融合其他位置的信息。</p>
</li>
</ul>
<p><img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209184638406.png" alt="image-20250209184638406"></p>
<blockquote>
<p>其大概的运算过程是：</p>
<p>（1）为了降低运算量，采用三个不同1x1卷积层进行维度减半，即图上的 x→ϕ(x),θ(x),g(x) ;</p>
<p>（2）为 ϕ(x),θ(x),g(x) 按照w,h维度进行铺平，即图上三个flatten；</p>
<p>（3）利用铺平的 ϕ(x),θ(x) 进行矩阵乘法运算，并通过softmax获得各空间位置之间的<strong>关联图</strong>（即图中的 vc ），很明显这个关联图的大小为 (w×h,w×h) ，表征着各个像素点（区域）之间的联系；</p>
<p>（4）将铺平转置的 g(x) 与vc进行矩阵乘法，获得 y ；</p>
<p>（5）对y进行展开与特征提取（Conv4），获得注意力（refined）;</p>
<p>（6）利用注意力调整原始输入的分布，Over!!!</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/350760243">https://zhuanlan.zhihu.com/p/350760243</a></p>
</blockquote>
<h4 id="non-local改进版-gcnet"><a class="markdownIt-Anchor" href="#non-local改进版-gcnet"></a> Non Local改进版 — GCNet</h4>
<p>原因:<strong>不同查询点（区域）居然对应相同的attention map</strong></p>
<img src="https://pic4.zhimg.com/v2-31d05dfac01c6ae64bd5bf4e52ffc069_1440w.jpg" alt="img" style="zoom: 33%;">
<p>与查询（点）无关的依赖（query-independent dependency）。那么这是否意味着原始Non Local中<strong>query分支可以剪去不要</strong>呢</p>
<img src="https://pic4.zhimg.com/v2-dc2339ac78de452e286317dd259070f5_1440w.jpg" alt="img" style="zoom: 50%;">
<img src="https://pic3.zhimg.com/v2-104058d724746316b12298ec27fafe26_1440w.jpg" alt="img" style="zoom:50%;">
<h3 id="dfc-注意力模块"><a class="markdownIt-Anchor" href="#dfc-注意力模块"></a> DFC 注意力模块</h3>
<p>利用固定权重的全连接层来创建具有全局感受野的注意力图，以捕 获长距离空间位置的依赖关系</p>
<h1 id="efficientdet"><a class="markdownIt-Anchor" href="#efficientdet"></a> EfficientDet</h1>
<h2 id="结构概述"><a class="markdownIt-Anchor" href="#结构概述"></a> <strong>结构概述</strong></h2>
<h3 id="1-backboneefficientnet"><a class="markdownIt-Anchor" href="#1-backboneefficientnet"></a> 1 Backbone（EfficientNet）</h3>
<ul>
<li>EfficientDet 的 backbone 使用了 EfficientNet 作为特征提取器。EfficientNet 是一种使用<strong>复合缩放</strong>策略的高效网络，在目标检测任务中，EfficientDet 对 EfficientNet 进行调整和优化，使得其能更好地适应目标检测的要求。</li>
</ul>
<p>BiFPN（双向特征金字塔网络）**</p>
<ul>
<li>BiFPN 通过对低层和高层特征的加权融合，使得低层和高层的特征能够互相补充，提升了多尺度特征的利用率。</li>
</ul>
<h3 id="2-head"><a class="markdownIt-Anchor" href="#2-head"></a> 2 Head</h3>
<ul>
<li><strong>分类头</strong>：用于对目标进行分类，预测每个框的类别。</li>
<li><strong>回归头</strong>：用于回归目标的边界框坐标。</li>
</ul>
<h3 id="3-复合缩放"><a class="markdownIt-Anchor" href="#3-复合缩放"></a> 3 复合缩放</h3>
<ul>
<li>EfficientDet 通过复合缩放策略调整模型的深度、宽度和输入分辨率，使得模型可以在不同的硬件环境下进行灵活的调整，同时提升了精度和效率。</li>
</ul>
<h2 id="fpn到bifpn"><a class="markdownIt-Anchor" href="#fpn到bifpn"></a> FPN.到BiFPN</h2>
<img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209190732084.png" style="zoom: 80%;">
<img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209224653761.png" style="zoom: 80%;">
<blockquote>
<p>a. 网络只是针对某一特点的分辨率进行训练, 如果只是在测试和推理阶段使用图像金字塔的话, 可能导致训练和测试推理过程不匹配</p>
<p>b. 利用单个高层特征图(主干网络产生)进行物体的分类和bounding box的回归</p>
<p>c. 在 <strong>多个不同尺度的特征图（如 38×38、19×19、10×10）</strong> 上直接预测目标框。</p>
<p>d 尽管在SSD中我们已经使用了特征金字塔, 但该金字塔中的所有要素都处于不同的比例, 并且由于网络中层的深度不同而存在巨大的语义鸿沟. 高分辨率地图具有低级语义特征, 而低分辨率地图具有较高的语义特征, 这会损害其对象识别的表示能力.</p>
</blockquote>
<img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209223353104.png" alt="image-20250209223353104" style="zoom:50%;">
<blockquote>
<p>先对高阶特征进行上采样, 然后使用横向连接将其与低阶特征进行组合, 该横向连接基本上是1x1卷积, 然后进行求和,</p>
</blockquote>
<h3 id="fully-connected-fpn-and-nas-fpn"><a class="markdownIt-Anchor" href="#fully-connected-fpn-and-nas-fpn"></a> Fully connected FPN and NAS-FPN</h3>
<p><img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209224903212.png" alt="image-20250209224903212"></p>
<blockquote>
<p>a. 传统fpn</p>
<p>b. 全连接网络</p>
<p>c. NAS</p>
</blockquote>
<h3 id="panet"><a class="markdownIt-Anchor" href="#panet"></a> PANet</h3>
<p><img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209224555819.png" alt="image-20250209224555819"></p>
<blockquote>
<p>添加<strong>自下而上的路径</strong>以增强FPN中的自上而下的路径</p>
</blockquote>
<h3 id="simplified-fpn-and-bifpn"><a class="markdownIt-Anchor" href="#simplified-fpn-and-bifpn"></a> Simplified FPN and BiFPN</h3>
<p><img src="https://pic4.zhimg.com/v2-faec285a27615d8829af43172d9d1385_1440w.jpg" alt="img"></p>
<blockquote>
<p>a. 如果一个节点只有一个输入边并且没有特征融合, 那么它对特征网络的融合贡献较小, 这个节点可以删除(Simplified PANET)</p>
<p>b. 添加一条额外的连接路径,融合更多功能(BiFPN). 这点其实跟skip connection很相似</p>
<p>c. 重复叠加相同的特征网络层 复合缩放</p>
</blockquote>
<h2 id="权重计算"><a class="markdownIt-Anchor" href="#权重计算"></a> 权重计算</h2>
<p>快速归一化融合特征网络.计算路径,计算出不同特征节点的输入最合适的权值</p>
<p><img src="https://pic4.zhimg.com/v2-a0f1319eae0571829c79b1a70d4fc1b9_1440w.jpg" alt="img"></p>
<h2 id="compound-scaling"><a class="markdownIt-Anchor" href="#compound-scaling"></a> Compound Scaling</h2>
<p>复合缩放的目标是在任何给定的资源约束下最大化模型精度, 因此可以表述为优化问题.</p>
<p><strong>对网络深度、宽度和分辨率中的任何尺度进行缩放都可以提高精度, 但是当模型足够大时, 这种放大的收益会减弱。</strong></p>
<p><strong>FLOPS</strong>是floating point operations per second的缩写, 意指每秒浮点运算次数, 理解为计算速度. 是一个衡量硬件性能的指标. 我们假设我们能使用的FLOPS是2.</p>
<p><img src="https://pic2.zhimg.com/v2-032dfea7e52345c3f26a543e3dbcc6fd_1440w.jpg" alt="img"></p>
<blockquote>
<p>a. 对于网络模型depth来说, 加倍深度会使得FLOPS加倍.</p>
</blockquote>
<p><img src="https://pic1.zhimg.com/v2-4f1414b59ea6700387f0dea2dac0d878_1440w.jpg" alt="img"></p>
<blockquote>
<p>b. 对于网络模型width来说, 由于width(#channel)的增加导致卷积计算的路径平方级增加, 因此加倍宽度会使得FLOPS加4倍.</p>
</blockquote>
<p><img src="https://pica.zhimg.com/v2-646a538d15c17d28145024953ff90eda_1440w.jpg" alt="img"></p>
<blockquote>
<p>c. 对于网络模型resolution来说, 和width的情况一样, 由于resolution的增加会导致feather map呈现平方级扩张, 因此加倍图像分辨率也会使得FLOPS加4倍.</p>
</blockquote>
<p><strong>复合缩放公式:</strong></p>
<p><img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209230157789.png" alt="image-20250209230157789"></p>
<p><strong>网格搜索</strong></p>
<p><img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209230342459.png" alt></p>
<p><strong>复合缩放的总结图:</strong></p>
<p><img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209231222947.png" alt="image-20250209231222947"></p>
<h1 id="ghostnet"><a class="markdownIt-Anchor" href="#ghostnet"></a> GhostNet</h1>
<p>特征图存在<strong>冗余性</strong> 廉价操作 生成 Ghost 特征图</p>
<h2 id="原理-2"><a class="markdownIt-Anchor" href="#原理-2"></a> 原理</h2>
<h2 id="ghost-卷积"><a class="markdownIt-Anchor" href="#ghost-卷积"></a> Ghost 卷积</h2>
<p>利用计算量较低的线性操作来增加特征图的数量， 从而在保持输出特征图不变的同时，显著地降低了计算复杂度</p>
<ol>
<li>使用1×1卷积对输入特征进行压缩，以获取少量特征图</li>
<li>这部分特征图通过线性变换生成另一部分特征图</li>
<li>将两部分特征图进行拼接，形 成最终的输出特征图</li>
</ol>
<img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250303184803366.png" alt="image-20250303184803366" style="zoom:67%;">
<h1 id="mobilevit"><a class="markdownIt-Anchor" href="#mobilevit"></a> MobileViT</h1>
<p><strong>融合 CNN 和 Transformer</strong>：通过将卷积操作与 Transformer 结合，模型能够同时捕捉局部和全局特征，提高了特征表达能力。</p>
<p><strong>高效的特征处理</strong>：通过<strong>展开和折叠操作</strong>(unfold )，将特征映射到序列空间进行处理，然后再映射回原始空间，实现了高效的特征处理。</p>
<p><strong>模型图：</strong></p>
<img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250210155707027.png" alt="image-20250210155707027" style="zoom:50%;">
<h2 id="mobile-vit-块"><a class="markdownIt-Anchor" href="#mobile-vit-块"></a> Mobile ViT 块</h2>
<img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250210155825206.png" alt="image-20250210155825206" style="zoom: 50%;">
<blockquote>
<p>…</p>
</blockquote>
<p>减少self-attention</p>
<img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250210160135377.png" alt="image-20250210160135377" style="zoom:50%;"> 
<blockquote>
<p>冗余信息多,相邻token信息差异小</p>
</blockquote>
<h3 id="fold"><a class="markdownIt-Anchor" href="#fold"></a> fold</h3>
<img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250210160342687.png" alt="image-20250210160342687" style="zoom:33%;">
<h1 id="emo结合-cnn-和-transformerhttpsdeveloperaliyuncomarticle1210407"><a class="markdownIt-Anchor" href="#emo结合-cnn-和-transformerhttpsdeveloperaliyuncomarticle1210407"></a> [EMO：结合 CNN 和 Transformer][<a target="_blank" rel="noopener" href="https://developer.aliyun.com/article/1210407">https://developer.aliyun.com/article/1210407</a>]</h1>
<h3 id="1-特征提取网络sd-ghostnet"><a class="markdownIt-Anchor" href="#1-特征提取网络sd-ghostnet"></a> 1. 特征提取网络（SD-Ghostnet）</h3>
<ul>
<li><strong>输入图像</strong>：原始图像作为输入。</li>
<li><strong>Conv(64,6,2,2)</strong>：首先通过一个卷积层，输出通道数为64，核大小为6x6，步长为2，填充为2。</li>
<li><strong>S-GhostConv (128,3,2)</strong> 和 <strong>SD-Ghost(128)</strong>：经过一系列的S-GhostConv和SD-Ghost模块，逐步增加通道数到128。</li>
<li><strong>S-GhostConv (256,3,2)</strong> 和 <strong>SD-Ghost(256)</strong>：进一步通过S-GhostConv和SD-Ghost模块，通道数增加到256。</li>
<li><strong>S-GhostConv (512,3,2)</strong> 和 <strong>SD-Ghost(512)</strong>：继续通过S-GhostConv和SD-Ghost模块，通道数增加到512。</li>
<li><strong>S-GhostConv (1024,3,2)</strong> 和 <strong>SD-Ghost(1024)</strong>：最后通过S-GhostConv和SD-Ghost模块，通道数增加到1024。</li>
<li><strong>SPPF(1024,5)</strong>：空间金字塔池化模块，进一步增强特征表示。</li>
</ul>
<h3 id="2-特征融合网络ffn"><a class="markdownIt-Anchor" href="#2-特征融合网络ffn"></a> 2. 特征融合网络（FFN）</h3>
<ul>
<li><strong>Concat(512)</strong>：将特征提取网络中不同层次的特征进行拼接，形成512通道的特征图。</li>
<li><strong>Upsample(256)</strong>：对特征图进行上采样，通道数减少到256。</li>
<li><strong>GSCovn (256,1,1)</strong>：通过一个1x1的卷积层调整通道数。</li>
<li><strong>VOVGSCSP (256)</strong>：通过VOVGSCSP模块进一步处理特征。</li>
<li><strong>GSCovn (256,3,2)</strong>：通过一个1x1的卷积层调整通道数。</li>
<li><strong>Concat(512)</strong>：再次拼接特征，形成512通道的特征图。</li>
<li><strong>VOVGSCSP (512)</strong>：通过VOVGSCSP模块进一步处理特征。</li>
<li><strong>GSCovn (512,3,2)</strong>：通过一个1x1的卷积层调整通道数。</li>
<li><strong>Concat(1024)</strong>：再次拼接特征，形成1024通道的特征图。</li>
<li><strong>VOVGSCSP (1024)</strong>：通过VOVGSCSP模块进一步处理特征。</li>
</ul>
<h3 id="3-多尺度检测网络mdn"><a class="markdownIt-Anchor" href="#3-多尺度检测网络mdn"></a> 3. 多尺度检测网络（MDN）</h3>
<ul>
<li><strong>CBAM (256)</strong>：通过CBAM注意力机制模块，增强特征表示。</li>
<li><strong>Conv2d (256)</strong>：通过一个卷积层，输出通道数为256。</li>
<li><strong>CBAM (512)</strong>：通过CBAM注意力机制模块，增强特征表示。</li>
<li><strong>Conv2d (512)</strong>：通过一个卷积层，输出通道数为512。</li>
<li><strong>CBAM (1024)</strong>：通过CBAM注意力机制模块，增强特征表示。</li>
<li><strong>Conv2d (1024)</strong>：通过一个卷积层，输出通道数为1024。</li>
<li><strong>输出图像</strong>：最终生成输出图像。</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://shakewely.github.io/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/" data-id="cm6yfcd2b0000mcv4hi073hzf" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E3%80%81model/" rel="tag">轻量化神经网络、model</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2025/02/10/2025%E5%B9%B42%E6%9C%8810%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C3/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">上一篇</strong>
      <div class="article-nav-title">
        
          2025年2月10日 轻量化神经网络3
        
      </div>
    </a>
  
  
    <a href="/2025/02/05/2025%E5%B9%B42%E6%9C%885%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">下一篇</strong>
      <div class="article-nav-title">2025年2月5日 轻量化神经网络1</div>
    </a>
  
</nav>

  
</article>



</section>
        
          <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title"></h3>
  <div class="widget">
    <h1 class="blog-title">Weakliy_Blog</h1>
    <h2 class="blog-subtitle"></h2>
    <ul class="blog-link">
     
          <a href="/" title="Home">
            <li>主页</li>
          </a>
        
          <a href="/archives" title="Archives">
            <li>归档</li>
          </a>
        
          <a href="/categories" title="Categories">
            <li>分类</li>
          </a>
        
          <a href="/tags" title="Tags">
            <li>标签</li>
          </a>
        
    </ul>
  </div>
</div>

  
    <div class="widget-wrap">
  <h3 class="widget-title"></h3>
  <div class="widget">
    <img class="avatar" src="https://raw.githubusercontent.com/ShakeWeLy/Weakliy.github.io/main/%E5%A4%B4%E5%83%8F/mmexport1683194148817.png">
    <h2 class="author">Weakliy</h2>
    <h3 class="description"></h3>
    <div class="count-box">
      <a href="/archives"><div><strong>119</strong><br>文章</div></a>
      <a href="/categories"><div><strong>0</strong><br>分类</div></a>
      <a href="/tags"><div><strong>34</strong><br>标签</div></a>
    </div>



    <div class="social-link">
      
        <a class="hvr-bounce-in" href="https://github.com/ShakeWeLy" target="_blank" title="Github">
          Github
        </a>
      
    </div>

    <div class="friend-link">
      <h2>联系我</h2>
      
        <a class="hvr-bounce-in" href="https://github.com/ShakeWeLy" target="_blank" title="ShanaMaid">
          ShanaMaid
        </a>
      
    </div>
  </div>
</div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy;2024 - 2025 Weakliy<br>
      由<a href="http://hexo.io/" target="_blank">Hexo</a>强力驱动 | 
      主题-<a target="_blank" rel="noopener" href="https://github.com/ShanaMaid/hexo-theme-shana">Shana</a>
      
    </div>
    
  </div>
</footer>
    </div>
    

<script src="//apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"></script>
<script src="//apps.bdimg.com/libs/wow/0.1.6/wow.min.js"></script>
<script>
new WOW().init();
</script>   


  
<link rel="stylesheet" href="/plugin/fancybox/jquery.fancybox.css">

  
<script src="/plugin/fancybox/jquery.fancybox.pack.js"></script>




  
<link rel="stylesheet" href="/plugin/galmenu/GalMenu.css">

  
<script src="/plugin/galmenu/GalMenu.js"></script>

  <div class="GalMenu GalDropDown">
      <div class="circle" id="gal">
        <div class="ring">
          
            <a href="/" title="" class="menuItem">首页</a>
          
            <a href="/tags" title="" class="menuItem">标签</a>
          
            <a href="/categories" title="" class="menuItem">分类</a>
          
            <a href="/archives" title="" class="menuItem">总览</a>
          
            <a href="/xxxxxxxxx" title="" class="menuItem">xxx</a>
          
            <a href="/xxxxxxx" title="" class="menuItem">xxxx</a>
          
        </div>
        
          <audio id="audio" src="#"></audio>
        
      </div> 
</div>
<div id="overlay" style="opacity: 1; cursor: pointer;"></div>
  <script type="text/javascript">var items = document.querySelectorAll('.menuItem');
    for (var i = 0,
    l = items.length; i < l; i++) {
      items[i].style.left = (50 - 35 * Math.cos( - 0.5 * Math.PI - 2 * (1 / l) * i * Math.PI)).toFixed(4) + "%";
      items[i].style.top = (50 + 35 * Math.sin( - 0.5 * Math.PI - 2 * (1 / l) * i * Math.PI)).toFixed(4) + "%"
    }</script>
<script type="text/javascript">
  $(document).ready(function() {
    $('body').GalMenu({
      'menu': 'GalDropDown'
    })
  });
</script>

  <section class="hidden-xs"> 
  <ul class="cb-slideshow"> 
    <li><span>苟利</span></li> 
    <li><span>国家</span></li> 
    <li><span>生死以</span></li> 
    <li><span>岂能</span></li> 
    <li><span>祸福</span></li> 
    <li><span>趋避之</span></li> 
  </ul>
</section>

<script src="/js/script.js"></script>




  </div>
</body>
</html>