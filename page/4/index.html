<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Weakliy_Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Weakliy_Blog">
<meta property="og:url" content="https://shakewely.github.io/page/4/index.html">
<meta property="og:site_name" content="Weakliy_Blog">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Weakliy">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Weakliy_Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

  
<link rel="stylesheet" href="/plugin/bganimation/bg.css">

  

  <link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css" rel="stylesheet" type="text/css">
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <div class="outer">
        <div class="widget-wrap mobile-header">
  <h3 class="widget-title"></h3>
  <div class="widget">
    <img class="avatar" src="https://raw.githubusercontent.com/ShakeWeLy/Weakliy.github.io/main/%E5%A4%B4%E5%83%8F/mmexport1683194148817.png">
    <h2 class="author">Weakliy</h2>
    <h3 class="description"></h3>
    <div class="count-box">
      <a href="/archives"><div><strong>102</strong><br>文章</div></a>
      <a href="/categories"><div><strong>0</strong><br>分类</div></a>
      <a href="/tags"><div><strong>30</strong><br>标签</div></a>
    </div>
    <ul class="blog-link">
     
          <a href="/" title="Home">
            <li>主页</li>
          </a>
        
          <a href="/archives" title="Archives">
            <li>归档</li>
          </a>
        
          <a href="/categories" title="Categories">
            <li>分类</li>
          </a>
        
          <a href="/tags" title="Tags">
            <li>标签</li>
          </a>
        
    </ul>
  </div>
</div>

        <section id="main">
  
    <article id="post-2025年3月16日-DL八股文" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2025/03/14/2025%E5%B9%B43%E6%9C%8816%E6%97%A5-DL%E5%85%AB%E8%82%A1%E6%96%87/" class="article-date">
  <time class="post-time" datetime="2025-03-14T06:44:16.000Z" itemprop="datePublished">
    <span class="post-month">3月</span><br/>
    <span class="post-day">14</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/03/14/2025%E5%B9%B43%E6%9C%8816%E6%97%A5-DL%E5%85%AB%E8%82%A1%E6%96%87/">2025年3月16日 DL八股文</a>
    </h1>
  

        <div>
          
          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <blockquote>
<p>记录一些学了又忘或是没学的DL八股文知识</p>
</blockquote>
<h1 id="dl八股文"><a class="markdownIt-Anchor" href="#dl八股文"></a> DL八股文</h1>
<h1 id="原理"><a class="markdownIt-Anchor" href="#原理"></a> 原理</h1>
<p>一种基于深层神经网络的机器学习方法，通过多层次的非线性变换自动提取数据特征，实现复杂模式的识别与预测。核心是模仿人脑神经元的连接方式，构建“输入-隐藏-输出”层级的计算模型，从数据中学习从简单到抽象的特征表示</p>
<h1 id="训练"><a class="markdownIt-Anchor" href="#训练"></a> 训练</h1>
<blockquote>
<p>模型训练的核心是通过调整参数（如权重 <em>w</em> 和偏置 <em>b</em>） 最小化损失函数 L</p>
</blockquote>
<h2 id="过拟合"><a class="markdownIt-Anchor" href="#过拟合"></a> 过拟合</h2>
<h3 id="1-什么是过拟合"><a class="markdownIt-Anchor" href="#1-什么是过拟合"></a> 1 什么是过拟合</h3>
<p>指模型在训练集上表现优异，但在测试集或新数据上泛化能力显著下降的现象。其核心矛盾是模型过度<strong>学习了训练数据中的噪声和非全局性特征</strong>，而非数据背后的真实规律 。</p>
<h3 id="2-原因"><a class="markdownIt-Anchor" href="#2-原因"></a> 2 原因</h3>
<ol>
<li>数据层面：<strong>数据量不足</strong>，导致没有学到全局特征。<strong>噪声干扰</strong>，数据中存在异常值或错误标签，模型误将其作为学习目标</li>
<li>模型层面：模型过于复杂，<strong>过度适应训练数据细节</strong></li>
<li>训练层面：迭代次数过多（未早停）或学习率未优化/</li>
</ol>
<h3 id="3-如何解决"><a class="markdownIt-Anchor" href="#3-如何解决"></a> 3 如何解决</h3>
<ol>
<li>
<p>：数据增强与数据清洗</p>
</li>
<li>
<p>：选用简单模型。添加正则化策略，<strong>Dropout</strong>、<strong>批量归一化</strong>BatchNorm、<strong>池化</strong></p>
</li>
<li>
<p>：早停法、动态学习率、</p>
</li>
<li>
<p><strong>集成方法</strong>：结合多个模型的预测结果（如随机森林投票），减少单个模型方差</p>
<p><strong>迁移学习等</strong></p>
</li>
</ol>
<h3 id="4-欠拟合呢"><a class="markdownIt-Anchor" href="#4-欠拟合呢"></a> 4 欠拟合呢？</h3>
<h2 id="训练方法"><a class="markdownIt-Anchor" href="#训练方法"></a> 训练方法</h2>
<p>训练方法</p>
<h3 id="1-监督学习"><a class="markdownIt-Anchor" href="#1-监督学习"></a> 1 监督学习</h3>
<h3 id="3-自监督学习"><a class="markdownIt-Anchor" href="#3-自监督学习"></a> 3 自监督学习</h3>
<blockquote>
<p>参考：<a target="_blank" rel="noopener" href="https://developer.aliyun.com/article/1257862?spm=a2c6h.24874632.expert-profile.15.45a63181v37e02%5C">https://developer.aliyun.com/article/1257862?spm=a2c6h.24874632.expert-profile.15.45a63181v37e02\</a></p>
</blockquote>
<h4 id="定义"><a class="markdownIt-Anchor" href="#定义"></a> 定义</h4>
<p>​	它利用未标记的数据来训练模型，而无需人工标注的标签</p>
<p>​	利用数据中的自动生成的标签或任务来训练模型</p>
<h4 id="自监督学习方法"><a class="markdownIt-Anchor" href="#自监督学习方法"></a> 自监督学习方法</h4>
<h3 id="4-zero-shot-one-shot-and-few-shot-learning概念"><a class="markdownIt-Anchor" href="#4-zero-shot-one-shot-and-few-shot-learning概念"></a> 4 Zero-Shot, One-Shot, and Few-Shot Learning概念</h3>
<ol>
<li>Zero-Shot是指训练一个模型来对其从未见过的对象进行分类。其核心思想是利用另一个模型的现有知识，以获得新类别的有意义的表示。</li>
<li>One-Shot是确定图像A是否等同于图像B。这是通过将模型从先前任务的经验中获得的信息进行概括来实现的。</li>
<li>Few-Shot Learning它是元学习的一个子领域，旨在开发能够从少量有标签示例中学习的算法</li>
</ol>
<h2 id="迁移学习微调-️"><a class="markdownIt-Anchor" href="#迁移学习微调-️"></a> 迁移学习+微调 ☑️</h2>
<h3 id="1-迁移学习"><a class="markdownIt-Anchor" href="#1-迁移学习"></a> 1 迁移学习</h3>
<p>在大型数据集上训练好的模型可以被“迁移”到新的任务中，从而避免从零开始训练。大地缩短训练时间，并显著提高性能</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载预训练的ResNet模型</span></span><br><span class="line">resnet = models.resnet50(pretrained=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="2-微调"><a class="markdownIt-Anchor" href="#2-微调"></a> 2 微调</h3>
<ul>
<li>通过对预训练模型<strong>进行部分或全部参数的微调</strong>，模型可以适应新任务中的特定数据。微调的程度取决于新任务的相似性和目标。</li>
<li>通常会冻结模型的部分层，以保留通用的特征提取能力，针对新任务<strong>只对高层</strong>进行微调。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 冻结所有层的参数，以便只微调最后的全连接层</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> resnet.parameters():</span><br><span class="line">    param.requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 解冻部分层，允许更多层进行训练</span></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> resnet.named_parameters():</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&quot;layer4&quot;</span> <span class="keyword">in</span> name:  <span class="comment"># 假设只解冻ResNet的最后一层</span></span><br><span class="line">        param.requires_grad = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        param.requires_grad = <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h2 id="梯度"><a class="markdownIt-Anchor" href="#梯度"></a> 梯度</h2>
<p><img src="/2025/03/14/2025%E5%B9%B43%E6%9C%8816%E6%97%A5-DL%E5%85%AB%E8%82%A1%E6%96%87/image-20250320184024743.png" alt></p>
<h3 id="2-梯度稀疏化问题"><a class="markdownIt-Anchor" href="#2-梯度稀疏化问题"></a> 2 梯度稀疏化问题</h3>
<h4 id="定义-2"><a class="markdownIt-Anchor" href="#定义-2"></a> 定义</h4>
<p>在深度学习中，梯度是指损失函数对模型参数的偏导数。通过反向传播算法，我们可以计算得到每个参数的梯度，并使用优化算法（如随机梯度下降）来更新参数。然而，在深层神经网络中，梯度通常具有很高的维度，<strong>其中大部分元素都是接近于零或者非常小的值</strong>。这些接近于零的梯度元素对参数更新几乎没有贡献，并且会占用大量内存空间和计算资源。</p>
<ul>
<li>
<p>模型更新偏差：仅部分参数被有效更新，可能偏离全局最优解</p>
</li>
<li>
<p>训练不稳定：梯度分布不均加剧收敛震荡</p>
</li>
</ul>
<h4 id="成因"><a class="markdownIt-Anchor" href="#成因"></a> <strong>成因</strong></h4>
<ol>
<li><strong>模型结构与激活函数</strong>
<ul>
<li>局部激活稀疏性：ReLU等激活函数在输入为负时输出零梯度，导致大量神经元处于“死亡”状态</li>
<li>深度网络层间依赖：深层网络中矩阵连乘导致梯度衰减或爆炸，加剧稀疏性</li>
</ul>
</li>
<li><strong>优化策略与正则化</strong>
<ul>
<li>L1正则化：通过惩罚权重绝对值，强制稀疏参数分布，间接导致梯度稀疏</li>
<li>梯度裁剪：限制梯度幅值，抑制小梯度更新</li>
</ul>
</li>
<li><strong>训练数据分布</strong>
<ul>
<li>数据不均衡：长尾数据导致部分类别梯度贡献微弱</li>
<li>特征冗余：输入数据中存在大量无关特征，相关梯度趋近于零</li>
</ul>
</li>
</ol>
<h4 id="处理方法"><a class="markdownIt-Anchor" href="#处理方法"></a> <strong>处理方法</strong></h4>
<p><strong>优化算法改进</strong></p>
<ul>
<li><strong>动量修正与误差补偿</strong>：如DGC中的Momentum Correction，缓解因稀疏化导致的梯度滞后问题</li>
<li><strong>自适应优化器</strong>：Adam等算法动态调整学习率，避免小梯度参数被完全抑制</li>
</ul>
<p><strong>数据与训练策略调整</strong></p>
<ul>
<li><strong>数据增强与重采样</strong>：平衡类别分布，提升尾部数据的梯度贡献</li>
<li><strong>渐进式稀疏训练</strong>：初期保留更多梯度，逐步增加稀疏率以避免过早信息丢失</li>
</ul>
<h4 id="应用"><a class="markdownIt-Anchor" href="#应用"></a> 应用</h4>
<ol>
<li>剪枝与梯度量化</li>
<li></li>
</ol>
<h2 id="激活函数"><a class="markdownIt-Anchor" href="#激活函数"></a> 激活函数</h2>
<h3 id="1-定义"><a class="markdownIt-Anchor" href="#1-定义"></a> 1 定义</h3>
<p>引入<strong>非线性因素</strong>,处理复杂的问题</p>
<h3 id="2-常见激活函数"><a class="markdownIt-Anchor" href="#2-常见激活函数"></a> 2 常见激活函数</h3>
<h4 id="1-sigmoid"><a class="markdownIt-Anchor" href="#1-sigmoid"></a> 1 Sigmoid</h4>
<img src="/2025/03/14/2025%E5%B9%B43%E6%9C%8816%E6%97%A5-DL%E5%85%AB%E8%82%A1%E6%96%87/image-20250320185054836.png" alt="image-20250320185054836" style="zoom:33%;"> 
<h4 id="2-tanh"><a class="markdownIt-Anchor" href="#2-tanh"></a> 2 Tanh</h4>
<img src="/2025/03/14/2025%E5%B9%B43%E6%9C%8816%E6%97%A5-DL%E5%85%AB%E8%82%A1%E6%96%87/image-20250320185034344.png" alt="image-20250320185034344" style="zoom: 33%;">  
<blockquote>
<p>存在梯度消失的问题，因为它们的导数（连乘）在两端会趋近于零，导致深层网络训练困难</p>
</blockquote>
<h4 id="3-relu"><a class="markdownIt-Anchor" href="#3-relu"></a> 3 relu</h4>
<img src="/2025/03/14/2025%E5%B9%B43%E6%9C%8816%E6%97%A5-DL%E5%85%AB%E8%82%A1%E6%96%87/image-20250320184950902.png" alt="image-20250320184950902" style="zoom: 67%;"> 
<h5 id="优势"><a class="markdownIt-Anchor" href="#优势"></a> 优势</h5>
<ol>
<li><strong>缓解梯度消失</strong>：
<ul>
<li>Sigmoid/Tanh的饱和区：导数在输入绝对值较大时趋近于0（如Sigmoid导数值最大仅0.25），导致深层网络梯度链式相乘后指数级衰减</li>
<li>ReLU的非饱和性：<strong>正区间的导数为1</strong>，保证梯度稳定传递，支持深层网络训练</li>
</ul>
</li>
<li><strong>计算高效</strong>：ReLU仅需判断输入是否大于0（<strong>无指数运算</strong>），比Sigmoid/Tanh快6倍以上，适合大规模数据和复杂模型</li>
<li><strong>稀疏激活</strong>：ReLU将负输入强制置零，使网络<strong>稀疏化</strong>（约50%神经元激活），降低参数依赖性和过拟合风险，增强模型鲁棒性</li>
</ol>
<h5 id="不足"><a class="markdownIt-Anchor" href="#不足"></a> 不足</h5>
<ol>
<li>为负时，ReLU输出恒为0，梯度为0，导致对应权重无法更新（“<strong>死亡</strong>”）</li>
</ol>
<h5 id="改进"><a class="markdownIt-Anchor" href="#改进"></a> 改进</h5>
<blockquote>
<p>改进ReLU的方法，比如Leaky ReLU、Parametric ReLU（PReLU）和ELU，</p>
</blockquote>
<h2 id="损失函数"><a class="markdownIt-Anchor" href="#损失函数"></a> 损失函数</h2>
<blockquote>
<p>解释什么是损失函数，它的作用，常见的损失函数类型，以及如何根据不同的任务选择合适的损失函数，还有在训练过程中如何优化损失。</p>
</blockquote>
<h3 id="1-定义-2"><a class="markdownIt-Anchor" href="#1-定义-2"></a> <strong>1 定义</strong>：</h3>
<p>损失函数是衡量模型预测值与真实值<strong>差异的量化指标</strong>，</p>
<p><strong>用于指导模型参数优化</strong>。反向传播更新梯度</p>
<p><img src="/2025/03/14/2025%E5%B9%B43%E6%9C%8816%E6%97%A5-DL%E5%85%AB%E8%82%A1%E6%96%87/image-20250320182842833.png" alt="image-20250320182842833"></p>
<h3 id="2-常见"><a class="markdownIt-Anchor" href="#2-常见"></a> 2 常见</h3>
<ol>
<li><strong>均方误差</strong> 绝对误差等等</li>
<li>交叉熵</li>
</ol>
<h1 id="模型"><a class="markdownIt-Anchor" href="#模型"></a> 模型</h1>
<h2 id="cnn"><a class="markdownIt-Anchor" href="#cnn"></a> CNN</h2>
<h3 id="0-图像性质"><a class="markdownIt-Anchor" href="#0-图像性质"></a> 0 图像性质</h3>
<ol>
<li><strong>平移不变性</strong>：池化与卷积核滑动使模型对目标位置变化不敏感。</li>
</ol>
<h3 id="1-定义-3"><a class="markdownIt-Anchor" href="#1-定义-3"></a> 1 定义</h3>
<p>核心思想是通过<strong>局部感受野</strong>和<strong>参数共享</strong>，高效提取空间或时序特征。<br>
从原始像素中学习到“边缘→纹理→物体部件→完整物体”的层次化特征表示</p>
<h3 id="2-组成"><a class="markdownIt-Anchor" href="#2-组成"></a> 2 组成</h3>
<h4 id="卷积层"><a class="markdownIt-Anchor" href="#卷积层"></a> <strong>卷积层</strong></h4>
<p><strong>卷积层堆叠</strong>：</p>
<ul>
<li>浅层卷积核提取边缘、颜色等低级特征。</li>
<li>深层卷积核组合低级特征，形成高级语义（如车轮、车窗</li>
</ul>
<h4 id="池化层"><a class="markdownIt-Anchor" href="#池化层"></a> <strong>池化层</strong></h4>
<blockquote>
<p>减少计算量并增强平移不变性。</p>
</blockquote>
<h2 id="resnet"><a class="markdownIt-Anchor" href="#resnet"></a> resnet</h2>
<h3 id="1思想"><a class="markdownIt-Anchor" href="#1思想"></a> 1<strong>思想</strong></h3>
<blockquote>
<p><strong>学习残差而非直接学习目标映射</strong>   通过（跳跃连接）实现残差学习</p>
</blockquote>
<h3 id="2-解决问题"><a class="markdownIt-Anchor" href="#2-解决问题"></a> 2 解决问题</h3>
<h5 id="1-深层网络退化degradation"><a class="markdownIt-Anchor" href="#1-深层网络退化degradation"></a> 1. <strong>深层网络退化（Degradation）</strong></h5>
<ul>
<li><strong>现象</strong>：传统CNN（如VGG）随着层数增加（如20层以上），训练误差和测试误差反而上升，<strong>这不是过拟合，而是模型难以优化</strong>。</li>
<li><strong>原因</strong>：叠加非线性层导致信息传递效率下降，深层网络难以学习有效的恒等映射（Identity Mapping）。</li>
<li><strong>解决</strong>：跳跃连接强制网络学习残差 F(x)<em>F</em>(<em>x</em>)，当最优解接近恒等映射时，F(x)<em>F</em>(<em>x</em>) 只需逼近0，比直接拟合 H(x)=x<em>H</em>(<em>x</em>)=<em>x</em> 更简单。</li>
</ul>
<h5 id="2-梯度消失vanishing-gradient"><a class="markdownIt-Anchor" href="#2-梯度消失vanishing-gradient"></a> 2. <strong>梯度消失（Vanishing Gradient）</strong></h5>
<ul>
<li><strong>现象</strong>：反向传播时，梯度在深层网络中逐层连乘，导致浅层权重更新缓慢甚至停滞。</li>
<li><strong>解决</strong>：跳跃连接提供<strong>梯度高速公路</strong>，梯度可通过加法操作直接回传至浅层（如公式 ∂L∂x=∂L∂H(x)⋅(1+∂F(x)∂x)∂<em>x</em>∂<em>L</em>=∂<em>H</em>(<em>x</em>)∂<em>L</em>⋅(1+∂<em>x</em>∂<em>F</em>(<em>x</em>))），避免梯度被多次非线性变换稀释。</li>
</ul>
<h2 id="模型难以优化"><a class="markdownIt-Anchor" href="#模型难以优化"></a> <strong>模型难以优化</strong></h2>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://shakewely.github.io/2025/03/14/2025%E5%B9%B43%E6%9C%8816%E6%97%A5-DL%E5%85%AB%E8%82%A1%E6%96%87/" data-id="cm8q1tfhq0011pcv41q1mcut5" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/dl/" rel="tag">dl</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-2025年3月14日-据增强方法" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2025/03/14/2025%E5%B9%B43%E6%9C%8814%E6%97%A5-%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%96%B9%E6%B3%95/" class="article-date">
  <time class="post-time" datetime="2025-03-14T05:34:43.000Z" itemprop="datePublished">
    <span class="post-month">3月</span><br/>
    <span class="post-day">14</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/03/14/2025%E5%B9%B43%E6%9C%8814%E6%97%A5-%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%96%B9%E6%B3%95/">2025年3月14日 数据增强方法</a>
    </h1>
  

        <div>
          
          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="数据预处理"><a class="markdownIt-Anchor" href="#数据预处理"></a> 数据预处理</h1>
<h2 id="数据增强方法"><a class="markdownIt-Anchor" href="#数据增强方法"></a> 数据增强方法</h2>
<h3 id="1-数据增强的作用"><a class="markdownIt-Anchor" href="#1-数据增强的作用"></a> 1 数据增强的作用</h3>
<p>1） 避免过拟合。当数据集具有某种明显的特征，例如数据集中图片基本在同一个场景中拍摄，使用Cutout方法和风格迁移变化等相关方法可避免模型学到跟目标无关的信息。</p>
<p>2）提升模型鲁棒性，降低模型对图像的敏感度。当训练数据都属于比较理想的状态，碰到一些特殊情况，如遮挡，亮度，模糊等情况容易识别错误，对训练数据加上噪声，掩码等方法可提升模型鲁棒性。</p>
<p>3）增加训练数据，提高模型泛化能力。</p>
<p>4）避免样本不均衡。在工业缺陷检测方面，医疗疾病识别方面，容易出现正负样本极度不平衡的情况，通过对少样本进行一些数据增强方法，降低样本不均衡比例。</p>
<p>———————————————</p>
<h3 id="2-哪些数据增强方法"><a class="markdownIt-Anchor" href="#2-哪些数据增强方法"></a> 2 哪些数据增强方法</h3>
<h4 id="几何变换"><a class="markdownIt-Anchor" href="#几何变换"></a> 几何变换</h4>
<p>随机旋转（±15°）、</p>
<p>翻转</p>
<p>裁剪，</p>
<p>缩放，</p>
<p>平移，</p>
<h4 id="像素变换"><a class="markdownIt-Anchor" href="#像素变换"></a> 像素变换</h4>
<p>高斯噪声、</p>
<p>运动模糊模拟</p>
<p>调整HSV对比度</p>
<p>调节亮度，饱和度，直方图均衡化，调整白平衡</p>
<h3 id="3-特殊任务"><a class="markdownIt-Anchor" href="#3-特殊任务"></a> 3 特殊任务</h3>
<h4 id="适合分类任务"><a class="markdownIt-Anchor" href="#适合分类任务"></a> 适合分类任务</h4>
<p><img src="/2025/03/14/2025%E5%B9%B43%E6%9C%8814%E6%97%A5-%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%96%B9%E6%B3%95/image-20250314143616191.png" alt="image-20250314143616191"></p>
<blockquote>
<p>Mixup, Cutmix只用于分类任务</p>
</blockquote>
<h4 id="适合检测任务"><a class="markdownIt-Anchor" href="#适合检测任务"></a> 适合检测任务</h4>
<p>GridMask</p>
<img src="/2025/03/14/2025%E5%B9%B43%E6%9C%8814%E6%97%A5-%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%96%B9%E6%B3%95/image-20250314143710300.png" alt="image-20250314143710300" style="zoom: 33%;"> 
<h4 id="mosaic数据增强"><a class="markdownIt-Anchor" href="#mosaic数据增强"></a> mosaic数据增强</h4>
<img src="/2025/03/14/2025%E5%B9%B43%E6%9C%8814%E6%97%A5-%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%96%B9%E6%B3%95/image-20250314143803362.png" alt="image-20250314143803362" style="zoom: 43%;"> 
<h2 id="4-transformscompose"><a class="markdownIt-Anchor" href="#4-transformscompose"></a> 4 Transforms.Compose</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Resize(<span class="number">256</span>),          <span class="comment"># 调整图像尺寸</span></span><br><span class="line">    transforms.RandomCrop(<span class="number">224</span>),      <span class="comment"># 随机裁剪为224x224</span></span><br><span class="line">    transforms.RandomHorizontalFlip(), <span class="comment"># 随机水平翻转（概率0.5）</span></span><br><span class="line">    transforms.ToTensor(),           <span class="comment"># 转为张量并归一化至[0,1]</span></span><br><span class="line">    transforms.Normalize(            <span class="comment"># 标准化（均值、标准差）</span></span><br><span class="line">        mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], </span><br><span class="line">        std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]</span><br><span class="line">    )</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<h3 id="5-复杂场景的特殊处理"><a class="markdownIt-Anchor" href="#5-复杂场景的特殊处理"></a> 5 复杂场景的特殊处理</h3>
<p>复杂场景如图像可能包含以下问题：</p>
<ol>
<li><strong>遮挡与局部模糊</strong>（如交通监控中的车辆遮挡）</li>
<li><strong>光照不均</strong>（如夜间低光照或强反光区域）</li>
<li><strong>动态模糊</strong>（如快速移动目标）</li>
<li><strong>背景干扰</strong>（如医疗图像中器官重叠</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">transform = Compose([</span><br><span class="line">    RandomPerspective(distortion_scale=0.5),  # 透视变形模拟遮挡  </span><br><span class="line">    AdjustGamma(gamma=0.5),                   # 调整伽马值应对低光照  </span><br><span class="line">    GaussianBlur(kernel_size=5)               # 模拟运动模糊  </span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<h2 id="数据不平衡问题"><a class="markdownIt-Anchor" href="#数据不平衡问题"></a> <strong>数据不平衡问题</strong></h2>
<p>参考:<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/296632599%5C">https://zhuanlan.zhihu.com/p/296632599\</a></p>
<h3 id="1-数据不平衡"><a class="markdownIt-Anchor" href="#1-数据不平衡"></a> 1 数据不平衡</h3>
<p>不同类别的样本量差异非常大，或者少数样本代表了业务的关键数据</p>
<h3 id="2-如何处理"><a class="markdownIt-Anchor" href="#2-如何处理"></a> 2 <strong>如何处理</strong></h3>
<ol>
<li>
<p>欠采样：<strong>在少量样本数量不影响模型训练的情况下</strong>，可通过对<strong>多数类样本欠采样</strong>，实现少数样本和多数样本均衡。</p>
</li>
<li>
<p>过采样：在少量样本数量<strong>不支持</strong>模型训练的情况下，可以通过对<strong>少数类样本过采样</strong>，实现少数样本和多数样本的均衡。</p>
</li>
<li>
<p>模型算法：通过引入有权重的模型算法，<strong>针对少量样本着重拟合</strong>，以提升对少量样本特征的学习。</p>
</li>
</ol>
<h4 id="1欠采样"><a class="markdownIt-Anchor" href="#1欠采样"></a> 1.欠采样</h4>
<p>​	减少大样本数量的采集</p>
<h5 id="随机法删除"><a class="markdownIt-Anchor" href="#随机法删除"></a> <strong>随机法</strong>删除</h5>
<blockquote>
<p>随机的删除一些多数类样本</p>
</blockquote>
<h4 id="2过采样"><a class="markdownIt-Anchor" href="#2过采样"></a> 2.过采样</h4>
<h5 id="随机复制"><a class="markdownIt-Anchor" href="#随机复制"></a> <strong>随机复制</strong></h5>
<blockquote>
<p>对少量样本进行复制后达到样本均衡的效果以提升模型的效果。</p>
</blockquote>
<p><strong>样本生成</strong></p>
<blockquote>
<p>SMOTE方法为例，对于任意选取的少类样本，它用K近邻选取相似样本，并通过对样本线性插值得到新样本。</p>
</blockquote>
<h4 id="3模型算法"><a class="markdownIt-Anchor" href="#3模型算法"></a> 3.模型算法</h4>
<p>​	从算法层面</p>
<h5 id="focal-loss"><a class="markdownIt-Anchor" href="#focal-loss"></a> <strong>Focal Loss</strong></h5>
<blockquote>
<p>Focal loss 在标准交叉熵损失的基础上修改得到的，通过减少易分类样本的权重，使得模型在训练时更专注于难分类的样本</p>
</blockquote>
<p><strong>解耦特征和分类器</strong></p>
<h3 id="3-目标检测中"><a class="markdownIt-Anchor" href="#3-目标检测中"></a> 3 目标检测中</h3>
<p><strong>定义</strong></p>
<ul>
<li>正样本：<strong>标签区域内</strong>的图像区域，即目标图像块</li>
<li>负样本：<strong>标签区域以外</strong>的图像区域，即图像背景区域</li>
<li>易分正样本：容易正确分类的正样本，在实际训练过程中，<strong>该类占总体样本的比重非常高，单个样本的损失函数较小</strong>，但是累计的损失函数会主导损失函数</li>
<li>易分负样本：容易正确分类的负样本，在实际训练过程中，<strong>该类占的比重非常高</strong>，单个样本的损失函数较小，但是累计的损失函数会主导损失函数</li>
<li>难分正样本：错分成负样本的正样本，这部分样本在训练过程中单个样本的损失函数较高，但是<strong>该类占总体样本的比例较小</strong></li>
<li>难分负样本：错分成正样本的负样本，这部分样本在训练过程中单个样本的损失函数教高，但是该类占总体样本的比例教小</li>
</ul>
<img src="/2025/03/14/2025%E5%B9%B43%E6%9C%8814%E6%97%A5-%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%96%B9%E6%B3%95/image-20250314152506435.png" alt="image-20250314152506435" style="zoom:50%;">
<blockquote>
<p>和grand true 的框重叠部分</p>
</blockquote>
<h4 id="1正负样本不均衡"><a class="markdownIt-Anchor" href="#1正负样本不均衡"></a> <strong>1.正负样本不均衡</strong></h4>
<p>以Faster RCNN为例，在RPN部分会生成20000个左右的Anchor，由于一张图中通常有10个左右的物体，<strong>导致可能只有100个左右的Anchor会是正样本，正负样本比例约为1∶200，存在严重的不均衡。</strong></p>
<h4 id="2难易样本不均衡"><a class="markdownIt-Anchor" href="#2难易样本不均衡"></a> <strong>2.难易样本不均衡</strong></h4>
<ol>
<li>难样本指的是<strong>分类不太明确的边框，处在前景与背景的过渡区域上</strong></li>
<li>然而，大量的样本并非处在前景与背景的过渡区，而是**与真实物体没有重叠区域的负样本，或者与真实物体重叠程度很高的正样本，<strong>这部分被称为</strong>简单样本，**单个损失会较小，对参数收敛的作用有限。</li>
</ol>
<h4 id="3类别间样本不均衡"><a class="markdownIt-Anchor" href="#3类别间样本不均衡"></a> <strong>3.类别间样本不均衡</strong></h4>
<p>机器学习中，样本不均衡问题的解决方法</p>
<h1 id="多进程"><a class="markdownIt-Anchor" href="#多进程"></a> 多进程</h1>
<h2 id="几种方法实现"><a class="markdownIt-Anchor" href="#几种方法实现"></a> 几种方法实现：</h2>
<h3 id="多进程创建模型推理"><a class="markdownIt-Anchor" href="#多进程创建模型推理"></a> 多进程创建模型推理</h3>
<p><strong>多进程并行推理</strong></p>
<ol>
<li>创建多个进程：每个进程加载一个YOLOv8模型实例，将待推理的图像或视频流分配给不同的进程并行处理。</li>
<li>进程间通信：使用队列、管道等机制实现进程间的通信和数据交换。例如，可以使用multiprocessing.Queue来传递待处理的图像数据和推理结果。</li>
<li>合理分配任务：根据硬件资源和任务特性，合理分配每个进程的推理任务量，避免资源浪费和性能瓶颈。</li>
</ol>
<p><strong>硬件加速与优化</strong></p>
<ol>
<li>使用GPU加速：将YOLOv8模型部署在GPU上，利用GPU的并行计算能力加速模型推理。可以使用TensorRT等工具对模型进行优化，进一步提升推理效率。</li>
<li>模型优化：对YOLOv8模型进行量化、剪枝等优化操作，减少模型的计算复杂度和参数量，从而提高推理速度。</li>
<li>前后处理加速：优化图像的预处理和后处理步骤，例如使用内存0拷贝技术、快速的解码和编码算法等，减少数据传输和处理的时间。</li>
</ol>
<p><strong>多线程辅助</strong></p>
<ol>
<li>多线程数据读写：在多进程推理的基础上，可以使用多线程来辅助数据的读写操作，提高数据处理的效率。例如，使用线程池来维护多个线程，每个线程负责读取或写入一部分数据。</li>
<li>界面与推理分离：如果需要在GUI界面中进行YOLOv8推理，可以将推理任务放在单独的线程中执行，避免阻塞界面线程，提高用户体验。</li>
</ol>
<p>原文链接：<a target="_blank" rel="noopener" href="https://blog.csdn.net/zhiqingAI/article/details/145017566">https://blog.csdn.net/zhiqingAI/article/details/145017566</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Process, Queue</span><br><span class="line"><span class="keyword">from</span> ultralytics <span class="keyword">import</span> YOLO</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">worker</span>(<span class="params">q, model_path</span>):</span><br><span class="line">    model = YOLO(model_path)</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        img = q.get()</span><br><span class="line">        <span class="keyword">if</span> img <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        results = model.predict(source=img)</span><br><span class="line">        <span class="comment"># 处理推理结果</span></span><br><span class="line">        <span class="built_in">print</span>(results)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    q = Queue()</span><br><span class="line">    model_path = <span class="string">&#x27;yolov8n.pt&#x27;</span></span><br><span class="line">    num_processes = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建多个进程</span></span><br><span class="line">    processes = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_processes):</span><br><span class="line">        p = Process(target=worker, args=(q, model_path))</span><br><span class="line">        p.start()</span><br><span class="line">        processes.append(p)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将图像数据放入队列</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        img_path = <span class="string">f&#x27;image_<span class="subst">&#123;i&#125;</span>.jpg&#x27;</span></span><br><span class="line">        img = cv2.imread(img_path)</span><br><span class="line">        q.put(img)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 结束进程</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_processes):</span><br><span class="line">        q.put(<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> processes:</span><br><span class="line">        p.join()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="多线程同时访问相同模型"><a class="markdownIt-Anchor" href="#多线程同时访问相同模型"></a> 多线程同时访问相同模型</h3>
<p>多线程推理函数</p>
<h4 id="1-解决图像接收端与推理端速度不匹配的问题"><a class="markdownIt-Anchor" href="#1-解决图像接收端与推理端速度不匹配的问题"></a> 1. 解决图像接收端与推理端速度不匹配的问题</h4>
<p>当出现生产者生产太快，而消费太慢时：会让图片发生堆积，一张图片可能有2MB，如果长期运行，就会发现，程序可能在某一刻发生崩溃（内存消耗殆尽）</p>
<h3 id="检测与识别并行"><a class="markdownIt-Anchor" href="#检测与识别并行"></a> 检测与识别并行</h3>
<h4 id="一张图片经多个模型推理的异步模式设计"><a class="markdownIt-Anchor" href="#一张图片经多个模型推理的异步模式设计"></a> 一张图片，经多个模型推理的异步模式设计</h4>
<p>参考:<a target="_blank" rel="noopener" href="https://blog.csdn.net/hh1357102/article/details/127586846%5C">https://blog.csdn.net/hh1357102/article/details/127586846\</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://shakewely.github.io/2025/03/14/2025%E5%B9%B43%E6%9C%8814%E6%97%A5-%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%96%B9%E6%B3%95/" data-id="cmanj8o220009lcv41hue4oqi" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/dl/" rel="tag">dl</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-2025年3月15日-模型评价指标" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2025/03/14/2025%E5%B9%B43%E6%9C%8815%E6%97%A5-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/" class="article-date">
  <time class="post-time" datetime="2025-03-13T16:26:50.000Z" itemprop="datePublished">
    <span class="post-month">3月</span><br/>
    <span class="post-day">14</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/03/14/2025%E5%B9%B43%E6%9C%8815%E6%97%A5-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/">2025年3月15日 模型评价指标</a>
    </h1>
  

        <div>
          
          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="计算量与参数量"><a class="markdownIt-Anchor" href="#计算量与参数量"></a> 计算量与参数量</h1>
<h2 id="1-定义"><a class="markdownIt-Anchor" href="#1-定义"></a> 1 定义</h2>
<ol>
<li>计算量-时间复杂度  <strong>乘法和加发的操作次数</strong></li>
<li>参数量-空间复杂度</li>
</ol>
<h3 id="卷积层"><a class="markdownIt-Anchor" href="#卷积层"></a> 卷积层</h3>
<p><strong>计算量</strong>: (KxKxWxH) x C_in x C_out</p>
<blockquote>
<p>W\H: 输出feature map 的size</p>
<p>回忆卷积操作过程</p>
</blockquote>
<p><strong>参数量</strong>: KxKxC_inxC_out</p>
<blockquote>
<p>卷积核的weight</p>
</blockquote>
<h3 id="池化层"><a class="markdownIt-Anchor" href="#池化层"></a> 池化层</h3>
<p><strong>计算量</strong>: 不同池化操作不同 Cin x Cout x W x H x K xK</p>
<blockquote></blockquote>
<p><strong>参数量</strong>  无</p>
<h3 id="全连接层"><a class="markdownIt-Anchor" href="#全连接层"></a> 全连接层</h3>
<p>参数量 weight_in*weight_out</p>
<p>计算量＝ 2 x Cin x Cout</p>
<h3 id="batchnorm"><a class="markdownIt-Anchor" href="#batchnorm"></a> BatchNorm</h3>
<p>计算量: 4WHC</p>
<blockquote></blockquote>
<p>参数量: 2C</p>
<blockquote>
<p>(y=ax+b) xC</p>
</blockquote>
<h3 id="激活函数"><a class="markdownIt-Anchor" href="#激活函数"></a> 激活函数</h3>
<p>计算量: H×W×C</p>
<blockquote>
<p>path个数</p>
</blockquote>
<h2 id="硬件需求"><a class="markdownIt-Anchor" href="#硬件需求"></a> 硬件需求</h2>
<p>计算量的要求是在于芯片的__（指的是gpu的运算能力）</p>
<p>参数量取决于__大小</p>
<h3 id="计算量和参数量查看"><a class="markdownIt-Anchor" href="#计算量和参数量查看"></a> 计算量和参数量查看</h3>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_40507857/article/details/118764782">https://blog.csdn.net/qq_40507857/article/details/118764782</a></p>
<h3 id="输入数据对模型的参数量和计算量的影响"><a class="markdownIt-Anchor" href="#输入数据对模型的参数量和计算量的影响"></a> 输入数据对模型的参数量和计算量的影响</h3>
<h1 id="tf-fp-fn-tn"><a class="markdownIt-Anchor" href="#tf-fp-fn-tn"></a> TF FP FN TN</h1>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/wzk4869/article/details/127879761">https://blog.csdn.net/wzk4869/article/details/127879761</a></p>
<img src="https://picx.zhimg.com/70/v2-895d6f8e80c0078c92244158dec97bfc_1440w.image?source=172ae18b&biz_tag=Post" alt="深挖一下F1 score (F-measure, F-score)[根据公式分析]" style="zoom:50%;">
<h2 id="1-准确率precision"><a class="markdownIt-Anchor" href="#1-准确率precision"></a> 1 准确率（Precision）</h2>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="2-召回率-recall"><a class="markdownIt-Anchor" href="#2-召回率-recall"></a> 2 召回率 (Recall)</h2>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="目标检测的评价指标"><a class="markdownIt-Anchor" href="#目标检测的评价指标"></a> 目标检测的评价指标</h1>
<h2 id="1-iou"><a class="markdownIt-Anchor" href="#1-iou"></a> 1: IoU</h2>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="2-f1-score"><a class="markdownIt-Anchor" href="#2-f1-score"></a> 2: F1 Score</h2>
<p><strong>定义</strong>: F Score 是 Precision 和 Recall 的调和平均数 (harmonic mean)</p>
<p><img src="/2025/03/14/2025%E5%B9%B43%E6%9C%8815%E6%97%A5-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/image-20250314011150884.png" alt="image-20250314011150884"></p>
<blockquote>
<p>Precision 与 Recall 看起来很相似，实际上这两个是“冤家”。为什么这么说呢？因为在大多数情况下，这两者有一定的互斥性。****</p>
</blockquote>
<p><strong>作用:</strong></p>
<h2 id="平均准确度-average-precisionap"><a class="markdownIt-Anchor" href="#平均准确度-average-precisionap"></a> 平均准确度 (Average Precision，AP)</h2>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://shakewely.github.io/2025/03/14/2025%E5%B9%B43%E6%9C%8815%E6%97%A5-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/" data-id="cm8q1tfhp000xpcv44db75i16" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/dl/" rel="tag">dl</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-2025年3月12日-onnx5-算子" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2025/03/12/2025%E5%B9%B43%E6%9C%8812%E6%97%A5-onnx5-%E7%AE%97%E5%AD%90/" class="article-date">
  <time class="post-time" datetime="2025-03-12T14:42:57.000Z" itemprop="datePublished">
    <span class="post-month">3月</span><br/>
    <span class="post-day">12</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/03/12/2025%E5%B9%B43%E6%9C%8812%E6%97%A5-onnx5-%E7%AE%97%E5%AD%90/">2025年3月12日 onnx问答</a>
    </h1>
  

        <div>
          
          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>参考：<a target="_blank" rel="noopener" href="https://blog.csdn.net/zhaoyqcsdn/article/details/135512992%E3%80%81">https://blog.csdn.net/zhaoyqcsdn/article/details/135512992、</a></p>
<h1 id="1-算子不兼容"><a class="markdownIt-Anchor" href="#1-算子不兼容"></a> 1 算子不兼容</h1>
<p>onnx算子是否支持<br>
<a target="_blank" rel="noopener" href="https://github.com/onnx/onnx/blob/main/docs/Operators.md">https://github.com/onnx/onnx/blob/main/docs/Operators.md</a></p>
<h2 id="11-pytorch中有onnx中也有的算子"><a class="markdownIt-Anchor" href="#11-pytorch中有onnx中也有的算子"></a> 1.1 pytorch中有，onnx中也有的算子</h2>
<h2 id="12-pytorch中有onnx中无的算子"><a class="markdownIt-Anchor" href="#12-pytorch中有onnx中无的算子"></a> 1.2 pytorch中有，onnx中无的算子</h2>
<p>继承<code>torch.autograd.Function</code>实现自定义算子</p>
<h3 id="1-案例示例-focus层的切片操作"><a class="markdownIt-Anchor" href="#1-案例示例-focus层的切片操作"></a> <strong>1 案例示例</strong>： <code>Focus</code>层的切片操作</h3>
<blockquote>
<p>通过其他变换来改成兼容的算子</p>
</blockquote>
<p>在目标检测项目中，将PyTorch训练的YOLOv5模型转换为ONNX时，需处理<code>Focus</code>层的切片操作兼容性问题，通过重构为<code>Conv</code>层解决，将size减少一半通道数增加一倍</p>
<h3 id="2-实际案例动态切片问题"><a class="markdownIt-Anchor" href="#2-实际案例动态切片问题"></a> 2 <strong>实际案例：动态切片问题</strong></h3>
<p>静态图无法处理随机性</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 原始PyTorch代码（无法导出）</span><br><span class="line">x_slice = x[:, :, start_idx:start_idx+10]</span><br><span class="line"></span><br><span class="line"># 修改为ONNX兼容方式</span><br><span class="line">indices = torch.arange(start_idx, start_idx+10, device=x.device)</span><br><span class="line">x_slice = torch.index_select(x, dim=2, index=indices)</span><br></pre></td></tr></table></figure>
<h3 id="3-案例示例创建新的算子-手动实现"><a class="markdownIt-Anchor" href="#3-案例示例创建新的算子-手动实现"></a> 3 案例示例：创建新的算子 手动实现</h3>
<blockquote>
<p>使用支持的操作来手动实现算子</p>
</blockquote>
<ul>
<li>
<p>****：部分框架特定操作（如PyTorch自定义层）需替换为ONNX标准算子或自定义实现。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.onnx</span><br><span class="line"><span class="keyword">from</span> torch.onnx <span class="keyword">import</span> register_custom_op_symbolic</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 定义自定义算子</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomRelu</span>(torch.autograd.Function):</span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">ctx, <span class="built_in">input</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;前向传播：自定义的ReLU操作&quot;&quot;&quot;</span></span><br><span class="line">        ctx.save_for_backward(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> torch.maximum(<span class="built_in">input</span>, torch.tensor(<span class="number">0.5</span>))  <span class="comment"># 比普通ReLU多了一个偏移</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">ctx, grad_output</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;反向传播：ReLU的梯度计算&quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">input</span>, = ctx.saved_tensors</span><br><span class="line">        grad_input = grad_output.clone()</span><br><span class="line">        grad_input[<span class="built_in">input</span> &lt; <span class="number">0.5</span>] = <span class="number">0</span>  <span class="comment"># 低于0.5的部分梯度置零</span></span><br><span class="line">        <span class="keyword">return</span> grad_input</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 创建 ONNX 计算图中的算子</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">symbolic_custom_relu</span>(<span class="params">g, <span class="built_in">input</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;定义 ONNX 计算图中的 custom_relu&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> g.op(<span class="string">&quot;custom_domain::CustomRelu&quot;</span>, <span class="built_in">input</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 绑定 ONNX 导出</span></span><br><span class="line">register_custom_op_symbolic(<span class="string">&quot;::CustomRelu&quot;</span>, symbolic_custom_relu, <span class="number">11</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 使用自定义算子</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomModel</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> CustomRelu.apply(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 实例化模型</span></span><br><span class="line">model = CustomModel()</span><br><span class="line">x = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6. 导出为 ONNX（确保 opset 版本支持）</span></span><br><span class="line">torch.onnx.export(</span><br><span class="line">    model, x, <span class="string">&quot;custom_relu.onnx&quot;</span>,</span><br><span class="line">    opset_version=<span class="number">11</span>,</span><br><span class="line">    verbose=<span class="literal">True</span>,</span><br><span class="line">    operator_export_type=torch.onnx.OperatorExportTypes.ONNX</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="问题动态batch或可变输入尺寸导致转换失败"><a class="markdownIt-Anchor" href="#问题动态batch或可变输入尺寸导致转换失败"></a> <strong>问题</strong>：动态Batch或可变输入尺寸导致转换失败。</h2>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">torch.onnx.export(</span><br><span class="line">    model, dummy_input, &quot;yolov5_dynamic.onnx&quot;,</span><br><span class="line">    opset_version=11,</span><br><span class="line">    input_names=[&quot;input&quot;], output_names=[&quot;output&quot;],</span><br><span class="line">    dynamic_axes=&#123;</span><br><span class="line">        &quot;input&quot;: &#123;0: &quot;batch_size&quot;&#125;,   # 让 input 的 batch 维度 (dim=0) 变为动态</span><br><span class="line">        &quot;output&quot;: &#123;0: &quot;batch_size&quot;&#125;   # 让 output 的 batch 维度 (dim=0) 变为动态</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h1 id="2-写占位符号替换乱七八糟的算子"><a class="markdownIt-Anchor" href="#2-写占位符号替换乱七八糟的算子"></a> 2 写占位符号替换乱七八糟的算子</h1>
<p><img src="/2025/03/12/2025%E5%B9%B43%E6%9C%8812%E6%97%A5-onnx5-%E7%AE%97%E5%AD%90/image-20250315101551480.png" alt="image-20250315101551480" style="zoom: 50%;"> <img src="/2025/03/12/2025%E5%B9%B43%E6%9C%8812%E6%97%A5-onnx5-%E7%AE%97%E5%AD%90/image-20250315101542696.png"></p>
<h2 id="代码怎么写"><a class="markdownIt-Anchor" href="#代码怎么写"></a> 代码怎么写</h2>
<h3 id="todo"><a class="markdownIt-Anchor" href="#todo"></a> TODO</h3>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://shakewely.github.io/2025/03/12/2025%E5%B9%B43%E6%9C%8812%E6%97%A5-onnx5-%E7%AE%97%E5%AD%90/" data-id="cmanj8o23000dlcv4dnxrcuh0" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/onnx/" rel="tag">onnx</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-2025年3月12日-onnx4-图优化" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2025/03/12/2025%E5%B9%B43%E6%9C%8812%E6%97%A5-onnx4-%E5%9B%BE%E4%BC%98%E5%8C%96/" class="article-date">
  <time class="post-time" datetime="2025-03-12T14:14:32.000Z" itemprop="datePublished">
    <span class="post-month">3月</span><br/>
    <span class="post-day">12</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/03/12/2025%E5%B9%B43%E6%9C%8812%E6%97%A5-onnx4-%E5%9B%BE%E4%BC%98%E5%8C%96/">2025年3月12日 onnx3</a>
    </h1>
  

        <div>
          
          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="图优化"><a class="markdownIt-Anchor" href="#图优化"></a> 图优化</h1>
<h3 id="1"><a class="markdownIt-Anchor" href="#1"></a> 1</h3>
<p><img src="/2025/03/12/2025%E5%B9%B43%E6%9C%8812%E6%97%A5-onnx4-%E5%9B%BE%E4%BC%98%E5%8C%96/image-20250312222559294.png" alt="image-20250312222559294"></p>
<p><strong>引擎能够针对特定硬件进行深度优化，实现最佳的性能和效率</strong></p>
<p><img src="/2025/03/12/2025%E5%B9%B43%E6%9C%8812%E6%97%A5-onnx4-%E5%9B%BE%E4%BC%98%E5%8C%96/image-20250312224327684.png" alt="image-20250312224327684"></p>
<h3 id="3-查看netron节点"><a class="markdownIt-Anchor" href="#3-查看netron节点"></a> 3 查看netron节点</h3>
<p>ONNX的optimizer模块提供部分图优化的功能，例如最常用的：fuse_bn_into_conv，fuse_pad_into_conv等等。</p>
<p><img src="/2025/03/12/2025%E5%B9%B43%E6%9C%8812%E6%97%A5-onnx4-%E5%9B%BE%E4%BC%98%E5%8C%96/image-20250312232520596.png" alt="image-20250312232520596"></p>
<h3 id="2-为什么需要forward_fuse"><a class="markdownIt-Anchor" href="#2-为什么需要forward_fuse"></a> 2. 为什么需要forward_fuse？</h3>
<ul>
<li>
<p>forward_fuse通常用于把卷积和BN层合并，或者把多个卷积分支合并，减少计算量，提高推理速度。</p>
</li>
<li>
<p>如果你的模块没有BN层，或者结构很简单，不需要合并，也应该定义一个forward_fuse，让它和forward一样。</p>
</li>
</ul>
<h3 id="4-onnxslim-模块未安装简化失败"><a class="markdownIt-Anchor" href="#4-onnxslim-模块未安装简化失败"></a> <code>4 onnxslim</code> 模块未安装，简化失败</h3>
<h4 id="简化-onnx-的主要作用"><a class="markdownIt-Anchor" href="#简化-onnx-的主要作用"></a> ✅ 简化 ONNX 的主要作用：</h4>
<table>
<thead>
<tr>
<th>好处</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>📦 减少模型大小</td>
<td>去除冗余算子与中间变量，减小 <code>.onnx</code> 文件大小</td>
</tr>
<tr>
<td>🚀 加速推理速度</td>
<td>简化计算图后，ONNX Runtime / TensorRT 推理更快</td>
</tr>
<tr>
<td>🔧 提高兼容性</td>
<td>某些部署平台（如 OpenVINO、MNN）对简化后的模型兼容性更强</td>
</tr>
<tr>
<td>🧩 易于可视化分析</td>
<td>模型结构更清晰，利于在 Netron 中查看和理解</td>
</tr>
</tbody>
</table>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://shakewely.github.io/2025/03/12/2025%E5%B9%B43%E6%9C%8812%E6%97%A5-onnx4-%E5%9B%BE%E4%BC%98%E5%8C%96/" data-id="cmanj8o210008lcv42fxu34el" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/onnx/" rel="tag">onnx</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-2025年3月11日-yolov5-5-0训练" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2025/03/11/2025%E5%B9%B43%E6%9C%8811%E6%97%A5-yolov5-5-0%E8%AE%AD%E7%BB%83/" class="article-date">
  <time class="post-time" datetime="2025-03-11T03:59:58.000Z" itemprop="datePublished">
    <span class="post-month">3月</span><br/>
    <span class="post-day">11</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/03/11/2025%E5%B9%B43%E6%9C%8811%E6%97%A5-yolov5-5-0%E8%AE%AD%E7%BB%83/">2025年3月11日 yolov5 5.0训练</a>
    </h1>
  

        <div>
          
          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="sad"><a class="markdownIt-Anchor" href="#sad"></a> sad</h2>
<h2 id="报错"><a class="markdownIt-Anchor" href="#报错"></a> 报错</h2>
<p>库的版本要匹配上 否则会出现不确定错误</p>
<ol>
<li>
<p>pydantic_core._pydantic_core.ValidationError: 1 validation error for Settings<br>
anonymous<br>
Input should be ‘allow’, ‘must’ or ‘never’ [type=literal_error, input_value=‘true’, input_type=str]<br>
For further information visit <a target="_blank" rel="noopener" href="https://errors.pydantic.dev/2.10/v/literal_error">https://errors.pydantic.dev/2.10/v/literal_error</a></p>
<p>建议：wandb 下降版本<code>pip install wandb==0.12.10</code></p>
</li>
<li>
<p>You are using <code>torch.load</code> with <code>weights_only=False</code> (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling</p>
<p>修改代码中加载模型权重的部分</p>
<p><code> run_id = torch.load(weights, weights_only=True).get('wandb_id')  # 添加weights_only=True</code></p>
</li>
<li>
<p>File “F:\Pydata\DL_NEW\YOLO\yolov5-5.0\utils\datasets.py”, line 411, in <strong>init</strong><br>
bi = np.floor(np.arange(n) / batch_size).astype(np.int)  # batch index</p>
</li>
</ol>
<p><code>pip install numpy==1.22.4       # 部分新库兼容的中间版本</code></p>
<ol start="4">
<li>
<p>TypeError: Descriptors cannot be created directly.<br>
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc &gt;= 3.19.0.<br>
If you cannot immediately regenerate your protos, some other possible workarounds are:</p>
<ol>
<li>Downgrade the protobuf package to 3.20.x or lower.</li>
<li>Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).</li>
</ol>
<p><code>pip install protobuf==3.20.1 </code></p>
</li>
<li>
<p>是</p>
</li>
</ol>
<p>代码出现的问题:</p>
<ol>
<li>
<p>手动添加SSF层</p>
</li>
<li>
<p>缺少logger</p>
<p>手动添加 <code>logger = logging.getLogger(__name__)</code></p>
</li>
<li></li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://shakewely.github.io/2025/03/11/2025%E5%B9%B43%E6%9C%8811%E6%97%A5-yolov5-5-0%E8%AE%AD%E7%BB%83/" data-id="cm8q1tfhp000upcv4f52t1sn8" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/yolo/" rel="tag">yolo</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-2025年3月10日-onnx3进阶使用与常见问题" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2025/03/10/2025%E5%B9%B43%E6%9C%8810%E6%97%A5-onnx3%E8%BF%9B%E9%98%B6%E4%BD%BF%E7%94%A8%E4%B8%8E%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/" class="article-date">
  <time class="post-time" datetime="2025-03-10T13:15:13.000Z" itemprop="datePublished">
    <span class="post-month">3月</span><br/>
    <span class="post-day">10</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/03/10/2025%E5%B9%B43%E6%9C%8810%E6%97%A5-onnx3%E8%BF%9B%E9%98%B6%E4%BD%BF%E7%94%A8%E4%B8%8E%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/">2025年3月10日 onnx2</a>
    </h1>
  

        <div>
          
          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ul>
<li></li>
</ul>
<h2 id="导出报错"><a class="markdownIt-Anchor" href="#导出报错"></a> 导出报错</h2>
<p>Unsupported: ONNX export of convolution for kernel of unknown shape”，这通常与动态形状或无法推断的卷积核尺寸有</p>
<ol>
<li>
<p>静态化Reshape操作</p>
</li>
<li>
<p>修复多级融合的TracerWarning fused_cls = sum(w * c for w, c in zip(weights_cls, preds_cls))</p>
<p><code>sum(w * c for w, c in zip(...))</code>中的迭代操作会被PyTorch的JIT Tracer识别为动态控制流，导致ONNX导出时无法生成静态计算图。这种Python层的迭代操作在模型跟踪阶段会被视为潜在动态行为，从而触发TracerWarning</p>
</li>
</ol>
<h3 id="2-importerror-dll-load-failed-while-importing-onnx_cpp2py_export-动态链接库dll初始化例程失败"><a class="markdownIt-Anchor" href="#2-importerror-dll-load-failed-while-importing-onnx_cpp2py_export-动态链接库dll初始化例程失败"></a> 2 ImportError: DLL load failed while importing onnx_cpp2py_export: 动态链接库(DLL)初始化例程失败。</h3>
<blockquote>
<p>重新安装 onnx</p>
</blockquote>
<h2 id="结构报错"><a class="markdownIt-Anchor" href="#结构报错"></a> 结构报错</h2>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="1-为什么导出的onnx会有那么多identity这是什么怎么消除"><a class="markdownIt-Anchor" href="#1-为什么导出的onnx会有那么多identity这是什么怎么消除"></a> 1 为什么导出的onnx会有那么多Identity，这是什么，怎么消除</h3>
<p><img src="/2025/03/10/2025%E5%B9%B43%E6%9C%8810%E6%97%A5-onnx3%E8%BF%9B%E9%98%B6%E4%BD%BF%E7%94%A8%E4%B8%8E%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/image-20250310215555781.png" alt="image-20250310215555781"></p>
<ol>
<li><strong>数据传递占位符</strong> Identity节点的核心功能是将输入张量原样输出，不进行任何计算</li>
<li><strong>框架转换机制缺陷</strong></li>
<li><strong>自定义算子副作用</strong></li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://shakewely.github.io/2025/03/10/2025%E5%B9%B43%E6%9C%8810%E6%97%A5-onnx3%E8%BF%9B%E9%98%B6%E4%BD%BF%E7%94%A8%E4%B8%8E%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/" data-id="cmanj8o1s0006lcv45hyj3669" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/onnx/" rel="tag">onnx</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-2025年3月10日-C-记录" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2025/03/10/2025%E5%B9%B43%E6%9C%8810%E6%97%A5-C-%E8%AE%B0%E5%BD%95/" class="article-date">
  <time class="post-time" datetime="2025-03-10T05:29:15.000Z" itemprop="datePublished">
    <span class="post-month">3月</span><br/>
    <span class="post-day">10</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/03/10/2025%E5%B9%B43%E6%9C%8810%E6%97%A5-C-%E8%AE%B0%E5%BD%95/">2025年3月10日 C++记录</a>
    </h1>
  

        <div>
          
          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="c"><a class="markdownIt-Anchor" href="#c"></a> ✨C++</h1>
<h2 id="基础知识"><a class="markdownIt-Anchor" href="#基础知识"></a> 基础知识</h2>
<h3 id="1-变量作用域"><a class="markdownIt-Anchor" href="#1-变量作用域"></a> 1 变量作用域</h3>
<ol>
<li>什么是局部变量 ☑️</li>
<li>什么是全局变量 ☑️</li>
<li>什么是块作用域，如何实现？</li>
<li>类？</li>
</ol>
<h3 id="2-常量"><a class="markdownIt-Anchor" href="#2-常量"></a> 2 常量？</h3>
<ol>
<li>定义？</li>
<li>如何定义常量</li>
</ol>
<h3 id="3-修饰符"><a class="markdownIt-Anchor" href="#3-修饰符"></a> 3 修饰符</h3>
<ol>
<li>
<p>修饰符类型</p>
</li>
<li>
<p>类型限定符：是什么？</p>
<table>
<thead>
<tr>
<th>static</th>
<th>用于定义静态变量，表示该变量的作用域仅限于当前文件或当前函数内，不会被其他文件或函数访问。</th>
</tr>
</thead>
<tbody>
<tr>
<td>const</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</li>
<li></li>
</ol>
<h3 id="4-c-存储类"><a class="markdownIt-Anchor" href="#4-c-存储类"></a> 4 C++ 存储类</h3>
<p>控制变量和函数生命周期及可见性的手段。？？？</p>
<ul>
<li><strong>extern</strong> 都有什么用？全局变量，具有外部链接，默认存储类为extern</li>
<li><strong>auto</strong></li>
<li><strong>static</strong></li>
</ul>
<h3 id="5-运算符"><a class="markdownIt-Anchor" href="#5-运算符"></a> 5 运算符</h3>
<ul>
<li>
<p>逻辑运算符</p>
</li>
<li>
<h4 id="运算符优先级"><a class="markdownIt-Anchor" href="#运算符优先级"></a> 运算符优先级</h4>
</li>
</ul>
<h3 id="6-lambda-函数与表达式"><a class="markdownIt-Anchor" href="#6-lambda-函数与表达式"></a> 6 Lambda 函数与表达式</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[](int x, int y) -&gt; int &#123; int z = x + y; return z + x; &#125;</span><br></pre></td></tr></table></figure>
<p>递归+lamda</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">auto dfs = [&amp;](this auto&amp;&amp; dfs, int i, int j) -&gt; int &#123;   // 递归逻辑  &#125;;</span><br></pre></td></tr></table></figure>
<h3 id="7-c-引用"><a class="markdownIt-Anchor" href="#7-c-引用"></a> 7 C++ 引用</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">int a = 10;</span><br><span class="line">int &amp;ref = a;  // ref 是 a 的引用</span><br></pre></td></tr></table></figure>
<ul>
<li><code>int &amp;ref</code> 表示 <code>ref</code> 是一个 <code>int</code> 类型的引用。</li>
<li><code>ref</code> 是 <code>a</code> 的别名，对 <code>ref</code> 的操作会直接作用于 <code>a</code>。</li>
</ul>
<h2 id="code-细节"><a class="markdownIt-Anchor" href="#code-细节"></a> code 细节</h2>
<h4 id="1-越界"><a class="markdownIt-Anchor" href="#1-越界"></a> 1 越界</h4>
<ul>
<li><strong>先判断空栈，再取栈顶元素</strong>。<code>if(val &lt; min_stack.back() || min_stack.empty())</code></li>
</ul>
<h2 id="std"><a class="markdownIt-Anchor" href="#std"></a> std</h2>
<h3 id="1-map-和-set"><a class="markdownIt-Anchor" href="#1-map-和-set"></a> 1 map 和 set</h3>
<table>
<thead>
<tr>
<th>特点</th>
<th>unordered_map</th>
</tr>
</thead>
<tbody>
<tr>
<td>存储</td>
<td>键值对 (key, value)</td>
</tr>
<tr>
<td>使用场景</td>
<td>需要存储键值对，例如统计频率，映射关系</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>函数</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>map[key]</code></td>
<td>访问 / 插入元素，若 key 不存在会默认插入一个默认值</td>
</tr>
<tr>
<td><code>map.at(key)</code></td>
<td>访问元素，key 不存在会抛异常</td>
</tr>
<tr>
<td><code>map.insert(&#123;key, value&#125;)</code></td>
<td>插入键值对</td>
</tr>
<tr>
<td><code>map.emplace(key, value)</code></td>
<td>就地构造，效率比 <code>insert</code> 更高</td>
</tr>
<tr>
<td><code>map.erase(key)</code></td>
<td>删除键为 <code>key</code> 的元素</td>
</tr>
<tr>
<td><code>map.clear()</code></td>
<td>清空所有元素</td>
</tr>
<tr>
<td><code>map.count(key)</code></td>
<td>判断 key 是否存在，存在返回 1，不存在返回 0</td>
</tr>
<tr>
<td><code>map.find(key)</code></td>
<td>查找 key，返回迭代器，等于 <code>map.end()</code> 表示没找到</td>
</tr>
<tr>
<td><code>map.size()</code></td>
<td>返回元素个数</td>
</tr>
<tr>
<td><code>map.empty()</code></td>
<td>判断是否为空</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>函数</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>set.insert(key)</code></td>
<td>插入元素</td>
</tr>
<tr>
<td><code>set.emplace(key)</code></td>
<td>就地构造，效率更高</td>
</tr>
<tr>
<td><code>set.erase(key)</code></td>
<td>删除元素</td>
</tr>
<tr>
<td><code>set.clear()</code></td>
<td>清空元素</td>
</tr>
<tr>
<td><code>set.count(key)</code></td>
<td>判断元素是否存在，存在返回 1</td>
</tr>
<tr>
<td><code>set.find(key)</code></td>
<td>查找元素，返回迭代器</td>
</tr>
<tr>
<td><code>set.size()</code></td>
<td>返回元素个数</td>
</tr>
<tr>
<td><code>set.empty()</code></td>
<td>判断是否为空</td>
</tr>
</tbody>
</table>
<h3 id="2-vector"><a class="markdownIt-Anchor" href="#2-vector"></a> 2 vector</h3>
<table>
<thead>
<tr>
<th>功能类别</th>
<th>函数名</th>
<th>作用描述</th>
<th>时间复杂度</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>插入</strong></td>
<td><code>push_back(val)</code></td>
<td>在 vector 尾部插入元素 <code>val</code></td>
<td><strong>摊销 O(1)</strong></td>
</tr>
<tr>
<td><strong>删除</strong></td>
<td><code>pop_back()</code></td>
<td>删除 vector 尾部元素</td>
<td><strong>O(1)</strong></td>
</tr>
<tr>
<td><strong>访问</strong></td>
<td><code>back()</code></td>
<td>返回 vector 的最后一个元素（不删除）</td>
<td><strong>O(1)</strong></td>
</tr>
<tr>
<td></td>
<td><code>front()</code></td>
<td>返回 vector 的第一个元素（不删除）</td>
<td><strong>O(1)</strong></td>
</tr>
<tr>
<td></td>
<td><code>at(index)</code></td>
<td>返回指定下标元素，带边界检查</td>
<td><strong>O(1)</strong></td>
</tr>
<tr>
<td></td>
<td><code>operator[]</code></td>
<td>返回指定下标元素（不带边界检查）</td>
<td><strong>O(1)</strong></td>
</tr>
<tr>
<td><strong>状态</strong></td>
<td><code>empty()</code></td>
<td>检查 vector 是否为空</td>
<td><strong>O(1)</strong></td>
</tr>
<tr>
<td></td>
<td><code>size()</code></td>
<td>返回 vector 中元素个数</td>
<td><strong>O(1)</strong></td>
</tr>
<tr>
<td><strong>清空</strong></td>
<td><code>clear()</code></td>
<td>清空 vector 所有元素</td>
<td><strong>O(n)</strong></td>
</tr>
<tr>
<td><strong>其他</strong></td>
<td><code>resize(n)</code></td>
<td>调整 vector 大小为 <code>n</code>，默认填充值为零</td>
<td><strong>O(n)</strong></td>
</tr>
<tr>
<td></td>
<td><code>reserve(n)</code></td>
<td>预留空间，避免多次扩容</td>
<td><strong>O(n)</strong></td>
</tr>
</tbody>
</table>
<h3 id="3-字符串操作函数"><a class="markdownIt-Anchor" href="#3-字符串操作函数"></a> 3、字符串操作函数</h3>
<table>
<thead>
<tr>
<th>函数</th>
<th>功能</th>
<th>示例</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>s.length()</code> / <code>s.size()</code></td>
<td>字符串长度</td>
<td></td>
</tr>
<tr>
<td><code>s.substr(pos, len)</code></td>
<td>子串</td>
<td><code>s.substr(0, 3)</code></td>
</tr>
<tr>
<td><code>s.find(sub)</code></td>
<td>查找子串位置</td>
<td><code>s.find(&quot;abc&quot;)</code></td>
</tr>
<tr>
<td><code>s.rfind(sub)</code></td>
<td>反向查找子串位置</td>
<td></td>
</tr>
<tr>
<td><code>s.erase(pos, len)</code></td>
<td>删除子串</td>
<td></td>
</tr>
<tr>
<td><code>s.insert(pos, sub)</code></td>
<td>插入子串</td>
<td></td>
</tr>
<tr>
<td><code>s.replace(pos, len, sub)</code></td>
<td>替换子串</td>
<td></td>
</tr>
<tr>
<td><code>reverse(s.begin(), s.end())</code></td>
<td>字符串反转</td>
<td></td>
</tr>
<tr>
<td><code>sort(s.begin(), s.end())</code></td>
<td>字符串排序</td>
<td></td>
</tr>
<tr>
<td><code>toupper(ch)</code> / <code>tolower(ch)</code></td>
<td>字符大小写转换</td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="4-数字cmath"><a class="markdownIt-Anchor" href="#4-数字cmath"></a> 4、数字（<cmath>`）</cmath></h3>
<table>
<thead>
<tr>
<th>函数</th>
<th>功能</th>
<th>示例</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>abs(x)</code></td>
<td>绝对值</td>
<td><code>abs(-3) // 3</code></td>
</tr>
<tr>
<td><code>pow(a, b)</code></td>
<td>幂</td>
<td><code>pow(2, 3) // 8</code></td>
</tr>
<tr>
<td><code>sqrt(x)</code></td>
<td>开方</td>
<td><code>sqrt(16) // 4</code></td>
</tr>
<tr>
<td><code>log(x)</code></td>
<td>自然对数</td>
<td></td>
</tr>
<tr>
<td><code>log10(x)</code></td>
<td>以 10 为底的对数</td>
<td></td>
</tr>
<tr>
<td><code>ceil(x)</code></td>
<td>向上取整</td>
<td><code>ceil(2.3) // 3</code></td>
</tr>
<tr>
<td><code>floor(x)</code></td>
<td>向下取整</td>
<td><code>floor(2.7) // 2</code></td>
</tr>
<tr>
<td><code>round(x)</code></td>
<td>四舍五入</td>
<td><code>round(2.5) // 3</code></td>
</tr>
<tr>
<td><code>max(a, b)</code> / <code>min(a, b)</code></td>
<td>最大/最小值</td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="5-算法algorithm"><a class="markdownIt-Anchor" href="#5-算法algorithm"></a> 5 算法（<code>&lt;algorithm&gt;</code>）</h3>
<table>
<thead>
<tr>
<th>函数</th>
<th>功能</th>
<th>示例</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>sort(start, end)</code></td>
<td>排序</td>
<td></td>
</tr>
<tr>
<td><code>reverse(start, end)</code></td>
<td>反转</td>
<td></td>
</tr>
<tr>
<td><code>next_permutation(start, end)</code></td>
<td>下一个排列</td>
<td></td>
</tr>
<tr>
<td><code>prev_permutation(start, end)</code></td>
<td>上一个排列</td>
<td></td>
</tr>
<tr>
<td><code>lower_bound(start, end, val)</code></td>
<td>二分查找，返回 ≥val 的首元素迭代器</td>
<td></td>
</tr>
<tr>
<td><code>upper_bound(start, end, val)</code></td>
<td>二分查找，返回 &gt;val 的首元素迭代器</td>
<td></td>
</tr>
<tr>
<td><code>count(start, end, val)</code></td>
<td>统计元素出现次数</td>
<td></td>
</tr>
<tr>
<td><code>find(start, end, val)</code></td>
<td>查找元素</td>
<td></td>
</tr>
<tr>
<td><code>accumulate(start, end, init)</code></td>
<td>元素累加 (需 <code>&lt;numeric&gt;</code>)</td>
<td></td>
</tr>
</tbody>
</table>
<h1 id="力扣刷题"><a class="markdownIt-Anchor" href="#力扣刷题"></a> 🚀力扣刷题</h1>
<h2 id="算法设计"><a class="markdownIt-Anchor" href="#算法设计"></a> 算法设计</h2>
<h3 id="1-快慢指针算法"><a class="markdownIt-Anchor" href="#1-快慢指针算法"></a> 1 快慢指针算法</h3>
<ol>
<li>判断循环</li>
<li>遍历</li>
</ol>
<h3 id="2-贪心算法"><a class="markdownIt-Anchor" href="#2-贪心算法"></a> 2 贪心算法</h3>
<ol>
<li>一般用在有cost的</li>
</ol>
<h3 id="3-动态规划"><a class="markdownIt-Anchor" href="#3-动态规划"></a> 3 动态规划</h3>
<ol>
<li>后面的前面已经解决过了// 重后面开始算起，简单的算起</li>
<li>背包问题 爬楼梯</li>
<li>滚动数组</li>
<li>递归【枚举判断初始条件是否正确】–&gt; 变成动态规划  存储前状态变量</li>
</ol>
<h3 id="4-滑动窗口思想总结"><a class="markdownIt-Anchor" href="#4-滑动窗口思想总结"></a> 4 滑动窗口思想总结</h3>
<p><strong>核心思想：</strong><br>
滑动窗口是一种高效解决「子数组 / 子区间 / 子串」问题的技巧，</p>
<p>维护窗口大小，缩小当前维护与扩展窗口</p>
<p>通常用于：</p>
<ul>
<li><strong>连续子序列</strong>：比如最长不重复子串、子数组和等。</li>
<li></li>
</ul>
<h3 id="5-用栈追溯"><a class="markdownIt-Anchor" href="#5-用栈追溯"></a> 5 用栈追溯</h3>
<h3 id="5-链表"><a class="markdownIt-Anchor" href="#5-链表"></a> 5 链表</h3>
<ol>
<li>呀节点 避免nullptr的判断 <code>return dummy-&gt;next</code> 即可</li>
</ol>
<h1 id="记录"><a class="markdownIt-Anchor" href="#记录"></a> 🌏记录</h1>
<h2 id="命名空间"><a class="markdownIt-Anchor" href="#命名空间"></a> 命名空间</h2>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/cl122763974/article/details/139457699">https://blog.csdn.net/cl122763974/article/details/139457699</a></p>
<ol>
<li>避免冲突</li>
<li>易于维护, 不同模块和相同类</li>
</ol>
</blockquote>
<h2 id="code"><a class="markdownIt-Anchor" href="#code"></a> code</h2>
<blockquote>
<p>缩进也报错?</p>
<p>选择&quot;UTF-8 带签名(BOM)&quot;???</p>
</blockquote>
<h3 id="数"><a class="markdownIt-Anchor" href="#数"></a> 数</h3>
<blockquote>
<p>INFINITY 是 C++ 中表示正无穷大的宏</p>
<p>NaN 不等于任何值（包括自身），用 std::isnan(x) 检查。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">float nan = std::nan(&quot;&quot;); // NaN</span><br></pre></td></tr></table></figure>
</blockquote>
<h3 id="vector"><a class="markdownIt-Anchor" href="#vector"></a> Vector</h3>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Vector <span class="title">output</span><span class="params">(output_channel, <span class="number">0.0f</span>)</span></span>;  <span class="comment">// 初始化输出向量</span></span><br><span class="line"><span class="comment">// 初始化权重矩阵</span></span><br><span class="line"><span class="function">Matrix <span class="title">weight</span><span class="params">(output_channel, Vector(x_size, <span class="number">0.0f</span>))</span></span>;</span><br></pre></td></tr></table></figure>
<p>区别</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cout &lt;&lt; list.<span class="built_in">at</span>(<span class="number">2</span>) &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; list[<span class="number">2</span>] &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; list.<span class="keyword">operator</span>[](<span class="number">2</span>) &lt;&lt; endl;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">capacity</span>()</span><br><span class="line"><span class="built_in">max_size</span>()</span><br></pre></td></tr></table></figure>
<h3 id="map"><a class="markdownIt-Anchor" href="#map"></a> MAP</h3>
<blockquote>
<p>std::map 是一个关联容器，存储键值对（key-value pairs），每个元素是一个 std::pair&lt;const Key, T&gt;。</p>
<ul>
<li>Key：键的类型（在你的代码中是 string）。</li>
<li>T：值的类型（在你的代码中是 int）。</li>
<li>const Key：键是常量，不能修改，以保证 map 的排序和唯一性。</li>
</ul>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">print</span><span class="params">(map&lt;string, <span class="type">int</span>&gt;map)</span></span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">auto</span> it = map.<span class="built_in">begin</span>(); it != map.<span class="built_in">end</span>(); it++)&#123;</span><br><span class="line">        cout &lt;&lt; it-&gt;first &lt;&lt; <span class="string">&quot; &quot;</span> &lt;&lt; it-&gt;second &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="it-vs-it"><a class="markdownIt-Anchor" href="#it-vs-it"></a> it++ vs ++it</h5>
<blockquote>
<p>无影响,会优化</p>
</blockquote>
<p><strong>it-&gt;first 和 it-&gt;second 是什么？</strong></p>
<blockquote>
<p>it-&gt;first 和 it-&gt;second 是访问 std::map 中键值对的成员，</p>
</blockquote>
<h3 id="函数"><a class="markdownIt-Anchor" href="#函数"></a> 函数</h3>
<p>三个有什么区别</p>
<blockquote>
<ul>
<li>
<h3 id="1-void-swapint-a-int-b"><a class="markdownIt-Anchor" href="#1-void-swapint-a-int-b"></a> 1. void swap(int a, int b)</h3>
<ul>
<li>
<p><strong>参数传递方式</strong>：值传递（Pass by Value）</p>
</li>
<li>
<p>接收参数的副本</p>
</li>
</ul>
<h3 id="2-void-swapint-a-int-b"><a class="markdownIt-Anchor" href="#2-void-swapint-a-int-b"></a> 2. void swap(int *a, int *b)</h3>
<ul>
<li>
<p><strong>参数传递方式</strong>：指针传递（Pass by Pointer）</p>
</li>
<li>
<p>通过解引用（*a 和 *b）直接访问和修改调用者的变量</p>
</li>
</ul>
<h3 id="3-void-swapint-a-int-b"><a class="markdownIt-Anchor" href="#3-void-swapint-a-int-b"></a> 3. void swap(int &amp;a, int &amp;b)</h3>
<ul>
<li><strong>参数传递方式</strong>：引用传递（Pass by Reference）</li>
<li>函数接收的是调用者变量的引用，a 和 b 是原始变量的别名，任何对 a 和 b 的修改直接反映到调用者的变量上。</li>
</ul>
</li>
</ul>
</blockquote>
<h4 id="函数重载"><a class="markdownIt-Anchor" href="#函数重载"></a> <strong>函数重载</strong></h4>
<blockquote>
<p>允许在同一作用域内定义多个同名函数，但这些函数的参数列表（参数数量、类型或顺序）必须不同。编译器会根据调用时提供的参数类型和数量，自动选择最匹配的函数版本来执行</p>
</blockquote>
<h4 id="函数指针数组的写法规则"><a class="markdownIt-Anchor" href="#函数指针数组的写法规则"></a> 函数指针数组的写法规则</h4>
<p><strong>函数签名一致</strong>：</p>
<ul>
<li>数组中所有函数指针指向的函数必须具有<strong>相同的返回类型</strong>和<strong>相同的参数列表</strong>（包括参数类型和数量）。</li>
<li>例如，所有函数指针都指向 void(int*, int*) 类型的函数。</li>
</ul>
<p><strong>声明语法</strong>：</p>
<ul>
<li>函数指针的声明形式为：返回类型 (*指针名)(参数列表)。</li>
<li>函数指针数组的声明形式为：返回类型 (*数组名[大小])(参数列表)。</li>
<li>例如：void (<em>swapFuncs[2])(int</em>, int*) 表示一个数组，包含两个指向 void(int*, int*) 类型函数的指针。</li>
</ul>
<h4 id="模板函数"><a class="markdownIt-Anchor" href="#模板函数"></a> 模板函数</h4>
<blockquote>
<p>定义一个函数的“蓝图”，让它能够处理多种数据类型（如 int、double、std::string 等），而不需要为每种类型写一个单独的函数。</p>
</blockquote>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt; <span class="comment">// 或者 template &lt;class T&gt;</span></span><br><span class="line">返回类型 函数名(参数列表) &#123;</span><br><span class="line">    <span class="comment">// 函数体，使用 T 作为类型</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="什么是函数对象"><a class="markdownIt-Anchor" href="#什么是函数对象"></a> 什么是函数对象？</h4>
<blockquote>
<p>函数对象是一个<strong>行为类似函数的类对象</strong>。在C++中，函数对象是通过定义了<strong>函数调用运算符</strong>（operator()）的类或结构体创建的.像普通函数一样被调用，但因为是对象，它可以保存状态、支持更多灵活性</p>
</blockquote>
<blockquote>
<p>一个函数对象需要重载 operator()，使其可以通过 () 调用。</p>
</blockquote>
<h3 id="构造函数"><a class="markdownIt-Anchor" href="#构造函数"></a> 构造函数</h3>
<p><code>NoBackprop() : previous_state(Config::enable_backprop) &#123;&#125;</code></p>
<p>和<code>NoBackprop() &#123;previous_state = Config::enable_backprop;&#125;</code></p>
<h3 id="指针"><a class="markdownIt-Anchor" href="#指针"></a> 指针</h3>
<ul>
<li><code>int *p, q;   // p 是指针，q 是普通整数（新手常见错误）</code></li>
</ul>
<blockquote>
<h3 id="什么是函数指针和指针函数以及区别"><a class="markdownIt-Anchor" href="#什么是函数指针和指针函数以及区别"></a> 🔥 什么是函数指针和指针函数以及区别</h3>
<h3 id="什么是常量指针和指针常量以及区别"><a class="markdownIt-Anchor" href="#什么是常量指针和指针常量以及区别"></a> 🔥 什么是常量指针和指针常量以及区别</h3>
<blockquote>
<p><strong>常量指针</strong>： 指的是指针本身的值（即指向的地址）可以改变，但指针指向的对象的值不能通过这个指针进行修改。</p>
<p><strong>指针常量</strong>：指的是指针本身的值（即指向的地址）不能改变，但可以通过这个指针修改指向对象的值</p>
</blockquote>
</blockquote>
<h4 id><a class="markdownIt-Anchor" href="#"></a> </h4>
<h3 id="类"><a class="markdownIt-Anchor" href="#类"></a> 类</h3>
<p>if not define --&gt; define  --&gt; endif</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#ifndef MYCLASS_H</span><br><span class="line">#define MYCLASS_H</span><br><span class="line"></span><br><span class="line">class MyClass &#123;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br><span class="line">#endif</span><br></pre></td></tr></table></figure>
<h4 id="cpp"><a class="markdownIt-Anchor" href="#cpp"></a> cpp</h4>
<blockquote>
<h2 id="用-this-或改名避免混淆"><a class="markdownIt-Anchor" href="#用-this-或改名避免混淆"></a> 用 <code>this-&gt;</code> 或改名避免混淆</h2>
<h3 id="方法-1用-this-明确访问成员变量"><a class="markdownIt-Anchor" href="#方法-1用-this-明确访问成员变量"></a> 方法 1：用 <code>this-&gt;</code> 明确访问成员变量</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cpp复制编辑void MyClass::setValue(int value) &#123;</span><br><span class="line">    this-&gt;value = value;  // ✅ 正确：this-&gt;value 表示成员变量</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="方法-2参数改名避免遮蔽"><a class="markdownIt-Anchor" href="#方法-2参数改名避免遮蔽"></a> 方法 2：参数改名，避免遮蔽</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cpp复制编辑void MyClass::setValue(int v) &#123;</span><br><span class="line">    value = v;  // ✅ 正确：v 是参数，value 是成员变量</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p>MyClass::MyClass():value(0){</p>
<p>std::cout&lt;&lt;“运行构造函数”&lt;&lt;std::endl;}</p>
<p>MyClass::MyClass(){</p>
<p>value = 0;</p>
<p>std::cout&lt;&lt;“运行构造函数”&lt;&lt;std::endl;}</p>
<table>
<thead>
<tr>
<th>写法</th>
<th>初始化时机</th>
<th>效率</th>
<th>推荐吗？</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>: value(0)</code></td>
<td>构造函数进入前</td>
<td>✅ 更快</td>
<td>✅ 推荐</td>
</tr>
<tr>
<td><code>value = 0;</code></td>
<td>构造函数体内赋值</td>
<td>稍慢一点</td>
<td>✅ 也可以</td>
</tr>
</tbody>
</table>
</blockquote>
<h4 id="h"><a class="markdownIt-Anchor" href="#h"></a> .h</h4>
<p>为什么要写const</p>
<blockquote>
<p>// 成员函数</p>
<p>void setValue(int value);</p>
<p>int getValue() const;</p>
</blockquote>
<h4 id="静态和非静态成员"><a class="markdownIt-Anchor" href="#静态和非静态成员"></a> 静态和非静态成员</h4>
<p>1 这个错误是因为我们试图直接访问 Config 类的非静态成员变量 enable_backprop。我们需要将 Config 类中的成员变量声明为静态的。</p>
<blockquote>
<ol>
<li>普通成员变量：</li>
</ol>
<ul>
<li>
<p>每个类的实例（对象）都有自己独立的成员变量副本</p>
</li>
<li>
<p>需要通过对象来访问（例如：config.enable_backprop）</p>
</li>
<li>
<p>每个对象的内存中都会存储这些变量</p>
</li>
</ul>
<ol>
<li>静态成员变量：</li>
</ol>
<ul>
<li>
<p>所有类的实例共享同一个变量</p>
</li>
<li>
<p>可以直接通过类名访问（例如：Config::enable_backprop）</p>
</li>
<li>
<p>在内存中只有一份副本</p>
</li>
<li>
<p>在程序启动时就分配内存，程序结束时才释放</p>
</li>
</ul>
</blockquote>
<blockquote>
<p>/ 静态成员需要在类外定义</p>
</blockquote>
<h4 id="虚函数"><a class="markdownIt-Anchor" href="#虚函数"></a> 虚函数</h4>
<blockquote>
<p><strong>虚函数</strong> 允许在派生类中重写函数，但可以有默认实现。它是可选的，可以在基类中提供实现。</p>
<p><strong>纯虚函数</strong> 没有默认实现，派生类必须提供实现。它使得基类成为抽象类，不能实例化。</p>
</blockquote>
<h4 id="-2"><a class="markdownIt-Anchor" href="#-2"></a> </h4>
<h2 id="类似"><a class="markdownIt-Anchor" href="#类似"></a> 类似</h2>
<p>1 CPP类似python <code>with</code>的东西</p>
<blockquote>
<p>with using_config(“enable_backprop”, False):  x = Variable(np.array(2.Ð))  y = square(x)</p>
<p>使用构造函数和析构函数</p>
</blockquote>
<h2 id="库"><a class="markdownIt-Anchor" href="#库"></a> 库</h2>
<h3 id="include-iomanip-用于格式化输出"><a class="markdownIt-Anchor" href="#include-iomanip-用于格式化输出"></a> #include <iomanip>  // 用于格式化输出</iomanip></h3>
<h3 id="raiiresource-acquisition-is-initialization"><a class="markdownIt-Anchor" href="#raiiresource-acquisition-is-initialization"></a> RAII（Resource Acquisition Is Initialization</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ConfigTest(const ConfigTest&amp;) = delete;</span><br><span class="line">ConfigTest&amp; operator=(const ConfigTest&amp;) = delete;</span><br><span class="line">ConfigTest a;</span><br><span class="line">ConfigTest b = a;  // ❌ 编译错误：拷贝构造被删除</span><br><span class="line">b = a;             // ❌ 编译错误：拷贝赋值被删除</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="random"><a class="markdownIt-Anchor" href="#random"></a> random</h2>
<blockquote>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">random_device rd;</span><br><span class="line"><span class="function">mt19937 <span class="title">gen</span><span class="params">(rd())</span></span>;</span><br><span class="line"><span class="function">uniform_real_distribution&lt;<span class="type">float</span>&gt; <span class="title">dis</span><span class="params">(<span class="number">-0.1f</span>, <span class="number">0.1f</span>)</span></span>;</span><br></pre></td></tr></table></figure>
<ul>
<li>
<p>random_device 是 C++11 引入的  库中的类，用于生成非确定性随机数（通常基于硬件或操作系统的熵源）</p>
</li>
<li>
<p>mt19937 是  库中的梅森旋转算法（Mersenne Twister）伪随机数生成器，是一种高效、高周期的伪随机数生成器</p>
</li>
<li>
<p>uniform_real_distribution<float> 是 <random> 库中的类，用于生成指定范围内的<strong>均匀分布浮点随机数</strong>。</random></float></p>
<p>dis 是一个分布对象，指定范围为 [-0.1, 0.1)（包含 -0.1，不包含 0.1）。</p>
</li>
</ul>
</blockquote>
<h2 id="异常处理机制"><a class="markdownIt-Anchor" href="#异常处理机制"></a> 异常处理机制\</h2>
<h4 id="基本的异常处理语法"><a class="markdownIt-Anchor" href="#基本的异常处理语法"></a> ✅ 基本的异常处理语法</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cpp复制编辑try &#123;</span><br><span class="line">    // 可能抛出异常的代码</span><br><span class="line">    throw std::runtime_error(&quot;Something went wrong!&quot;);</span><br><span class="line">&#125;</span><br><span class="line">catch (const std::runtime_error&amp; e) &#123;</span><br><span class="line">    std::cout &lt;&lt; &quot;Caught runtime_error: &quot; &lt;&lt; e.what() &lt;&lt; std::endl;</span><br><span class="line">&#125;</span><br><span class="line">catch (...) &#123;</span><br><span class="line">    std::cout &lt;&lt; &quot;Caught unknown exception&quot; &lt;&lt; std::endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><code>try</code>：监控语句块。</li>
<li><code>throw</code>：抛出异常。</li>
<li><code>catch</code>：捕获异常。</li>
<li><code>...</code>：捕获所有类型的异常（最后一个兜底捕获器）。</li>
</ul>
<h3 id="标准异常类位于-stdexcept"><a class="markdownIt-Anchor" href="#标准异常类位于-stdexcept"></a> 标准异常类（位于 <code>&lt;stdexcept&gt;</code>）</h3>
<h3 id="noexcept"><a class="markdownIt-Anchor" href="#noexcept"></a> noexcept</h3>
<blockquote>
<p><code>noexcept</code> 是 C++11 引入的一个关键字，用于声明一个函数 <strong>不会抛出异常</strong>。它有助于：</p>
<ol>
<li><strong>编译器优化</strong></li>
<li><strong>提高异常安全性</strong></li>
<li><strong>使标准库函数更高效</strong></li>
</ol>
</blockquote>
<h2 id="threading"><a class="markdownIt-Anchor" href="#threading"></a> 🧠threading</h2>
<table>
<thead>
<tr>
<th>特性</th>
<th>多线程（<code>std::thread</code>）</th>
<th>异步（<code>std::async</code>）</th>
</tr>
</thead>
<tbody>
<tr>
<td>调度方式</td>
<td>手动管理线程生命周期</td>
<td>自动管理线程和返回值</td>
</tr>
<tr>
<td>获取结果</td>
<td>不提供返回值机制</td>
<td><code>future.get()</code> 可取回结果</td>
</tr>
<tr>
<td>使用场景</td>
<td>更复杂的并发控制（锁、同步）</td>
<td>简单并发任务、获取返回值</td>
</tr>
<tr>
<td>更适合谁？</td>
<td>控制更强的程序员</td>
<td>想写少点代码的程序员 😄</td>
</tr>
</tbody>
</table>
<h2 id="cpp思路"><a class="markdownIt-Anchor" href="#cpp思路"></a> CPP思路</h2>
<blockquote>
<h3 id="batch-normalization-的核心目标"><a class="markdownIt-Anchor" href="#batch-normalization-的核心目标"></a> 🧠 <strong>Batch Normalization 的核心目标</strong>：</h3>
<blockquote>
<p>在网络的每一层，将激活值标准化，使其均值为 0，方差为 1，从而加速训练、提升稳定性。</p>
</blockquote>
<hr>
<h3 id="你需要思考的关键点按执行流程划分"><a class="markdownIt-Anchor" href="#你需要思考的关键点按执行流程划分"></a> 🎯 <strong>你需要思考的关键点（按执行流程划分）</strong>：</h3>
<h4 id="1-输入数据是什么形状"><a class="markdownIt-Anchor" href="#1-输入数据是什么形状"></a> 1. <strong>输入数据是什么形状？</strong></h4>
<ul>
<li>是一个 batch，一个通道，一张图，还是一个数？</li>
<li>不同形状下，BN 应该在哪些维度上求均值和方差？</li>
</ul>
<h4 id="2-你需要计算什么"><a class="markdownIt-Anchor" href="#2-你需要计算什么"></a> 2. <strong>你需要计算什么？</strong></h4>
<ul>
<li><strong>均值（mean）</strong></li>
<li><strong>方差（variance）</strong></li>
<li>你会选择 <code>全局mean</code>、<code>通道mean</code> 还是 <code>按batch求mean</code>？</li>
</ul>
<h4 id="3-归一化怎么做"><a class="markdownIt-Anchor" href="#3-归一化怎么做"></a> 3. <strong>归一化怎么做？</strong></h4>
<ul>
<li>将数据减去均值，再除以标准差</li>
<li>要考虑 <strong>数值稳定性</strong>（加入小的 ε）</li>
</ul>
<h4 id="4-训练时-vs-推理时"><a class="markdownIt-Anchor" href="#4-训练时-vs-推理时"></a> 4. <strong>训练时 vs 推理时</strong></h4>
<ul>
<li>训练：用 batch 的均值/方差</li>
<li>推理：用<strong>累计的滑动平均值</strong></li>
</ul>
<h4 id="5-可学习的参数-γ-β缩放-平移"><a class="markdownIt-Anchor" href="#5-可学习的参数-γ-β缩放-平移"></a> 5. <strong>可学习的参数 γ、β（缩放 + 平移）</strong></h4>
<ul>
<li>为啥需要？BN 本质上是归一化 + 一个线性变换</li>
<li>γ 控制方差，β 控制均值 —— 不影响模型表达能力</li>
</ul>
<h4 id="6-反向传播要怎么处理"><a class="markdownIt-Anchor" href="#6-反向传播要怎么处理"></a> 6. <strong>反向传播要怎么处理？</strong></h4>
<ul>
<li>归一化涉及除法和平方，链式法则复杂</li>
<li>γ 和 β 的梯度如何计算？</li>
<li>对输入的梯度是否能用链式求导？</li>
</ul>
<hr>
<h3 id="总结你需要自己实现或思考的组件"><a class="markdownIt-Anchor" href="#总结你需要自己实现或思考的组件"></a> 📌 总结你需要自己实现或思考的组件：</h3>
<table>
<thead>
<tr>
<th>模块</th>
<th>是否必须</th>
<th>功能说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>均值与方差计算</td>
<td>✅</td>
<td>按 batch / 通道计算</td>
</tr>
<tr>
<td>归一化</td>
<td>✅</td>
<td><code>x_norm = (x - mean) / sqrt(var+ε)</code></td>
</tr>
<tr>
<td>γ 和 β 参数</td>
<td>✅</td>
<td>可学习参数，用于恢复分布</td>
</tr>
<tr>
<td>训练/推理切换机制</td>
<td>✅</td>
<td>使用 batch 统计 or 滑动平均值</td>
</tr>
<tr>
<td>backward</td>
<td>✅</td>
<td>计算输入、γ、β 的梯度</td>
</tr>
</tbody>
</table>
</blockquote>
<blockquote>
<h3 id="核心思路指南maxpooling-实现"><a class="markdownIt-Anchor" href="#核心思路指南maxpooling-实现"></a> 🧠 核心思路指南：MaxPooling 实现</h3>
<h4 id="1-操作对象是什么"><a class="markdownIt-Anchor" href="#1-操作对象是什么"></a> 1. <strong>操作对象是什么？</strong></h4>
<ul>
<li>输入通常是一个二维或三维矩阵（如单通道或多通道 feature map）。</li>
<li>你需要按固定窗口（例如 2×2 或 3×3）滑动，提取每个窗口的最大值。</li>
</ul>
<hr>
<h4 id="2-滑动的方式"><a class="markdownIt-Anchor" href="#2-滑动的方式"></a> 2. <strong>滑动的方式？</strong></h4>
<ul>
<li>核心概念是「窗口大小」和「步长（stride）」。</li>
<li>每次从输入矩阵中裁剪一个小块窗口，沿着行/列按 stride 移动。</li>
<li>边缘处理策略是否允许溢出窗口？是否需要 padding？</li>
</ul>
<hr>
<h4 id="3-提取什么信息"><a class="markdownIt-Anchor" href="#3-提取什么信息"></a> 3. <strong>提取什么信息？</strong></h4>
<ul>
<li>每个滑动窗口中提取一个值（最大值），形成输出矩阵中的一个元素。</li>
<li>输出矩阵的大小可通过公式推导得到（由输入大小、窗口大小、stride 决定）。</li>
</ul>
<hr>
<h4 id="4-输出结果结构"><a class="markdownIt-Anchor" href="#4-输出结果结构"></a> 4. <strong>输出结果结构？</strong></h4>
<ul>
<li>如果输入是二维矩阵 → 输出也是二维；</li>
<li>如果输入是多通道（如 [C, H, W]） → 每个通道独立池化，输出保持通道数不变。</li>
</ul>
<hr>
<h4 id="5-如何组织代码结构"><a class="markdownIt-Anchor" href="#5-如何组织代码结构"></a> 5. <strong>如何组织代码结构？</strong></h4>
<ul>
<li>一个通用函数 / 类，支持参数：
<ul>
<li><code>kernel_size</code></li>
<li><code>stride</code></li>
<li><code>padding</code>（可选）</li>
</ul>
</li>
<li>建议按“逐通道处理”和“窗口遍历”两个层级设计逻辑。</li>
</ul>
<hr>
<h4 id="6-可选进阶功能"><a class="markdownIt-Anchor" href="#6-可选进阶功能"></a> 6. <strong>可选进阶功能</strong></h4>
<ul>
<li>池化位置索引记录（用于反向传播 / Unpooling）</li>
<li>实现 Average Pooling 作为对照</li>
<li>支持 batch 输入维度（例如 [N, C, H, W]）</li>
</ul>
</blockquote>
<h1 id="cpp_dl"><a class="markdownIt-Anchor" href="#cpp_dl"></a> CPP_DL</h1>
<blockquote>
<h3 id="初级巩固类构建积木块"><a class="markdownIt-Anchor" href="#初级巩固类构建积木块"></a> ✅ <strong>初级巩固类（构建积木块）</strong></h3>
<ol>
<li><strong>手写 ReLU / Sigmoid / Tanh 等激活函数</strong>
<ul>
<li>实现 forward 和 backward（反向传播）</li>
<li>加深函数微分和链式法则的理解</li>
</ul>
</li>
<li><strong>手写 MaxPooling / AvgPooling 层</strong>
<ul>
<li>包括 forward 和反向传播（对位置进行追踪）</li>
</ul>
</li>
<li><strong>手写 Linear (全连接层) + Softmax + CrossEntropy</strong>
<ul>
<li>和你已有的 Conv 组合起来，构建 MLP</li>
</ul>
</li>
</ol>
<hr>
<h3 id="中级组合类搭建小模型"><a class="markdownIt-Anchor" href="#中级组合类搭建小模型"></a> 🔄 <strong>中级组合类（搭建小模型）</strong></h3>
<ol>
<li><strong>实现简单的 MLP（多层感知机）进行分类任务</strong>
<ul>
<li>用自己实现的 Linear + ReLU + Softmax</li>
</ul>
</li>
<li><strong>实现简化版的 CNN</strong>
<ul>
<li>使用你自己写的 Conv + Pooling + Linear，训练 MNIST 手写数字（可以从 CSV 数据加载开始）</li>
</ul>
</li>
<li><strong>手写一个计算图引擎（自动求导）</strong>
<ul>
<li>类似 PyTorch 的 autograd：支持前向 + 反向传播（Tensor + Node + backward）</li>
</ul>
</li>
</ol>
<hr>
<h3 id="进阶项目模仿框架"><a class="markdownIt-Anchor" href="#进阶项目模仿框架"></a> 📦 <strong>进阶项目（模仿框架）</strong></h3>
<ol>
<li><strong>手写简易的深度学习框架 miniDNN</strong>
<ul>
<li>模仿 PyTorch 的结构，模块化设计：<code>Tensor</code> 类、<code>Layer</code> 类、<code>Model</code> 类</li>
<li>支持动态图 / 静态图、自动求导、优化器等</li>
</ul>
</li>
<li><strong>实现 YOLO 输出解码 + 后处理（Anchor解码 + Sigmoid + NMS）</strong>
<ul>
<li>模拟实际输出张量解码流程，如 <code>[N, 13,13,3,85]</code> 解析</li>
</ul>
</li>
<li><strong>用 C++ 推理一个 ONNX 模型（如 ResNet18）</strong>
<ul>
<li>不依赖第三方框架，纯手写 Tensor 运算 + 模型结构 + 权重加载</li>
</ul>
</li>
</ol>
<hr>
<h3 id="挑战类偏实际落地"><a class="markdownIt-Anchor" href="#挑战类偏实际落地"></a> 🧠 <strong>挑战类（偏实际落地）</strong></h3>
<ol>
<li><strong>用 C++ 和 OpenCV 实现简易目标检测系统</strong></li>
</ol>
<ul>
<li>读取视频帧 + 推理（NMS + 解码）+ 绘制结果</li>
<li>可以用你自己的 NMS 和解码模块</li>
</ul>
<ol>
<li><strong>实现简化版 YOLOv3/YOLOv5 推理模块</strong></li>
</ol>
<ul>
<li>输入图片，输出检测框（用你写的 Conv、NMS）</li>
<li>加载预训练的权重文件（比如从 <code>.weights</code> 解析）</li>
</ul>
<ol>
<li><strong>移植 C++ 代码到嵌入式平台（边缘设备）</strong></li>
</ol>
<ul>
<li>比如使用 ARM 的板子，或 Jetson Nano 进行测试</li>
</ul>
<hr>
<h3 id="建议学习路线"><a class="markdownIt-Anchor" href="#建议学习路线"></a> 💡 建议学习路线：</h3>
<ul>
<li>✅ 项目 1~3 巩固基础</li>
<li>🔁 项目 4~6 做一个“可训练”的小网络</li>
<li>📦 项目 7~9 构建框架级别认知</li>
<li>🧠 项目 10~12 面向实际部署能力</li>
</ul>
</blockquote>
<h2 id="优化"><a class="markdownIt-Anchor" href="#优化"></a> 优化</h2>
<blockquote>
<p>(base) PS F:\Cppdata\cpp_code\ctorch\build&gt; .\Debug\autograd.exe<br>
iter: 0 z.data: 13<br>
iter: 10 z.data: 0.0933579<br>
iter: 20 z.data: 12.3269<br>
iter: 30 z.data: 1.93872<br>
iter: 40 z.data: 9.48611<br>
iter: 50 z.data: 5.50273<br>
iter: 60 z.data: 5.5404<br>
iter: 70 z.data: 9.45205<br>
iter: 80 z.data: 1.9659<br>
iter: 90 z.data: 12.3092<br>
x.data: -0.175315<br>
y.data: -0.262973</p>
<blockquote>
<p>问题在于：</p>
<ol>
<li>
<p>z.data的值在迭代过程中波动很大，没有稳定收敛</p>
</li>
<li>
<p>最终x和y的值变成了负数，这显然不是我们期望的结果</p>
</li>
</ol>
</blockquote>
<blockquote></blockquote>
</blockquote>
<blockquote></blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://shakewely.github.io/2025/03/10/2025%E5%B9%B43%E6%9C%8810%E6%97%A5-C-%E8%AE%B0%E5%BD%95/" data-id="cm8q1tfhn000ppcv4eds1bem1" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/C/" rel="tag">C++</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-2025年3月7日-loss汇总" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2025/03/06/2025%E5%B9%B43%E6%9C%887%E6%97%A5-loss%E6%B1%87%E6%80%BB/" class="article-date">
  <time class="post-time" datetime="2025-03-06T04:44:46.000Z" itemprop="datePublished">
    <span class="post-month">3月</span><br/>
    <span class="post-day">06</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/03/06/2025%E5%B9%B43%E6%9C%887%E6%97%A5-loss%E6%B1%87%E6%80%BB/">2025年3月7日 loss汇总</a>
    </h1>
  

        <div>
          
          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
      
    </div>
    <footer class="article-footer">
      <a data-url="https://shakewely.github.io/2025/03/06/2025%E5%B9%B43%E6%9C%887%E6%97%A5-loss%E6%B1%87%E6%80%BB/" data-id="cm8q1tfhy001cpcv43gu2ejcs" class="article-share-link">分享</a>
      
      
    </footer>
  </div>
  
</article>




  
    <article id="post-2025年3月6日-多尺度特征" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2025/03/06/2025%E5%B9%B43%E6%9C%886%E6%97%A5-%E5%A4%9A%E5%B0%BA%E5%BA%A6%E7%89%B9%E5%BE%81/" class="article-date">
  <time class="post-time" datetime="2025-03-06T02:22:16.000Z" itemprop="datePublished">
    <span class="post-month">3月</span><br/>
    <span class="post-day">06</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/03/06/2025%E5%B9%B43%E6%9C%886%E6%97%A5-%E5%A4%9A%E5%B0%BA%E5%BA%A6%E7%89%B9%E5%BE%81/">2025年3月6日 多尺度特征</a>
    </h1>
  

        <div>
          
          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="混合特征学习feature-fusion"><a class="markdownIt-Anchor" href="#混合特征学习feature-fusion"></a> 混合特征学习（Feature Fusion）</h1>
<p><strong>混合特征学习（Feature Fusion）</strong> 是指将来自不同来源或不同尺度的特征信息结合在一起，从而提升深度学习模型的表达能力。常见的特征融合方法包括 加权融合（Weighted Fusion） 和 级联融合（Concatenation Fusion）。</p>
<ol>
<li>
<p><strong>加权融合（Weighted Fusion）</strong><br>
加权融合是一种<strong>将多个特征图进行加权合并</strong>的方式。不同来源或不同尺度的特征图按照一定的权重加权后合并，从而突出重要的特征。</p>
</li>
<li>
<p><strong>级联融合（Concatenation Fusion）</strong><br>
级联融合是将多个特征图在<strong>某一维度（通常是通道维度）上拼接</strong>起来。级联融合的优势是它不会丢失任何信息，所有特征图都被保留下来，虽然拼接后的特征图会变得更加庞大。</p>
</li>
</ol>
<h2 id="多尺度卷积神经网络ms-cnn"><a class="markdownIt-Anchor" href="#多尺度卷积神经网络ms-cnn"></a> <strong>多尺度卷积神经网络（MS-CNN）</strong></h2>
<p>通过将图像<strong>输入多个卷积层或卷积核以不同的尺度处理，可以让模型同时捕获到不同尺寸的特征</strong></p>
<h3 id="原理"><a class="markdownIt-Anchor" href="#原理"></a> 原理</h3>
<p><strong>MS-CNN</strong> 通过以下几种方式来实现多尺度特征提取：</p>
<ul>
<li><strong>使用多个卷积核：不同的卷积核（大小、步长等）可以提取不同尺度的特征。</strong></li>
<li><strong>多层次网络结构：不同的层次使用不同尺度的卷积操作来处理输入图像，从而提取多尺度信息。</strong></li>
<li><strong>输入图像的多尺度处理：将同一张图像通过不同尺度（例如不同的图像缩放比例）进行处理，结合多个尺度的特征。</strong></li>
</ul>
<h3 id="实现"><a class="markdownIt-Anchor" href="#实现"></a> 实现</h3>
<ol>
<li>使用合适的填充和步长，确保不同卷积核提取的特征图在空间维度上一致。</li>
<li>在通道维度上拼接特征图，避免空间维度的变化。</li>
<li>使用1×1卷积进行特征融合，进一步整合多尺度信息。</li>
<li>使用全局池化将特征图调整为固定尺寸，确保输出的一致性</li>
</ol>
<h2 id="空间金字塔池化spatial-pyramid-pooling-spp"><a class="markdownIt-Anchor" href="#空间金字塔池化spatial-pyramid-pooling-spp"></a> 空间金字塔池化（Spatial Pyramid Pooling, SPP）</h2>
<h3 id="原理-2"><a class="markdownIt-Anchor" href="#原理-2"></a> 原理</h3>
<p>在多个尺度上提取特征</p>
<ul>
<li><strong>划分不同大小的网格</strong>：将输入特征图划分为多个不同尺寸的子区域（例如，1x1、2x2、4x4等）。</li>
<li><strong>在每个子区域上执行池化操作</strong>：对每个子区域使用池化操作（如最大池化或平均池化），生成不同尺度下的特征。</li>
<li><strong>拼接池化结果</strong>：将所有尺度上的池化结果<strong>展平并</strong>拼接成一个向量，作为最终的特征表示。</li>
</ul>
<h2 id="多尺度注意力机制multi-scale-attention-mechanism"><a class="markdownIt-Anchor" href="#多尺度注意力机制multi-scale-attention-mechanism"></a> 多尺度注意力机制（Multi-Scale Attention Mechanism）</h2>
<h3 id="原理-3"><a class="markdownIt-Anchor" href="#原理-3"></a> 原理</h3>
<p>通过注意力机制对图像的不同尺度或区域进行<strong>加权</strong>，帮助网络聚焦于关键区域，从而更有效地提取特征的方法</p>
<h3 id="实现-2"><a class="markdownIt-Anchor" href="#实现-2"></a> 实现</h3>
<h4 id="non-local-attention"><a class="markdownIt-Anchor" href="#non-local-attention"></a> <strong>Non-local Attention</strong></h4>
<h4 id="squeeze-and-excitation-networks-se-net"><a class="markdownIt-Anchor" href="#squeeze-and-excitation-networks-se-net"></a> Squeeze-and-Excitation Networks (SE-Net)</h4>
<p>SE-Net）是一种通过引入自适应的<strong>通道注意力机制</strong>来提升模型性能的网络。它通过“压缩”和“激励”两个操作来对不同通道的特征进行加权，使得网络能够自适应地学习哪些通道对特定任务更为重要。</p>
<h5 id="原理-4"><a class="markdownIt-Anchor" href="#原理-4"></a> 原理</h5>
<ul>
<li><strong>Squeeze：通过全局平均池化（Global Average Pooling），将每个通道的空间信息压缩成一个标量。</strong></li>
<li><strong>Excitation：通过一个全连接层（或者更深的网络）生成通道的权重，这些权重用于调整每个通道的特征响应。</strong></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nn.AdaptiveAvgPool2d(1)</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://shakewely.github.io/2025/03/06/2025%E5%B9%B43%E6%9C%886%E6%97%A5-%E5%A4%9A%E5%B0%BA%E5%BA%A6%E7%89%B9%E5%BE%81/" data-id="cm8q1tfhr0013pcv4fmixa517" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/dl/" rel="tag">dl</a></li></ul>

    </footer>
  </div>
  
</article>




  


  <nav id="page-nav">
    <a class="extend prev" rel="prev" href="/page/3/">&amp;laquo; pre</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><a class="page-number" href="/page/6/">6</a><span class="space">&hellip;</span><a class="page-number" href="/page/11/">11</a><a class="extend next" rel="next" href="/page/5/">next &amp;raquo;</a>
  </nav>
</section>
        
          <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title"></h3>
  <div class="widget">
    <h1 class="blog-title">Weakliy_Blog</h1>
    <h2 class="blog-subtitle"></h2>
    <ul class="blog-link">
     
          <a href="/" title="Home">
            <li>主页</li>
          </a>
        
          <a href="/archives" title="Archives">
            <li>归档</li>
          </a>
        
          <a href="/categories" title="Categories">
            <li>分类</li>
          </a>
        
          <a href="/tags" title="Tags">
            <li>标签</li>
          </a>
        
    </ul>
  </div>
</div>

  
    <div class="widget-wrap">
  <h3 class="widget-title"></h3>
  <div class="widget">
    <img class="avatar" src="https://raw.githubusercontent.com/ShakeWeLy/Weakliy.github.io/main/%E5%A4%B4%E5%83%8F/mmexport1683194148817.png">
    <h2 class="author">Weakliy</h2>
    <h3 class="description"></h3>
    <div class="count-box">
      <a href="/archives"><div><strong>102</strong><br>文章</div></a>
      <a href="/categories"><div><strong>0</strong><br>分类</div></a>
      <a href="/tags"><div><strong>30</strong><br>标签</div></a>
    </div>



    <div class="social-link">
      
        <a class="hvr-bounce-in" href="https://github.com/ShakeWeLy" target="_blank" title="Github">
          Github
        </a>
      
    </div>

    <div class="friend-link">
      <h2>联系我</h2>
      
        <a class="hvr-bounce-in" href="https://github.com/ShakeWeLy" target="_blank" title="ShanaMaid">
          ShanaMaid
        </a>
      
    </div>
  </div>
</div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy;2024 - 2025 Weakliy<br>
      由<a href="http://hexo.io/" target="_blank">Hexo</a>强力驱动 | 
      主题-<a target="_blank" rel="noopener" href="https://github.com/ShanaMaid/hexo-theme-shana">Shana</a>
      
    </div>
    
  </div>
</footer>
    </div>
    

<script src="//apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"></script>
<script src="//apps.bdimg.com/libs/wow/0.1.6/wow.min.js"></script>
<script>
new WOW().init();
</script>   


  
<link rel="stylesheet" href="/plugin/fancybox/jquery.fancybox.css">

  
<script src="/plugin/fancybox/jquery.fancybox.pack.js"></script>




  
<link rel="stylesheet" href="/plugin/galmenu/GalMenu.css">

  
<script src="/plugin/galmenu/GalMenu.js"></script>

  <div class="GalMenu GalDropDown">
      <div class="circle" id="gal">
        <div class="ring">
          
            <a href="/" title="" class="menuItem">首页</a>
          
            <a href="/tags" title="" class="menuItem">标签</a>
          
            <a href="/categories" title="" class="menuItem">分类</a>
          
            <a href="/archives" title="" class="menuItem">总览</a>
          
            <a href="/xxxxxxxxx" title="" class="menuItem">xxx</a>
          
            <a href="/xxxxxxx" title="" class="menuItem">xxxx</a>
          
        </div>
        
          <audio id="audio" src="#"></audio>
        
      </div> 
</div>
<div id="overlay" style="opacity: 1; cursor: pointer;"></div>
  <script type="text/javascript">var items = document.querySelectorAll('.menuItem');
    for (var i = 0,
    l = items.length; i < l; i++) {
      items[i].style.left = (50 - 35 * Math.cos( - 0.5 * Math.PI - 2 * (1 / l) * i * Math.PI)).toFixed(4) + "%";
      items[i].style.top = (50 + 35 * Math.sin( - 0.5 * Math.PI - 2 * (1 / l) * i * Math.PI)).toFixed(4) + "%"
    }</script>
<script type="text/javascript">
  $(document).ready(function() {
    $('body').GalMenu({
      'menu': 'GalDropDown'
    })
  });
</script>

  <section class="hidden-xs"> 
  <ul class="cb-slideshow"> 
    <li><span>苟利</span></li> 
    <li><span>国家</span></li> 
    <li><span>生死以</span></li> 
    <li><span>岂能</span></li> 
    <li><span>祸福</span></li> 
    <li><span>趋避之</span></li> 
  </ul>
</section>

<script src="/js/script.js"></script>




  </div>
</body>
</html>