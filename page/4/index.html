<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Weakliy_Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Weakliy_Blog">
<meta property="og:url" content="https://shakewely.github.io/page/4/index.html">
<meta property="og:site_name" content="Weakliy_Blog">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Weakliy">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Weakliy_Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

  
<link rel="stylesheet" href="/plugin/bganimation/bg.css">

  

  <link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css" rel="stylesheet" type="text/css">
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <div class="outer">
        <div class="widget-wrap mobile-header">
  <h3 class="widget-title"></h3>
  <div class="widget">
    <img class="avatar" src="https://raw.githubusercontent.com/ShakeWeLy/Weakliy.github.io/main/%E5%A4%B4%E5%83%8F/mmexport1683194148817.png">
    <h2 class="author">Weakliy</h2>
    <h3 class="description"></h3>
    <div class="count-box">
      <a href="/archives"><div><strong>116</strong><br>文章</div></a>
      <a href="/categories"><div><strong>0</strong><br>分类</div></a>
      <a href="/tags"><div><strong>34</strong><br>标签</div></a>
    </div>
    <ul class="blog-link">
     
          <a href="/" title="Home">
            <li>主页</li>
          </a>
        
          <a href="/archives" title="Archives">
            <li>归档</li>
          </a>
        
          <a href="/categories" title="Categories">
            <li>分类</li>
          </a>
        
          <a href="/tags" title="Tags">
            <li>标签</li>
          </a>
        
    </ul>
  </div>
</div>

        <section id="main">
  
    <article id="post-2025年5月15日-知识蒸馏1-介绍汇总" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2025/05/14/2025%E5%B9%B45%E6%9C%8815%E6%97%A5-%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F1-%E4%BB%8B%E7%BB%8D%E6%B1%87%E6%80%BB/" class="article-date">
  <time class="post-time" datetime="2025-05-14T07:57:53.000Z" itemprop="datePublished">
    <span class="post-month">5月</span><br/>
    <span class="post-day">14</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/05/14/2025%E5%B9%B45%E6%9C%8815%E6%97%A5-%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F1-%E4%BB%8B%E7%BB%8D%E6%B1%87%E6%80%BB/">2025年5月15日 知识蒸馏1-介绍汇总</a>
    </h1>
  

        <div>
          
          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="蒸馏介绍"><a class="markdownIt-Anchor" href="#蒸馏介绍"></a> 蒸馏介绍</h2>
<p>通过设计一个规模较小的学生模型，使其学习大型教师模型的有效信息</p>
<blockquote>
<p>知识蒸馏的本质可以被认为是一种正则化方法,即在学生模型中引入一些特殊信息来实现模型性能的提升。</p>
</blockquote>
<h2 id="蒸馏分类"><a class="markdownIt-Anchor" href="#蒸馏分类"></a> 蒸馏分类</h2>
<ol>
<li>
<p>知识类型</p>
<ol>
<li>
<p><strong>基于响应/模型输出</strong>的蒸馏（Response-based）：使用教师模型的最后输出层的信息（如类别概率）来训练学生模型。</p>
</li>
<li>
<p>基于<strong>特征</strong>的蒸馏（Feature-based）：利用教师模型的中间层特征来指导学生模型。</p>
</li>
<li>
<p>基于关系的蒸馏（Relation-based）：侧重于教师模型内<strong>不同特征之间的关系</strong>，如特征图之间的相互作用。</p>
</li>
</ol>
</li>
<li>
<p>教师模型类型</p>
</li>
<li>
<p>蒸馏策略三方面</p>
<ol>
<li>
<p>在线蒸馏（Online distillation）：教师模型和学生模型同时训练，学生模型实时学习教师模型的知识。</p>
</li>
<li>
<p>离线蒸馏（Offline distillation）：先训练教师模型，再使用该模型来训练学生模型，学生模型不会影响教师模型。</p>
</li>
<li>
<p>自蒸馏（Self distillation）：模型使用自己的预测作为软标签来提高自己的性能。</p>
</li>
</ol>
</li>
</ol>
<h3 id="11-输出蒸馏"><a class="markdownIt-Anchor" href="#11-输出蒸馏"></a> 1.1 输出蒸馏</h3>
<h3 id="12-特征蒸馏"><a class="markdownIt-Anchor" href="#12-特征蒸馏"></a> 1.2 特征蒸馏</h3>
<p>通道</p>
<h3 id="mgd"><a class="markdownIt-Anchor" href="#mgd"></a> mgd</h3>
<blockquote>
<p>之前的知识蒸馏方法着力于使学生去模仿更强的教师的特征，以使学生特征具有更强的表征能力。我们认为提升学生的表征能力并不一定需要通过直接模仿教师实现。从这点出发，我们把模仿任务修改成了生成任务：让学生凭借自己较弱的特征去生成教师较强的特征。在蒸馏过程中，我们对学生特征进行了随机mask，强制学生仅用自己的部分特征去生成教师的所有特征，以提升学生的表征能力。</p>
<p>原文链接：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_41335232/article/details/142717122">https://blog.csdn.net/qq_41335232/article/details/142717122</a></p>
</blockquote>
<blockquote>
<p>YOLO11 summary (fused): 100 layers, 2,585,467 parameters, 0 gradients, 6.3 GFLOPs<br>
all         53        396      0.971      0.954      0.979      0.671<br>
Bottom1         21         21          1      0.935      0.973      0.779<br>
Bottom2         26         26          1      0.981      0.995      0.669<br>
Left1         32         32      0.964      0.969       0.95      0.676<br>
Left2         16         16      0.978          1      0.995      0.657<br>
Left3         30         32      0.967      0.902      0.934      0.631<br>
Left4         28         33          1      0.853       0.94      0.709<br>
Left5         28         28       0.99      0.929      0.993      0.756<br>
Right1         38         39       0.96      0.974      0.971      0.559<br>
Right2         39         39      0.973      0.974      0.951      0.567<br>
Right3         14         14      0.895          1      0.986      0.734<br>
Right4         15         15      0.971          1      0.995      0.669<br>
Top1         32         32      0.974      0.938      0.987      0.697<br>
Top2         33         33      0.986          1      0.995      0.703<br>
out1          7          7      0.942          1      0.995      0.541<br>
out2         11         11          1      0.878      0.995      0.657<br>
out3         13         13          1      0.891      0.995      0.698<br>
out4          5          5       0.91          1      0.995      0.711</p>
</blockquote>
<h3 id="cwd"><a class="markdownIt-Anchor" href="#cwd"></a> cwd</h3>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://shakewely.github.io/2025/05/14/2025%E5%B9%B45%E6%9C%8815%E6%97%A5-%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F1-%E4%BB%8B%E7%BB%8D%E6%B1%87%E6%80%BB/" data-id="cmaqctqhp0001dwv417dy0ffa" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/" rel="tag">知识蒸馏</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-2025年5月14日-剪枝1-介绍汇总" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2025/05/14/2025%E5%B9%B45%E6%9C%8814%E6%97%A5-%E5%89%AA%E6%9E%9D1-%E4%BB%8B%E7%BB%8D%E6%B1%87%E6%80%BB/" class="article-date">
  <time class="post-time" datetime="2025-05-14T07:38:05.000Z" itemprop="datePublished">
    <span class="post-month">5月</span><br/>
    <span class="post-day">14</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/05/14/2025%E5%B9%B45%E6%9C%8814%E6%97%A5-%E5%89%AA%E6%9E%9D1-%E4%BB%8B%E7%BB%8D%E6%B1%87%E6%80%BB/">2025年5月14日 剪枝1-介绍汇总</a>
    </h1>
  

        <div>
          
          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="1-剪枝介绍"><a class="markdownIt-Anchor" href="#1-剪枝介绍"></a> 1 剪枝介绍</h2>
<p>模型过参数化.剪枝就是将模型的<strong>冗余信息/参数</strong>(连接,结构), 以某种<strong>度量标准</strong>进行评估， 然后删减掉评估后不重要的参数，进行去除。</p>
<blockquote>
<p><strong>剪枝的核心思想</strong>：<br>
通过某种**度量标准（例如权重大小、梯度、重要性评分等）**评估网络中每个参数、通道或模块的重要性，将不重要的部分剪除（即设置为零或移除结构）</p>
</blockquote>
<h4 id="微调"><a class="markdownIt-Anchor" href="#微调"></a> 微调:</h4>
<p>剪枝操作通常会对模型性能产生影响，因此在剪枝后需进行<strong>微调</strong>，以恢复模型性能。微调的过程包括：</p>
<blockquote>
<ol>
<li>继续训练保留部分的模型结构。</li>
<li>使用原始或缩小后的学习率，以避免破坏已有的特征。</li>
<li>在原始数据集或其子集上进行训练，使模型重新适应当前结构。</li>
</ol>
</blockquote>
<h2 id="2-剪枝分类"><a class="markdownIt-Anchor" href="#2-剪枝分类"></a> 2 剪枝分类</h2>
<h3 id="21-按粒度划分"><a class="markdownIt-Anchor" href="#21-按粒度划分"></a> 2.1 按粒度划分</h3>
<ul>
<li><strong>非结构化剪枝（Unstructured Pruning）</strong>：以单个权重为单位，剪除数值接近零的参数，产生稀疏矩阵</li>
<li><strong>结构化剪枝（Structured Pruning）</strong>：以卷积核、通道、层等结构为单位剪除参数</li>
</ul>
<blockquote>
<p>1、非结构化稀疏具有更高的模型压缩率和准确性，在通用硬件上的加速效果不好。因为其计算特征上的“不规则”，导致需要特定硬件支持才能实现加速效果。</p>
<p>2、结构化稀疏虽然牺牲了模型压缩率或准确率，但在通用硬件上的加速效果好，所以其被广泛应用。因为结构化稀疏使得权值矩阵更规则更加结构化，更利于硬件加速</p>
<p><img src="/2025/05/14/2025%E5%B9%B45%E6%9C%8814%E6%97%A5-%E5%89%AA%E6%9E%9D1-%E4%BB%8B%E7%BB%8D%E6%B1%87%E6%80%BB/F:.Hexo%5CMyBlog%5Csource_posts%5C2025%E5%B9%B45%E6%9C%8814%E6%97%A5-%E5%89%AA%E6%9E%9D1-%E4%BB%8B%E7%BB%8D%E6%B1%87%E6%80%BB%5Cimage-20250515230134271.png" alt="image-20250515230134271"></p>
</blockquote>
<h3 id="22-按剪枝时机划分"><a class="markdownIt-Anchor" href="#22-按剪枝时机划分"></a> 2.2 按剪枝时机划分</h3>
<ul>
<li><strong>训练后剪枝（Post-training Pruning）</strong>：模型训练完毕后进行剪枝，再微调以恢复精度。</li>
<li><strong>训练中剪枝（During-training Pruning）</strong>：剪枝嵌入训练过程，边训练边剪除，逐步达到压缩目标。</li>
</ul>
<h3 id="23-按剪枝标准划分"><a class="markdownIt-Anchor" href="#23-按剪枝标准划分"></a> 2.3 按剪枝标准划分</h3>
<ul>
<li><strong>基于权重大小（Magnitude-based）</strong>：按权重绝对值排序，小于阈值者被剪除，简单高效。</li>
<li><strong>基于梯度（Gradient-based）</strong>：依据梯度信息评估参数重要性，剪除对损失函数影响小的参数。</li>
<li><strong>基于敏感度分析（Sensitivity-based）</strong>：模拟移除参数后性能变化，影响小者被剪。</li>
<li><strong>基于正则化（Regularization-based）</strong>：在训练中引入稀疏性正则（如L1/L2、Group Lasso），促使网络自动趋于稀疏。</li>
</ul>
<h2 id="3-常见剪枝算法"><a class="markdownIt-Anchor" href="#3-常见剪枝算法"></a> 3 常见剪枝算法</h2>
<h3 id="31-基于权重的重要性评分"><a class="markdownIt-Anchor" href="#31-基于权重的重要性评分"></a> 3.1 基于权重的重要性评分</h3>
<p>✅ <strong>L1范数剪枝（L1-norm Pruning）</strong><br>
思路：对卷积核/通道等结构计算L1范数（绝对值之和），认为范数较小的结构对模型贡献小，优先剪除。<br>
特点：简单直观，适用于结构化剪枝，效果较稳健。<br>
典型应用：用于通道剪枝（Channel Pruning）、滤波器剪枝（Filter Pruning）。</p>
<blockquote>
<p>1 通道剪枝:</p>
<img src="/2025/05/14/2025%E5%B9%B45%E6%9C%8814%E6%97%A5-%E5%89%AA%E6%9E%9D1-%E4%BB%8B%E7%BB%8D%E6%B1%87%E6%80%BB/.Hexo\MyBlog\source\_posts\2025年5月14日-剪枝1-介绍汇总\image-20250515230146271.png" alt="image-20250515230146271" style="zoom:50%;"> 
<img src="/2025/05/14/2025%E5%B9%B45%E6%9C%8814%E6%97%A5-%E5%89%AA%E6%9E%9D1-%E4%BB%8B%E7%BB%8D%E6%B1%87%E6%80%BB/.Hexo\MyBlog\source\_posts\2025年5月14日-剪枝1-介绍汇总\image-20250515230150294.png" alt="image-20250515230150294" style="zoom:50%;"> 
</blockquote>
<p>✅ <strong>LAMP剪枝（Layer-Adaptive Magnitude-based Pruning）</strong><br>
论文：<em>LAMP: Pruning CNNs via Layer-wise Magnitude-based Pruning with Global Scaling</em><br>
思路：不是简单地按全局权重大小排序剪枝，而是考虑各层的重要性差异，对每层权重大小进行排序归一化再剪枝，从而实现层感知的稀疏性分配。<br>
优点：比传统全局剪枝更稳定，提升剪枝鲁棒性和性能保持能力。<br>
适用场景：特别适合深层网络如 ResNet 等。</p>
<hr>
<h3 id="32-基于梯度与敏感度分析"><a class="markdownIt-Anchor" href="#32-基于梯度与敏感度分析"></a> 3.2 基于梯度与敏感度分析</h3>
<p>✅ <strong>梯度敏感性剪枝（Gradient Sensitivity Pruning）</strong><br>
思路：分析各个参数对损失函数的梯度贡献，即参数对模型性能的“敏感程度”。<br>
方法：如 Hessian 矩阵法（如 Optimal Brain Damage）或直接计算损失变化量。</p>
<p>✅ <strong>Optimal Brain Damage / Optimal Brain Surgeon</strong><br>
思路：通过二阶导数（Hessian 矩阵）分析每个参数对损失的影响，剪除影响最小的参数。<br>
缺点：计算复杂度高，通常只适用于小模型或简化近似版本。</p>
<hr>
<h3 id="33-基于正则化regularization-based-pruning"><a class="markdownIt-Anchor" href="#33-基于正则化regularization-based-pruning"></a> 3.3 基于正则化（Regularization-based Pruning）</h3>
<p>✅ <strong>训练中引入稀疏正则项（如 L1/L2 范数）使网络权重自然趋于稀疏。</strong></p>
<ul>
<li><strong>L1 正则</strong>：使权重向 0 收敛，利于非结构化剪枝。</li>
<li><strong>Group Lasso 正则</strong>：适用于结构化剪枝（如整组卷积核、通道）。</li>
<li><strong>特点</strong>：训练过程中就考虑剪枝可行性，无需额外剪枝步骤。</li>
</ul>
<p>✅ <strong>动态约束稀疏训练（e.g. SNIP, GraSP）</strong></p>
<ul>
<li><strong>SNIP</strong>：在初始化阶段评估连接重要性，仅保留对损失函数最敏感的连接，适用于极早期剪枝。</li>
<li><strong>GraSP</strong>：结合梯度与二阶信息选择重要连接，强调剪枝过程中训练稳定性。</li>
</ul>
<hr>
<h3 id="34-训练过程中逐步剪枝dynamic-pruning"><a class="markdownIt-Anchor" href="#34-训练过程中逐步剪枝dynamic-pruning"></a> 3.4 训练过程中逐步剪枝（Dynamic Pruning）</h3>
<p>✅ <strong>迭代剪枝（Iterative Pruning）</strong><br>
思路：按固定比例每轮训练后剪除部分参数，再继续训练，重复迭代直至达到目标稀疏度。<br>
优点：剪枝过程渐进，性能退化小，适用于结构化与非结构化剪枝。</p>
<p>✅ <strong>Lottery Ticket Hypothesis（LTH）与子网络训练</strong><br>
论文：<em>The Lottery Ticket Hypothesis</em><br>
思路：大模型中存在性能不逊于原始模型的“幸运子网络”，可通过剪枝+重初始化发现。<br>
步骤：训练 → 剪枝 → 重置保留权重 → 再训练。<br>
意义：揭示模型的冗余性和高效子结构的存在性。</p>
<hr>
<h3 id="35-自动化与学习式剪枝方法"><a class="markdownIt-Anchor" href="#35-自动化与学习式剪枝方法"></a> 3.5 自动化与学习式剪枝方法</h3>
<p>✅ <strong>AMC（AutoML for Model Compression）</strong><br>
方法：使用强化学习（RL）自动确定每层剪枝率，学习出压缩策略。<br>
优点：自动化、可适配复杂网络；缺点：训练耗时，需高算力资源。</p>
<p>✅ <strong>NetAdapt、MetaPruning</strong></p>
<ul>
<li><strong>NetAdapt</strong>：逐层评估剪枝对硬件延迟的影响，做出资源感知剪枝决策。</li>
<li><strong>MetaPruning</strong>：基于元学习，训练出可泛化的剪枝策略网络，快速适配新模型</li>
</ul>
<h2 id="4-实例方法实现"><a class="markdownIt-Anchor" href="#4-实例方法实现"></a> 4 实例方法实现</h2>
<h3 id="lamp剪枝"><a class="markdownIt-Anchor" href="#lamp剪枝"></a> lamp剪枝</h3>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://shakewely.github.io/2025/05/14/2025%E5%B9%B45%E6%9C%8814%E6%97%A5-%E5%89%AA%E6%9E%9D1-%E4%BB%8B%E7%BB%8D%E6%B1%87%E6%80%BB/" data-id="cmaqctqhq0002dwv4efac7890" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%89%AA%E6%9E%9D/" rel="tag">剪枝</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-2025年5月14日-cmake" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2025/05/14/2025%E5%B9%B45%E6%9C%8814%E6%97%A5-cmake/" class="article-date">
  <time class="post-time" datetime="2025-05-14T01:44:29.000Z" itemprop="datePublished">
    <span class="post-month">5月</span><br/>
    <span class="post-day">14</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/05/14/2025%E5%B9%B45%E6%9C%8814%E6%97%A5-cmake/">2025年5月14日 cmake</a>
    </h1>
  

        <div>
          
          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="cmake"><a class="markdownIt-Anchor" href="#cmake"></a> cmake</h2>
<blockquote>
<p>作用： <em>CMake 通过使用简单的配置文件 CMakeLists.txt，自动生成不同平台的构建文件</em></p>
</blockquote>
<img src="/2025/05/14/2025%E5%B9%B45%E6%9C%8814%E6%97%A5-cmake/Users\admin\AppData\Roaming\Typora\typora-user-images\image-20250514101856087.png" alt="image-20250514101856087" style="zoom: 25%;">  
<h2 id="cmakeliststxt-文件"><a class="markdownIt-Anchor" href="#cmakeliststxt-文件"></a> CMakeLists.txt 文件</h2>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cmake_minimum_required</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">add_executable(&lt;target&gt; &lt;source_files&gt;...)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">add_library(&lt;target&gt; &lt;source_files&gt;...)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">target_link_libraries(&lt;target&gt; &lt;libraries&gt;...)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">include_directories(&lt;dirs&gt;...)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set(&lt;variable&gt; &lt;value&gt;...)</span><br></pre></td></tr></table></figure>
<h4 id="target_include_directories设置目标的头文件包含路径"><a class="markdownIt-Anchor" href="#target_include_directories设置目标的头文件包含路径"></a> <code>target_include_directories()</code>：设置目标的头文件包含路径</h4>
<p>作用：</p>
<p>用于为某个构建目标（例如可执行文件或库）添加<strong>头文件的搜索路径</strong>，这样在编译时编译器就可以找到相应的头文件</p>
<h3 id="9-installtargets-安装规则"><a class="markdownIt-Anchor" href="#9-installtargets-安装规则"></a> 9. <code>install(TARGETS ...)</code>：安装规则</h3>
<h3 id="作用"><a class="markdownIt-Anchor" href="#作用"></a> 作用：</h3>
<p>定义如何安装构建出来的目标（如可执行文件、库等）到系统目录中，常用于打包部署。</p>
<h3 id="格式说明"><a class="markdownIt-Anchor" href="#格式说明"></a> 格式说明：</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cmake复制编辑install(TARGETS &lt;target1&gt; [target2 ...]</span><br><span class="line">        [RUNTIME DESTINATION dir]      # 可执行文件安装路径</span><br><span class="line">        [LIBRARY DESTINATION dir]      # 共享库安装路径（.so/.dll）</span><br><span class="line">        [ARCHIVE DESTINATION dir]      # 静态库安装路径（.a/.lib）</span><br><span class="line">        [INCLUDES DESTINATION [dir ...]]   # 头文件的安装路径</span><br><span class="line">        [PRIVATE_HEADER DESTINATION dir]   # 私有头文件的安装路径</span><br><span class="line">        [PUBLIC_HEADER DESTINATION dir])   # 公共头文件的安装路径</span><br></pre></td></tr></table></figure>
<h2 id="3-构建"><a class="markdownIt-Anchor" href="#3-构建"></a> 3 构建</h2>
<p>构建文件放在源代码目录之外的独立目录</p>
<h3 id="1"><a class="markdownIt-Anchor" href="#1"></a> 1</h3>
<h3 id="2-生成构建文件"><a class="markdownIt-Anchor" href="#2-生成构建文件"></a> 2  生成构建文件</h3>
<p>**运行 CMake 配置：**在构建目录中运行 CMake 命令，指定源代码目录。源代码目录是包含 CMakeLists.txt 文件的目录。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cmake ..</span><br></pre></td></tr></table></figure>
<p>如果需要指定生成器（如 Ninja、Visual Studio），可以使用 -G 选项。例如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cmake -G &quot;Ninja&quot; ..</span><br></pre></td></tr></table></figure>
<p>如果需要指定构建类型（如 Debug 或 Release），可以使用 -DCMAKE_BUILD_TYPE 选项。例如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cmake -DCMAKE_BUILD_TYPE=Release ..</span><br></pre></td></tr></table></figure>
<h3 id="3-编译和构建"><a class="markdownIt-Anchor" href="#3-编译和构建"></a> 3 编译和构建</h3>
<p>使用生成的<strong>构建文件</strong>进行编译和构建</p>
<p>**使用 Makefile（或类似构建系统）：**如果使用 Makefile，可以运行 make 命令来编译和构建项目。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make</span><br></pre></td></tr></table></figure>
<p>如果要构建特定的目标，可以指定目标名称。例如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make MyExecutable</span><br></pre></td></tr></table></figure>
<p>**使用 Ninja：**如果使用 Ninja 构建系统，运行 ninja 命令来编译和构建项目。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ninja</span><br></pre></td></tr></table></figure>
<p>与 make 类似，可以构建特定的目标：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ninja MyExecutable</span><br></pre></td></tr></table></figure>
<p>**使用 Visual Studio：**如果生成了 Visual Studio 工程文件，可以打开 <strong>.sln</strong> 文件，然后在 Visual Studio 中选择构建解决方案。</p>
<p>也可以使用 msbuild 命令行工具来编译：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">msbuild MyProject.sln /p:Configuration=Release</span><br></pre></td></tr></table></figure>
<h3 id="4-实例"><a class="markdownIt-Anchor" href="#4-实例"></a> 4 实例</h3>
<blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cmake -G &quot;Visual Studio 17 2022&quot; -A x64 ..</span><br></pre></td></tr></table></figure>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">cmake_minimum_required(VERSION 3.10)</span><br><span class="line">project(MyProject CXX)</span><br><span class="line"></span><br><span class="line"># 设置Visual Studio版本</span><br><span class="line">set(CMAKE_GENERATOR_TOOLSET &quot;v143&quot;)  # VS2022</span><br><span class="line">set(CMAKE_GENERATOR_PLATFORM &quot;x64&quot;)</span><br><span class="line"></span><br><span class="line">set(MY_VAR &quot;Hello World&quot;)</span><br><span class="line">message(STATUS &quot;MY_VAR: $&#123;MY_VAR&#125;&quot;)</span><br><span class="line"></span><br><span class="line"># 缓存变量</span><br><span class="line">set(MY_VAR &quot;Hello World&quot; CACHE STRING &quot;My Variable&quot;)</span><br><span class="line">message(STATUS &quot;MY_VAR: $&#123;MY_VAR&#125;&quot;)</span><br><span class="line"></span><br><span class="line"># 设置OpenCV路径</span><br><span class="line">set(OpenCV_DIR &quot;F:/Cppdata/pkg/opencv/build/x64/vc15&quot;)</span><br><span class="line">set(OpenCV_INCLUDE_DIRS &quot;F:/Cppdata/pkg/opencv/build/include&quot;)</span><br><span class="line">set(OpenCV_LIBS_DIR &quot;F:/Cppdata/pkg/opencv/build/x64/vc15/lib&quot;)</span><br><span class="line"></span><br><span class="line"># 添加 OpenCV 库</span><br><span class="line">find_package(OpenCV REQUIRED)</span><br><span class="line">include_directories($&#123;OpenCV_INCLUDE_DIRS&#125;)</span><br><span class="line">link_directories($&#123;OpenCV_LIBS_DIR&#125;)</span><br><span class="line"></span><br><span class="line"># 添加源文件</span><br><span class="line">add_executable(MyExecutable main.cpp)</span><br><span class="line"></span><br><span class="line"># 设置 C++ 标准</span><br><span class="line">set(CMAKE_CXX_STANDARD 11)</span><br><span class="line"></span><br><span class="line"># 添加 OpenCV 库</span><br><span class="line">target_link_libraries(MyExecutable $&#123;OpenCV_LIBS&#125;)</span><br><span class="line"></span><br><span class="line"># 设置输出目录</span><br><span class="line">set(CMAKE_RUNTIME_OUTPUT_DIRECTORY $&#123;CMAKE_BINARY_DIR&#125;/bin)</span><br><span class="line">set(CMAKE_LIBRARY_OUTPUT_DIRECTORY $&#123;CMAKE_BINARY_DIR&#125;/lib)</span><br><span class="line">set(CMAKE_ARCHIVE_OUTPUT_DIRECTORY $&#123;CMAKE_BINARY_DIR&#125;/lib)</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://shakewely.github.io/2025/05/14/2025%E5%B9%B45%E6%9C%8814%E6%97%A5-cmake/" data-id="cmanj8o27000tlcv4h8k61cym" class="article-share-link">分享</a>
      
      
    </footer>
  </div>
  
</article>




  
    <article id="post-2025年5月13日-TensorRT3infer" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2025/05/13/2025%E5%B9%B45%E6%9C%8813%E6%97%A5-TensorRT3infer/" class="article-date">
  <time class="post-time" datetime="2025-05-13T09:08:00.000Z" itemprop="datePublished">
    <span class="post-month">5月</span><br/>
    <span class="post-day">13</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/05/13/2025%E5%B9%B45%E6%9C%8813%E6%97%A5-TensorRT3infer/">2025年5月13日 TensorRT3infer</a>
    </h1>
  

        <div>
          
          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="tensorrt-进行推理的流程"><a class="markdownIt-Anchor" href="#tensorrt-进行推理的流程"></a> TensorRT 进行推理的流程</h2>
<blockquote>
<h2 id="一般使用-tensorrt-进行推理的流程7-步"><a class="markdownIt-Anchor" href="#一般使用-tensorrt-进行推理的流程7-步"></a> ✅ 一般使用 TensorRT 进行推理的流程（7 步）</h2>
<h3 id="1-准备模型通常为-onnx-格式"><a class="markdownIt-Anchor" href="#1-准备模型通常为-onnx-格式"></a> 🔹 1. <strong>准备模型（通常为 ONNX 格式）</strong></h3>
<ul>
<li>训练好的模型需导出为 <code>ONNX</code> 格式。</li>
<li>常见工具：
<ul>
<li>PyTorch: <code>torch.onnx.export()</code></li>
<li>TensorFlow → ONNX: 使用 <code>tf2onnx</code> 或 <code>onnx-tf</code></li>
<li>HuggingFace 模型: 直接支持 ONNX 导出</li>
</ul>
</li>
</ul>
<hr>
<h3 id="2-构建-tensorrt-引擎engine-文件"><a class="markdownIt-Anchor" href="#2-构建-tensorrt-引擎engine-文件"></a> 🔹 2. <strong>构建 TensorRT 引擎（<code>.engine</code> 文件）</strong></h3>
<p>你可以使用：</p>
<ul>
<li>
<p><strong>trtexec 命令行工具（推荐）</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bash</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">复制编辑</span><br><span class="line">trtexec --onnx=model.onnx --saveEngine=model.engine --fp16</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>TensorRT Python API（更灵活）</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">python复制编辑builder = trt.Builder(logger)</span><br><span class="line">network = builder.create_network(...)</span><br><span class="line">parser = trt.OnnxParser(network, logger)</span><br><span class="line">parser.parse_from_file(&quot;model.onnx&quot;)</span><br><span class="line">engine = builder.build_cuda_engine(network)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<blockquote>
<p>注意：此步骤可选，如果你已经有 <code>.engine</code> 文件，就无需重复构建。</p>
</blockquote>
<hr>
<h3 id="3-加载引擎文件并反序列化为可执行模型"><a class="markdownIt-Anchor" href="#3-加载引擎文件并反序列化为可执行模型"></a> 🔹 3. <strong>加载引擎文件并反序列化为可执行模型</strong></h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cpp复制编辑std::ifstream engineFile(&quot;model.engine&quot;, std::ios::binary);</span><br><span class="line">IRuntime* runtime = createInferRuntime(logger);</span><br><span class="line">ICudaEngine* engine = runtime-&gt;deserializeCudaEngine(data, size, nullptr);</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="4-创建执行上下文iexecutioncontext"><a class="markdownIt-Anchor" href="#4-创建执行上下文iexecutioncontext"></a> 🔹 4. <strong>创建执行上下文（IExecutionContext）</strong></h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cpp</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">复制编辑</span><br><span class="line">IExecutionContext* context = engine-&gt;createExecutionContext();</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="5-准备输入数据并分配-gpu-内存"><a class="markdownIt-Anchor" href="#5-准备输入数据并分配-gpu-内存"></a> 🔹 5. <strong>准备输入数据并分配 GPU 内存</strong></h3>
<ul>
<li>你需要知道输入 tensor 的 shape（通常是 <code>[1, 3, H, W]</code>）</li>
<li>分配输入/输出的 GPU 内存（<code>cudaMalloc</code>）</li>
<li>把输入数据拷贝到 GPU（<code>cudaMemcpy</code>）</li>
</ul>
<hr>
<h3 id="6-执行推理inference"><a class="markdownIt-Anchor" href="#6-执行推理inference"></a> 🔹 6. <strong>执行推理（Inference）</strong></h3>
<ul>
<li>使用 <code>enqueueV2</code>（异步）或 <code>executeV2</code>（同步）进行推理：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cpp</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">复制编辑</span><br><span class="line">context-&gt;enqueueV2(buffers, stream, nullptr);</span><br></pre></td></tr></table></figure>
<ul>
<li>注意：<code>buffers</code> 是输入/输出 tensor 指针数组。</li>
</ul>
<hr>
<h3 id="7-获取并处理输出结果"><a class="markdownIt-Anchor" href="#7-获取并处理输出结果"></a> 🔹 7. <strong>获取并处理输出结果</strong></h3>
<ul>
<li>将输出结果从 GPU 拷贝到 Host：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cpp</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">复制编辑</span><br><span class="line">cudaMemcpy(output_host, output_device, size, cudaMemcpyDeviceToHost);</span><br></pre></td></tr></table></figure>
<ul>
<li>通常后处理包括：
<ul>
<li>分类任务：取最大值 <code>argmax</code></li>
<li>检测任务：解析 bbox、类别置信度、NMS 等</li>
<li>分割任务：取最大通道作为 mask</li>
</ul>
</li>
</ul>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://shakewely.github.io/2025/05/13/2025%E5%B9%B45%E6%9C%8813%E6%97%A5-TensorRT3infer/" data-id="cmanj8o27000slcv4fgyigr7f" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/TensorRT/" rel="tag">TensorRT</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-2025年5月9日-anchor" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2025/05/09/2025%E5%B9%B45%E6%9C%889%E6%97%A5-anchor/" class="article-date">
  <time class="post-time" datetime="2025-05-09T07:33:38.000Z" itemprop="datePublished">
    <span class="post-month">5月</span><br/>
    <span class="post-day">09</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/05/09/2025%E5%B9%B45%E6%9C%889%E6%97%A5-anchor/">2025年5月9日 anchor</a>
    </h1>
  

        <div>
          
          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="流程"><a class="markdownIt-Anchor" href="#流程"></a> 流程</h2>
<p><strong>Anchor 生成</strong>：</p>
<ul>
<li>在训练过程中，为了检测不同尺度和长宽比的物体，首先会在特征图的每个位置生成一组 Anchor（锚框）。这些 Anchor 通常具有不同的尺度和长宽比，用于捕捉图像中物体的不同尺寸和形状。</li>
</ul>
<p><strong>标签分配</strong>：</p>
<ul>
<li>每个生成的 Anchor 会与真实框（ground truth box）进行比较，通常使用 <strong>IoU（Intersection over Union）</strong> 来计算重叠程度。</li>
<li>根据 IoU 的值将 Anchor 分为三类：
<ul>
<li><strong>正样本</strong>：当 Anchor 与真实框的 IoU 大于设定阈值（如 0.5）时，视为正样本。</li>
<li><strong>负样本</strong>：当 Anchor 与所有真实框的 IoU 小于设定的低阈值（如 0.4）时，视为负样本。</li>
<li><strong>忽略样本</strong>：IoU 介于这两个阈值之间的 Anchor 通常会被忽略。</li>
</ul>
</li>
</ul>
<p><strong>回归与分类</strong>：</p>
<ul>
<li>对于每个正样本 Anchor，模型需要预测一个目标框（预测框），通常通过回归来预测框的偏移量（例如，中心点坐标、宽度、高度）。</li>
<li>同时，模型对每个 Anchor 进行分类，判断该 Anchor 代表的物体类别。</li>
</ul>
<p><strong>损失函数计算</strong>：</p>
<ul>
<li>在训练时，模型通过以下两种损失函数来优化：
<ul>
<li><strong>分类损失</strong>：通常使用交叉熵损失来计算每个 Anchor 是否正确分类（正样本或负样本）。</li>
<li><strong>回归损失</strong>：通常使用平滑 L1 损失来计算预测框与真实框之间的差距。</li>
</ul>
</li>
</ul>
<p><strong>模型优化</strong>：</p>
<ul>
<li>根据计算出的损失（分类损失和回归损失），使用优化算法（如 SGD 或 Adam）来更新模型的权重，使其更好地拟合训练数据。</li>
</ul>
<p><strong>推理阶段的非极大值抑制（NMS）</strong>：</p>
<ul>
<li>在推理时，多个 Anchor 可能会检测到同一个物体，导致多个重叠的框。通过 <strong>NMS（Non-Maximum Suppression）</strong>，选择具有最高置信度的框，去除冗余框，只保留最佳的预测框。</li>
</ul>
<h2 id="anchor-生成算法"><a class="markdownIt-Anchor" href="#anchor-生成算法"></a> anchor 生成算法</h2>
<p>Faster R-CNN 通过在每个特征图位置生成多个不同尺度、不同长宽比的锚框。具体步骤如下：</p>
<ul>
<li>对于每个特征图上的位置，选择多个 <strong>尺度</strong>（如 32px、64px、128px 等）和 <strong>长宽比</strong>（如 1:1、2:1、1:2 等）。</li>
</ul>
<ol>
<li><strong>滑动窗口（Sliding Window）</strong>：</li>
</ol>
<p>在 <strong>YOLO</strong> 和 <strong>SSD</strong> 中，Anchor 的生成算法通常会在特征图的每个位置生成固定的 Anchor。具体过程包括：</p>
<ul>
<li>在每个位置生成多个尺寸和比例的 Anchor。</li>
</ul>
<h2 id="iou"><a class="markdownIt-Anchor" href="#iou"></a> IOU</h2>
<p>定义:</p>
<blockquote></blockquote>
<h2 id="nms"><a class="markdownIt-Anchor" href="#nms"></a> NMS</h2>
<p>算法流程:</p>
<blockquote></blockquote>
<h1 id="anchor-free"><a class="markdownIt-Anchor" href="#anchor-free"></a> Anchor free</h1>
<h3 id="1-目标中心预测center-prediction"><a class="markdownIt-Anchor" href="#1-目标中心预测center-prediction"></a> 1. <strong>目标中心预测（Center Prediction）</strong></h3>
<ul>
<li>在 anchor-free 方法中，最常见的一种做法是预测物体的 <strong>中心点</strong>。</li>
<li><strong>中心点</strong>：每个物体都有一个中心位置，模型的任务是预测这些中心点的位置。</li>
<li>这些方法通常会在特征图上为每个像素点预测一个 <strong>热力图（heatmap）</strong>，表示该位置是否是某个物体的中心点。这个热力图的值越高，表示该位置是物体中心的概率越大。</li>
</ul>
<h3 id="2-标签分配"><a class="markdownIt-Anchor" href="#2-标签分配"></a> 2. <strong>标签分配</strong></h3>
<p>在 anchor-free 方法中，标签分配的方式与传统的 anchor-based 方法有所不同。以下是一些常见的 anchor-free 标签分配策略：</p>
<ul>
<li><strong>正样本（Positive）</strong>：如果某个像素点的中心与某个真实框的中心位置足够接近（通常使用距离阈值），那么该像素点被视为正样本，模型的目标是回归这个位置。
<ul>
<li>例如，物体的中心点离特定像素点的距离小于某个阈值（如 5x5 区域），那么这个像素点就会被认为是正样本。</li>
</ul>
</li>
<li><strong>负样本（Negative）</strong>：那些没有包含任何物体中心的像素点被视为负样本。这些像素点的值应该接近零，表示它们不是物体中心。</li>
<li><strong>忽略样本（Ignore）</strong>：在某些情况下，靠近物体边界的像素点可能不会分配为正样本，而是被忽略。这是因为这些像素点的中心位置不明确，可能会影响模型的训练。</li>
</ul>
<h3 id="3-回归目标"><a class="markdownIt-Anchor" href="#3-回归目标"></a> 3. <strong>回归目标</strong></h3>
<p>对于每个正样本像素点，模型需要预测：</p>
<ul>
<li><strong>物体的中心点</strong>：通过回归来预测目标的中心位置。</li>
<li><strong>物体的尺寸和形状</strong>：例如，预测物体的宽度、高度、方向（如果是旋转框）等。</li>
</ul>
<h3 id="4-热力图损失heatmap-loss"><a class="markdownIt-Anchor" href="#4-热力图损失heatmap-loss"></a> 4. <strong>热力图损失（Heatmap Loss）</strong></h3>
<p>在 anchor-free 方法中，模型通常会通过 <strong>热力图</strong> 来预测目标中心的概率。每个像素点的热力图值表示该位置是目标中心的概率。</p>
<ul>
<li><strong>热力图损失</strong>（通常使用 <strong>均方误差（MSE）</strong> 或 <strong>交叉熵损失</strong>）会计算预测的热力图与真实热力图之间的差异。</li>
<li>对于正样本，目标热力图在该位置会有较高的值，负样本则会有接近零的值。</li>
</ul>
<h3 id="5-回归损失regression-loss"><a class="markdownIt-Anchor" href="#5-回归损失regression-loss"></a> 5. <strong>回归损失（Regression Loss）</strong></h3>
<p>对于每个正样本，模型不仅预测中心点，还需要回归物体的 <strong>边界框</strong>（例如，物体的宽度、高度，或旋转框的角度）。回归损失通常使用平滑L1损失（smooth L1 loss）或其他回归损失函数来进行计算。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://shakewely.github.io/2025/05/09/2025%E5%B9%B45%E6%9C%889%E6%97%A5-anchor/" data-id="cmanj8o2d0017lcv46e4a18ey" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" rel="tag">目标检测</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-2025年5月8日-MOT汇总" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2025/05/07/2025%E5%B9%B45%E6%9C%888%E6%97%A5-MOT%E6%B1%87%E6%80%BB/" class="article-date">
  <time class="post-time" datetime="2025-05-07T08:03:06.000Z" itemprop="datePublished">
    <span class="post-month">5月</span><br/>
    <span class="post-day">07</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/05/07/2025%E5%B9%B45%E6%9C%888%E6%97%A5-MOT%E6%B1%87%E6%80%BB/">2025年5月8日 MOT汇总</a>
    </h1>
  

        <div>
          
          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>目前，<code>Tracking-by-detection</code>成为 <code>MOT</code> 任务中最有效的范式。<code>Tracking-by-detection</code>包含一个步骤检测步骤，然后是一个跟踪步骤。跟踪步骤通常由2个主要部分组成：</p>
<ol>
<li>运动模型和状态估计，用于预测后续帧中轨迹的边界框。<code>卡尔曼滤波器</code> (KF) 是此任务的主流选择。</li>
<li>将新帧检测与当前轨迹集相关联。</li>
</ol>
<p>对于步骤2：有2种主要的方法用于处理关联任务：</p>
<ol>
<li>目标的定位，主要是预测轨迹边界框和检测边界框之间的IoU。</li>
<li>目标的外观模型和解决Re-ID任务。</li>
</ol>
<p>这2种方法都被量化为距离，并用于将关联任务作为全局分配问题来解决。</p>
<p><img src="/2025/05/07/2025%E5%B9%B45%E6%9C%888%E6%97%A5-MOT%E6%B1%87%E6%80%BB/image-20250508093348216.png" alt="image-20250508093348216"></p>
<h1 id="dbt"><a class="markdownIt-Anchor" href="#dbt"></a> DBT</h1>
<h2 id="bytetrack"><a class="markdownIt-Anchor" href="#bytetrack"></a> Bytetrack</h2>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://shakewely.github.io/2025/05/07/2025%E5%B9%B45%E6%9C%888%E6%97%A5-MOT%E6%B1%87%E6%80%BB/" data-id="cmanj8o290012lcv49q4h77kl" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tracking/" rel="tag">tracking</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-2025年5月7日-ReID汇总" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2025/05/07/2025%E5%B9%B45%E6%9C%887%E6%97%A5-ReID%E6%B1%87%E6%80%BB/" class="article-date">
  <time class="post-time" datetime="2025-05-07T08:02:32.000Z" itemprop="datePublished">
    <span class="post-month">5月</span><br/>
    <span class="post-day">07</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/05/07/2025%E5%B9%B45%E6%9C%887%E6%97%A5-ReID%E6%B1%87%E6%80%BB/">2025年5月7日 ReID汇总</a>
    </h1>
  

        <div>
          
          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="重识别可以分为基于表征学习的方法和基于度量学习的方法"><a class="markdownIt-Anchor" href="#重识别可以分为基于表征学习的方法和基于度量学习的方法"></a> 重识别可以分为基于表征学习的方法和基于度量学习的方法</h3>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://shakewely.github.io/2025/05/07/2025%E5%B9%B45%E6%9C%887%E6%97%A5-ReID%E6%B1%87%E6%80%BB/" data-id="cmanj8o290010lcv4cluh0rpw" class="article-share-link">分享</a>
      
      
    </footer>
  </div>
  
</article>




  
    <article id="post-2025年5月5日-torch汇总" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2025/05/05/2025%E5%B9%B45%E6%9C%885%E6%97%A5-torch%E6%B1%87%E6%80%BB/" class="article-date">
  <time class="post-time" datetime="2025-05-05T02:35:44.000Z" itemprop="datePublished">
    <span class="post-month">5月</span><br/>
    <span class="post-day">05</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/05/05/2025%E5%B9%B45%E6%9C%885%E6%97%A5-torch%E6%B1%87%E6%80%BB/">2025年5月5日 torch汇总</a>
    </h1>
  

        <div>
          
          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="module"><a class="markdownIt-Anchor" href="#module"></a> Module</h1>
<p>mymodule= MyModule()</p>
<p><code>Mymodule.bn.weight</code></p>
<p>mymodule.bn.weight.data</p>
<h1 id="parameters"><a class="markdownIt-Anchor" href="#parameters"></a> parameters</h1>
<p>卷积核（<code>nn.Conv2d</code>）的权重维度是 <strong>[out_channels, in_channels, kernel_height, kernel_width]</strong></p>
<blockquote>
<p>RuntimeError: Given groups=1, weight of size [6, 3, 3, 3], expected input[1, 640, 640, 3] to have 3 channels, but got 640 channels instead</p>
</blockquote>
<p><strong>model.parameters()</strong></p>
<h1 id="grad"><a class="markdownIt-Anchor" href="#grad"></a> Grad</h1>
<h3 id="requires_grad"><a class="markdownIt-Anchor" href="#requires_grad"></a> requires_grad</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.bn.weight.requires_grad</span><br></pre></td></tr></table></figure>
<h1 id="钩子函数"><a class="markdownIt-Anchor" href="#钩子函数"></a> 钩子函数</h1>
<blockquote>
<p><strong>钩子函数（Hook Function）*<em>是可以绑定到 <code>nn.Module</code> 的某一层（Layer）上的函数，在*<em>前向传播（forward）或反向传播（backward）时自动执行</em></em>，用来</strong>提取中间特征、修改梯度或中间输出、调试模型等**。</p>
<h3 id="什么是钩子函数hook-function"><a class="markdownIt-Anchor" href="#什么是钩子函数hook-function"></a> ✅ 什么是钩子函数（Hook Function）？</h3>
<p>在 PyTorch 中，<strong>钩子函数（Hook Function）*<em>是可以绑定到 <code>nn.Module</code> 的某一层（Layer）上的函数，在*<em>前向传播（forward）或反向传播（backward）时自动执行</em></em>，用来</strong>提取中间特征、修改梯度或中间输出、调试模型等**。</p>
<hr>
<h3 id="pytorch-中的钩子种类"><a class="markdownIt-Anchor" href="#pytorch-中的钩子种类"></a> 📌 PyTorch 中的钩子种类：</h3>
<table>
<thead>
<tr>
<th>类型</th>
<th>方法</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td>前向钩子</td>
<td><code>register_forward_hook()</code></td>
<td>在该层 <strong>forward输出后</strong> 自动调用</td>
</tr>
<tr>
<td>前向前钩子</td>
<td><code>register_forward_pre_hook()</code></td>
<td>在该层 <strong>forward输入前</strong> 调用</td>
</tr>
<tr>
<td>反向钩子</td>
<td><code>register_backward_hook()</code>（不推荐）或 <code>register_full_backward_hook()</code></td>
<td>在该层 <strong>backward执行时</strong> 调用</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="为什么要注册钩子函数"><a class="markdownIt-Anchor" href="#为什么要注册钩子函数"></a> 🧠 为什么要注册钩子函数？</h3>
<p>你这段代码中使用 <code>register_forward_hook()</code>，是为了在<strong>蒸馏过程中提取教师模型和学生模型某些层的输出特征</strong>，计算特征之间的差异作为蒸馏损失。</p>
</blockquote>
<p><code>.data</code></p>
<h1 id="tensor"><a class="markdownIt-Anchor" href="#tensor"></a> tensor</h1>
<p>1️⃣ view(shape)✅<br>
改变张量形状（reshape），但不改变内存布局。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(2,3,4)</span><br><span class="line">y = x.view(6,4)</span><br><span class="line">print(y.shape)  # torch.Size([6, 4])</span><br></pre></td></tr></table></figure>
<hr>
<p>2️⃣ <strong>reshape(shape)</strong><br>
类似 <code>view</code>，但更灵活，支持非连续内存。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">python复制编辑x = torch.randn(2,3,4)</span><br><span class="line">y = x.reshape(6,4)</span><br><span class="line">print(y.shape)  # torch.Size([6, 4])</span><br></pre></td></tr></table></figure>
<hr>
<p>3️⃣ <strong>squeeze(dim=None)</strong><br>
去掉所有为1的维度，或者指定维度的1。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">python复制编辑x = torch.randn(2,1,3)</span><br><span class="line">y = x.squeeze()</span><br><span class="line">print(y.shape)  # torch.Size([2,3])</span><br><span class="line">y2 = x.squeeze(1)</span><br><span class="line">print(y2.shape)  # torch.Size([2,3])</span><br></pre></td></tr></table></figure>
<hr>
<p>4️⃣ <strong>unsqueeze(dim)</strong><br>
在指定位置插入维度1。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">python复制编辑x = torch.randn(2,3)</span><br><span class="line">y = x.unsqueeze(1)</span><br><span class="line">print(y.shape)  # torch.Size([2,1,3])</span><br></pre></td></tr></table></figure>
<hr>
<p>5️⃣ **transpose(dim0, dim1)**✅<br>
只交换两个维度。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(2,3,4)</span><br><span class="line">y = x.transpose(1,2)</span><br><span class="line">print(y.shape)  # torch.Size([2,4,3])</span><br></pre></td></tr></table></figure>
<hr>
<p>6️⃣ **permute(dims)**✅<br>
任意维度换序。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(2,3,4)</span><br><span class="line">y = x.permute(0,2,1)</span><br><span class="line">print(y.shape)  # torch.Size([2,4,3])</span><br></pre></td></tr></table></figure>
<hr>
<p>7️⃣ <strong>contiguous()</strong><br>
返回内存连续的 tensor（一般 <code>view</code> 前用）。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">python复制编辑x = torch.randn(2,3,4).transpose(1,2)</span><br><span class="line">y = x.contiguous().view(2,12)</span><br><span class="line">print(y.shape)  # torch.Size([2,12])</span><br></pre></td></tr></table></figure>
<hr>
<p>8️⃣ <strong>to(device)</strong><br>
转移到指定设备。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python复制编辑x = torch.randn(2,3)</span><br><span class="line">y = x.to(&#x27;cuda&#x27;)</span><br></pre></td></tr></table></figure>
<hr>
<p>9️⃣ <strong>cpu() / cuda()</strong><br>
简写版的 to(‘cpu’) 或 to(‘cuda’)</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python复制编辑x = torch.randn(2,3).cuda()</span><br><span class="line">y = x.cpu()</span><br></pre></td></tr></table></figure>
<hr>
<p>🔟 <strong>clone()</strong><br>
复制 tensor（新开内存）。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python复制编辑x = torch.randn(2,3)</span><br><span class="line">y = x.clone()</span><br></pre></td></tr></table></figure>
<hr>
<p>1️⃣1️⃣ <strong>detach()</strong><br>
返回一个不参与计算图的新 tensor。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python复制编辑x = torch.randn(2,3, requires_grad=True)</span><br><span class="line">y = x.detach()</span><br></pre></td></tr></table></figure>
<hr>
<p>1️⃣2️⃣ <strong>item()</strong><br>
返回单元素 tensor 的 python 数值。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python复制编辑x = torch.tensor([3.14])</span><br><span class="line">v = x.item()  # 3.14 (float)</span><br></pre></td></tr></table></figure>
<hr>
<p>1️⃣3️⃣ <strong>numpy()</strong><br>
转为 numpy 数组（共享内存）。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python复制编辑x = torch.randn(2,3)</span><br><span class="line">arr = x.numpy()</span><br></pre></td></tr></table></figure>
<hr>
<p>1️⃣4️⃣ <strong>size(dim=None)</strong><br>
返回维度大小，dim=None 返回全部 shape。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">python复制编辑x = torch.randn(2,3,4)</span><br><span class="line">print(x.size())     # torch.Size([2,3,4])</span><br><span class="line">print(x.size(1))    # 3</span><br></pre></td></tr></table></figure>
<hr>
<p>1️⃣5️⃣ <strong>shape</strong><br>
属性，等价于 <code>size()</code>。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python复制编辑x = torch.randn(2,3,4)</span><br><span class="line">print(x.shape)  # torch.Size([2,3,4])</span><br></pre></td></tr></table></figure>
<hr>
<p>1️⃣6️⃣ <strong>numel()</strong><br>
张量元素总数。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python复制编辑x = torch.randn(2,3,4)</span><br><span class="line">print(x.numel())  # 24</span><br></pre></td></tr></table></figure>
<h1 id="torchcuda"><a class="markdownIt-Anchor" href="#torchcuda"></a> torch.cuda</h1>
<p>1️⃣ <strong>torch.cuda.is_available()</strong><br>
检查当前 PyTorch 是否可以使用 GPU。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">python</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">复制编辑</span><br><span class="line">torch.cuda.is_available()  # True / False</span><br></pre></td></tr></table></figure>
<hr>
<p>2️⃣ <strong>torch.cuda.device_count()</strong><br>
返回可用的 GPU 数量。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">python</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">复制编辑</span><br><span class="line">torch.cuda.device_count()  # 如 2</span><br></pre></td></tr></table></figure>
<hr>
<p>3️⃣ <strong>torch.cuda.get_device_name(device=0)</strong><br>
获取指定 GPU 的名称。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">python</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">复制编辑</span><br><span class="line">torch.cuda.get_device_name(0)  # &#x27;NVIDIA GeForce RTX 4090&#x27;</span><br></pre></td></tr></table></figure>
<hr>
<p>4️⃣ <strong>torch.cuda.current_device()</strong><br>
返回当前默认设备的索引（int）。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">python</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">复制编辑</span><br><span class="line">torch.cuda.current_device()  # 如 0</span><br></pre></td></tr></table></figure>
<hr>
<p>5️⃣ <strong>torch.cuda.set_device(device)</strong><br>
设置当前默认使用的 GPU。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">python</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">复制编辑</span><br><span class="line">torch.cuda.set_device(1)</span><br></pre></td></tr></table></figure>
<hr>
<p>6️⃣ <strong>torch.cuda.memory_allocated(device=0)</strong><br>
返回当前分配给张量的显存大小（单位：字节）。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">python</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">复制编辑</span><br><span class="line">torch.cuda.memory_allocated(0) / 1024**2  # 单位：MB</span><br></pre></td></tr></table></figure>
<hr>
<p>7️⃣ <strong>torch.cuda.memory_reserved(device=0)</strong><br>
返回为分配预留的显存（用于缓存机制）。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">python</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">复制编辑</span><br><span class="line">torch.cuda.memory_reserved(0) / 1024**2  # 单位：MB</span><br></pre></td></tr></table></figure>
<hr>
<p>8️⃣ <strong>torch.cuda.empty_cache()</strong><br>
释放未使用的显存缓存（不会释放已使用的显存）。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">python</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">复制编辑</span><br><span class="line">torch.cuda.empty_cache()</span><br></pre></td></tr></table></figure>
<hr>
<p>9️⃣ <strong>torch.cuda.synchronize(device=None)</strong><br>
等待所有 GPU 操作完成（用于计时时精准）。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">python</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">复制编辑</span><br><span class="line">torch.cuda.synchronize()</span><br></pre></td></tr></table></figure>
<hr>
<p>🔟 <strong>torch.cuda.Stream / torch.cuda.Event</strong><br>
用于手动控制 CUDA 操作顺序（高级用法）。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python复制编辑stream = torch.cuda.Stream()</span><br><span class="line">event = torch.cuda.Event()</span><br></pre></td></tr></table></figure>
<h1 id="model"><a class="markdownIt-Anchor" href="#model"></a> model</h1>
<p><strong><code>m.weight</code></strong>：获取<code>BatchNorm2d</code>层的权重参数。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w = m.weight.abs().detach()</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b = m.bias.abs().detach()</span><br></pre></td></tr></table></figure>
<p>1️⃣ **<code>model.parameters()</code>**✅<br>
返回一个生成器，只包含模型中所有参数（<code>nn.Parameter</code> 类型）。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">for param in model.parameters():</span><br><span class="line">    print(param.shape)</span><br><span class="line">    param.nelement() * param.element_size()</span><br></pre></td></tr></table></figure>
<hr>
<p>2️⃣ <strong><code>model.named_parameters()</code></strong> ✅<br>
返回 <code>(name, param)</code> 元组的生成器，带有名字。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">]for name, param in model.named_parameters():</span><br><span class="line">    print(name, param.shape)</span><br></pre></td></tr></table></figure>
<hr>
<p>3️⃣ <strong><code>model.children()</code></strong><br>
返回模型的<strong>第一层子模块</strong>，不递归。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python复制编辑for child in model.children():</span><br><span class="line">    print(child)  # 只是一层，例如 Conv, ReLU, Linear</span><br></pre></td></tr></table></figure>
<hr>
<p>4️⃣ <strong><code>model.named_children()</code></strong><br>
返回 <code>(name, module)</code> 的生成器，第一层子模块带名字。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python复制编辑for name, module in model.named_children():</span><br><span class="line">    print(name, module)</span><br></pre></td></tr></table></figure>
<hr>
<p>5️⃣ <strong><code>model.modules()</code></strong><br>
返回模型中所有模块（包括自身和递归子模块）。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python复制编辑for module in model.modules():</span><br><span class="line">    print(type(module))</span><br></pre></td></tr></table></figure>
<hr>
<p>6️⃣ <strong><code>model.named_modules()</code></strong><br>
返回 <code>(name, module)</code> 形式的所有模块（递归 + 包括自身）。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python复制编辑for name, module in model.named_modules():</span><br><span class="line">    print(name, module)</span><br></pre></td></tr></table></figure>
<hr>
<p>7️⃣ <strong><code>model.state_dict()</code></strong><br>
返回包含所有参数和缓冲区（如 <code>running_mean</code>）的字典。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python复制编辑for name, tensor in model.state_dict().items():</span><br><span class="line">    print(name, tensor.shape)</span><br></pre></td></tr></table></figure>
<hr>
<p>8️⃣ <strong><code>model.load_state_dict(state_dict)</code></strong><br>
加载 <code>.pth</code> 文件保存的权重。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">python</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">复制编辑</span><br><span class="line">model.load_state_dict(torch.load(&#x27;xxx.pth&#x27;))</span><br></pre></td></tr></table></figure>
<hr>
<p>9️⃣ <strong><code>model.eval()</code> / <code>model.train()</code></strong><br>
切换模型模式（影响 Dropout / BatchNorm 等）。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python复制编辑model.eval()   # 推理模式</span><br><span class="line">model.train()  # 训练模式</span><br></pre></td></tr></table></figure>
<hr>
<p>🔟 <strong><code>model.zero_grad()</code></strong><br>
清空所有参数的梯度（等效于 <code>optimizer.zero_grad()</code>）。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">python</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">复制编辑</span><br><span class="line">model.zero_grad()</span><br></pre></td></tr></table></figure>
<h1 id="torch"><a class="markdownIt-Anchor" href="#torch"></a> torch</h1>
<p><code>torch.sort</code> 是 PyTorch 中用于对张量进行排序的函数。</p>
<ul>
<li><code>descending=True</code> 表示按降序排序。</li>
<li>这个函数返回一个元组 <code>(sorted_tensor, indices)</code>，其中 <code>sorted_tensor</code> 是排序后的张量，<code>indices</code> 是排序后的索引。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line"># 创建一个二维张量</span><br><span class="line">tensor = torch.tensor([[0.1, 0.5, 0.3], [0.9, 0.7, 0.2]])</span><br><span class="line"></span><br><span class="line"># 设置条件</span><br><span class="line">condition = tensor &gt; 0.4</span><br><span class="line"></span><br><span class="line"># 使用 torch.where 找到满足条件的元素的索引</span><br><span class="line">indices = torch.where(condition)</span><br><span class="line"></span><br><span class="line">print(&quot;原始张量:&quot;, tensor)</span><br><span class="line">print(&quot;条件:&quot;, condition)</span><br><span class="line">print(&quot;返回的索引:&quot;, indices)</span><br></pre></td></tr></table></figure>
<p>1️⃣ <strong>torch.stack(preds_cls)</strong><br>
把 <code>preds_cls</code> 列表拼起来，增加一个新维度。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python复制编辑preds_cls = [cls1, cls2, cls3]  # 每个 cls shape (B, C, H, W)</span><br><span class="line">torch.stack(preds_cls) → (3, B, C, H, W)</span><br></pre></td></tr></table></figure>
<p>等价于 <code>torch.stack(preds_cls, dim=0)</code></p>
<hr>
<p>2️⃣ <strong>weights_cls.view(-1,1,1,1,1)</strong><br>
把 <code>(N,)</code> 的权重 reshape 成 <code>(N,1,1,1,1)</code>，为了和 <code>(N, B, C, H, W)</code> 广播相乘。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python复制编辑weights_cls = torch.tensor([0.2, 0.3, 0.5])  # (3,)</span><br><span class="line">weights_cls.view(-1,1,1,1,1) → (3,1,1,1,1)</span><br></pre></td></tr></table></figure>
<hr>
<p>3️⃣ <strong>torch.sum(…, dim=0)</strong><br>
沿第0维（分支维度）求和，得到融合后的最终预测。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">python</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">复制编辑</span><br><span class="line">final_out = torch.sum(preds_cls * weights_cls, dim=0)  # (B, C, H, W)</span><br></pre></td></tr></table></figure>
<hr>
<p>4️⃣ <strong>einsum 计算</strong><br>
用 <code>torch.einsum</code> 表达更紧凑的计算。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">python</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">复制编辑</span><br><span class="line">torch.einsum(&#x27;n,nbchw-&gt;bchw&#x27;, weights_cls, preds_cls)</span><br></pre></td></tr></table></figure>
<p>等价于：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">python</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">复制编辑</span><br><span class="line">torch.sum(weights_cls.view(-1,1,1,1,1) * preds_cls, dim=0)</span><br></pre></td></tr></table></figure>
<hr>
<p>5️⃣ <strong><em>*init*</em>(512) → forward(z, x)</strong><br>
初始化 <code>SiameseRPN</code>，512 是 Conv3 的通道数。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python复制编辑self.rpn3 = SiameseRPN(512)</span><br><span class="line">cls_out, reg_out = self.rpn3(z, x)  # z, x 是 Conv3 特征</span><br></pre></td></tr></table></figure>
<hr>
<p>6️⃣ <strong>torch.chunk(x, 2, dim=1)</strong><br>
把 <code>x</code> 在通道维（dim=1）一分为二。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python复制编辑x1, x2 = torch.chunk(x, 2, dim=1)</span><br><span class="line"># x1, x2 → (B, C//2, H, W)</span><br></pre></td></tr></table></figure>
<hr>
<p>7️⃣ <strong>torch.eye(H, device=x.device, dtype=x.dtype)</strong><br>
生成一个 H × H 的单位矩阵。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">I = torch.eye(H, device=x.device, dtype=x.dtype)  # (H, H)</span><br></pre></td></tr></table></figure>
<p>8️⃣ <strong>torch.cat(tensors, dim)</strong><br>
把 tensor 列表按指定维拼接起来（不增加新维度）。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python复制编辑tensors = [t1, t2, t3]  # 每个 t shape (B, C, H, W)</span><br><span class="line">torch.cat(tensors, dim=1) → (B, C*3, H, W)</span><br></pre></td></tr></table></figure>
<hr>
<p>9️⃣ <strong>torch.unsqueeze(x, dim)</strong><br>
在 dim 位置增加一个新维度。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python复制编辑x.shape → (B, C, H, W)</span><br><span class="line">torch.unsqueeze(x, dim=1) → (B, 1, C, H, W)</span><br></pre></td></tr></table></figure>
<hr>
<p>🔟 <strong>torch.squeeze(x, dim)</strong><br>
去掉 dim 维度上的1维。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python复制编辑x.shape → (B, 1, C, H, W)</span><br><span class="line">torch.squeeze(x, dim=1) → (B, C, H, W)</span><br></pre></td></tr></table></figure>
<hr>
<p>1️⃣1️⃣ <strong>torch.permute(x, dims)</strong><br>
交换维度顺序。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python复制编辑x.shape → (B, C, H, W)</span><br><span class="line">torch.permute(x, (0,2,3,1)) → (B, H, W, C)</span><br></pre></td></tr></table></figure>
<hr>
<p>1️⃣2️⃣ <strong>torch.transpose(x, dim0, dim1)</strong><br>
交换两个维度。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python复制编辑x.shape → (B, C, H, W)</span><br><span class="line">torch.transpose(x, 1, 2) → (B, H, C, W)</span><br></pre></td></tr></table></figure>
<hr>
<p>1️⃣3️⃣ <strong>torch.mean(x, dim, keepdim=False)</strong><br>
在指定维求平均。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python复制编辑x.shape → (B, C, H, W)</span><br><span class="line">torch.mean(x, dim=2) → (B, C, W)</span><br></pre></td></tr></table></figure>
<hr>
<p>1️⃣4️⃣ <strong>torch.max(x, dim, keepdim=False)</strong><br>
在指定维取最大值（返回值和索引）。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">python复制编辑vals, idxs = torch.max(x, dim=2)</span><br><span class="line">vals.shape → (B, C, W)</span><br><span class="line">idxs.shape → (B, C, W)</span><br></pre></td></tr></table></figure>
<hr>
<p>1️⃣5️⃣ <strong>torch.clamp(x, min, max)</strong><br>
把值裁剪到 [min, max] 区间。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python复制编辑x = torch.tensor([-1, 0.5, 2])</span><br><span class="line">torch.clamp(x, 0, 1) → tensor([0., 0.5, 1.])</span><br></pre></td></tr></table></figure>
<hr>
<p>1️⃣6️⃣ <strong>torch.where(condition, x, y)</strong><br>
按条件选择，condition=True 取 x，否则取 y。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">python复制编辑a = torch.tensor([1, 2, 3])</span><br><span class="line">b = torch.tensor([10, 20, 30])</span><br><span class="line">cond = a &gt; 1</span><br><span class="line">torch.where(cond, a, b) → tensor([10, 2, 3])</span><br></pre></td></tr></table></figure>
<hr>
<p>1️⃣7️⃣ <strong>torch.linspace(start, end, steps)</strong><br>
生成等间隔序列。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">python</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">复制编辑</span><br><span class="line">torch.linspace(0,1,5) → tensor([0., 0.25, 0.5, 0.75, 1.])</span><br></pre></td></tr></table></figure>
<hr>
<p>1️⃣8️⃣ <strong>torch.arange(start, end, step)</strong><br>
生成等步长序列。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">python</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">复制编辑</span><br><span class="line">torch.arange(0,5,1) → tensor([0,1,2,3,4])</span><br></pre></td></tr></table></figure>
<hr>
<p>1️⃣9️⃣ <strong>torch.nn.functional.one_hot(indices, num_classes)</strong><br>
把索引转为 one-hot 编码。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">python复制编辑idx = torch.tensor([0,2,1])</span><br><span class="line">torch.nn.functional.one_hot(idx, num_classes=3) → </span><br><span class="line">tensor([[1,0,0],</span><br><span class="line">        [0,0,1],</span><br><span class="line">        [0,1,0]])</span><br></pre></td></tr></table></figure>
<hr>
<p>2️⃣0️⃣ <strong>x.view_as(y)</strong><br>
把 x reshape 成和 y 相同的形状。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">python复制编辑x = torch.arange(6)</span><br><span class="line">y = torch.zeros(2,3)</span><br><span class="line">x.view_as(y) → shape (2,3)</span><br></pre></td></tr></table></figure>
<p>1️⃣5️⃣ <strong>torch.cuda.amp.autocast</strong><br>
混合精度推理 / 训练。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python复制编辑with torch.cuda.amp.autocast():</span><br><span class="line">    output = model(input)</span><br></pre></td></tr></table></figure>
<hr>
<p>1️⃣6️⃣ <strong>torch.nn.utils.clip_grad_norm_</strong><br>
梯度裁剪（防止梯度爆炸）。</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">python</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">复制编辑</span><br><span class="line">torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)</span><br></pre></td></tr></table></figure>
<p>inplace=False</p>
<p>RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [32, 2048, 32, 32]], which is output 0 of ReluBackward0, is at version 3; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).</p>
<p>RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR</p>
<p>GPU 内存不足或 CUDA 操作不当导致的。让我们通过以下方式解决这个问题</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">    if is_pretrain:</span><br><span class="line"></span><br><span class="line">​      \# 预训练时使用较小的学习率</span><br><span class="line"></span><br><span class="line">​      self.optimizer = optim.Adam(model.parameters(), lr=1e-5)</span><br><span class="line"></span><br><span class="line">​    else:</span><br><span class="line"></span><br><span class="line">​      \# 微调时使用较大的学习率</span><br><span class="line"></span><br><span class="line">​      self.optimizer = optim.Adam(model.parameters(), lr=1e-4)</span><br></pre></td></tr></table></figure>
<h3 id="小结"><a class="markdownIt-Anchor" href="#小结"></a> 📍 小结</h3>
<table>
<thead>
<tr>
<th>占用部分</th>
<th>推理占比</th>
<th>训练占比</th>
</tr>
</thead>
<tbody>
<tr>
<td>参数</td>
<td>✅</td>
<td>✅</td>
</tr>
<tr>
<td>特征图</td>
<td>✅</td>
<td>✅✅✅</td>
</tr>
<tr>
<td>梯度</td>
<td>❌</td>
<td>✅</td>
</tr>
<tr>
<td>优化器状态</td>
<td>❌</td>
<td>✅</td>
</tr>
<tr>
<td>数据缓存</td>
<td>✅（CPU）</td>
<td>✅（CPU）</td>
</tr>
</tbody>
</table>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">🌟 深度学习中，哪些因素和内存（memory，占比）有关？</span><br><span class="line">这里说的内存主要指 显存（GPU memory），但有些原则也适用于 CPU 内存。</span><br><span class="line"></span><br><span class="line">我们可以把它拆成几部分：</span><br><span class="line"></span><br><span class="line">✅ 1. 模型参数（parameters）</span><br><span class="line">包含权重、偏置（weights, biases），每个参数通常用 float32（4字节）、float16（2字节）等存储。</span><br><span class="line"></span><br><span class="line">只要模型加载到 GPU，就占用固定显存。</span><br><span class="line"></span><br><span class="line">例子：</span><br><span class="line"></span><br><span class="line">YOLOv8n → ~3M 参数，约 12MB (float32)</span><br><span class="line"></span><br><span class="line">YOLOv8x → ~70M 参数，约 280MB (float32)</span><br><span class="line"></span><br><span class="line">→ 和模型规模直接相关，占固定部分显存。</span><br><span class="line"></span><br><span class="line">✅ 2. 中间特征图（feature maps / activations）</span><br><span class="line">网络前向推理中每层产生的输出张量。</span><br><span class="line"></span><br><span class="line">在训练中需要保存这些特征图来做反向传播，因此训练显存需求大约是推理的 2~3 倍。</span><br><span class="line"></span><br><span class="line">这是训练中主要吃显存的部分。</span><br><span class="line"></span><br><span class="line">影响因素：</span><br><span class="line"></span><br><span class="line">输入图像大小（H × W）</span><br><span class="line"></span><br><span class="line">batch size</span><br><span class="line"></span><br><span class="line">模型的通道数、分辨率、层数</span><br><span class="line"></span><br><span class="line">例子：</span><br><span class="line">640×640 图像，batch=1，YOLOv8n 占用 ≈12GB</span><br><span class="line">640×640 图像，batch=8，YOLOv8n 占用 ≈810GB</span><br><span class="line"></span><br><span class="line">✅ 3. 梯度（gradients）</span><br><span class="line">训练时需要为每个参数存储梯度，用于优化器更新权重。</span><br><span class="line"></span><br><span class="line">通常大小 ≈ 参数大小。</span><br><span class="line"></span><br><span class="line">推理时不需要。</span><br><span class="line"></span><br><span class="line">例子：</span><br><span class="line"></span><br><span class="line">参数 100MB → 梯度也大约 100MB</span><br><span class="line"></span><br><span class="line">✅ 4. 优化器状态（optimizer state）</span><br><span class="line">只有训练时才有，例如 Adam 优化器要存动量和方差。</span><br><span class="line"></span><br><span class="line">通常是参数的 2~4 倍大小。</span><br><span class="line"></span><br><span class="line">例子：</span><br><span class="line"></span><br><span class="line">Adam 优化器，100MB 参数 → 额外约 200MB 优化器状态</span><br><span class="line"></span><br><span class="line">✅ 5. 输入数据缓存（data loading buffer）</span><br><span class="line">如果用 dataloader 预加载数据（比如 PyTorch DataLoader 的 num_workers 大、pin_memory=True），可能占用比较多 CPU 内存。</span><br><span class="line"></span><br><span class="line">对 GPU 内存影响较小，但需要注意。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">占用来源	主要和什么有关</span><br><span class="line">模型权重	模型规模（n、s、m、l、x）</span><br><span class="line">输入数据缓存	batch_size, num_workers, 图像分辨率</span><br><span class="line">数据增强	增强方式、复杂程度</span><br><span class="line">推理输出缓存	推理任务规模、是否及时落盘</span><br><span class="line">框架开销	框架本身（PyTorch, TensorFlow）</span><br><span class="line">系统多进程/线程	任务数、进程数、线程数</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>F.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://shakewely.github.io/2025/05/05/2025%E5%B9%B45%E6%9C%885%E6%97%A5-torch%E6%B1%87%E6%80%BB/" data-id="cmanj8o28000wlcv4bsx0grzy" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/dl%EF%BC%8Cpyhton/" rel="tag">dl，pyhton</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-2025年5月4日-yolo汇总" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2025/05/04/2025%E5%B9%B45%E6%9C%884%E6%97%A5-yolo%E6%B1%87%E6%80%BB/" class="article-date">
  <time class="post-time" datetime="2025-05-04T07:42:49.000Z" itemprop="datePublished">
    <span class="post-month">5月</span><br/>
    <span class="post-day">04</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/05/04/2025%E5%B9%B45%E6%9C%884%E6%97%A5-yolo%E6%B1%87%E6%80%BB/">2025年5月4日 yolo细节详解</a>
    </h1>
  

        <div>
          
          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="1yolo版本"><a class="markdownIt-Anchor" href="#1yolo版本"></a> 1.Yolo版本</h1>
<h2 id="yolo-v1"><a class="markdownIt-Anchor" href="#yolo-v1"></a> YOLO V1</h2>
<h2 id="yolo-v5"><a class="markdownIt-Anchor" href="#yolo-v5"></a> YOLO V5</h2>
<h3 id="csp"><a class="markdownIt-Anchor" href="#csp"></a> CSP</h3>
<p>定义: Conv + N个Res模块 + Conv</p>
<blockquote>
<p>作用: 特征增强</p>
</blockquote>
<p>组成:</p>
<ol>
<li>2</li>
<li>2</li>
<li>2</li>
<li>2</li>
</ol>
<p>优点:</p>
<p>区别:</p>
<p>细节:</p>
<ol>
<li></li>
</ol>
<h3 id="spp"><a class="markdownIt-Anchor" href="#spp"></a> SPP</h3>
<p>定义:</p>
<blockquote>
<p>作用: 特征增强+全局信息</p>
</blockquote>
<p>组成:</p>
<ol>
<li>多个不同k_size的maxpooling 进行concat</li>
</ol>
<p>区别:</p>
<h3 id="loss"><a class="markdownIt-Anchor" href="#loss"></a> loss</h3>
<p><code>[B, H, W, num_anchors*(4+cls+obj)]</code></p>
<h3 id="创新处理操作"><a class="markdownIt-Anchor" href="#创新处理操作"></a> 创新处理操作</h3>
<h4 id="1-切片"><a class="markdownIt-Anchor" href="#1-切片"></a> 1 切片</h4>
<p>2</p>
<h2 id="yolo-v8"><a class="markdownIt-Anchor" href="#yolo-v8"></a> YOLO V8</h2>
<blockquote>
<p>特点: 改进</p>
</blockquote>
<h3 id="c2f"><a class="markdownIt-Anchor" href="#c2f"></a> C2F</h3>
<p>定义:  Concatenate to fusion（可以理解为“融合后连接”）</p>
<blockquote>
<p>作用:   <strong>特征融合</strong>和__<strong>模型压缩</strong></p>
</blockquote>
<p>组成:</p>
<ol>
<li>Conv</li>
<li>n个Bottleneck</li>
<li>concat</li>
<li>Conv</li>
</ol>
<p>优点:</p>
<p>…</p>
<p>区别:</p>
<blockquote>
<p>V5: bottleneck数据进行concat的方式不同(线性向前与分级向前)</p>
</blockquote>
<p>细节:</p>
<ol>
<li>
<p>为什么开始和结束要使用1x1卷积</p>
<blockquote>
<p>1 开始: 升维</p>
<p>2  结束: 降维</p>
</blockquote>
</li>
<li>
<p>bottlenet为什么使用padding0和1的两个卷积</p>
<blockquote>
<p>1: **<code>1×1 conv</code> 没有 padding（padding=0）**只在通道维度做变换，不会改变空间大小</p>
<p>2:<code>3×3 conv</code>，空间特征提取，用 <code>padding=1</code> 是为了保持尺寸</p>
</blockquote>
</li>
</ol>
<h3 id="sppf"><a class="markdownIt-Anchor" href="#sppf"></a> SPPF</h3>
<p>定义: 快速金字塔</p>
<blockquote>
<p>作用: <strong>特征增强模块</strong> +<strong>多尺度融合</strong></p>
</blockquote>
<p>组成:</p>
<ol>
<li>将feature map 分为多个不同size的若干个path</li>
<li>进行max pooling</li>
<li>concat</li>
</ol>
<p>优点:</p>
<p><strong>不同大小的感受野</strong></p>
<p>PPF 用连续多次 <code>kernel=5</code> 的池化，避免了并行不同 kernel，转为串行相同 kernel</p>
<p>区别:</p>
<blockquote>
<p>V5 使用了不同K的池化</p>
</blockquote>
<ol>
<li>
<p>为什么要在backbone的最后才添加?</p>
<blockquote>
<p>1: 最后是高级语义 汇聚全局信息maxpooling</p>
<p>2: 最后size很小进行pooling 不会损失信息</p>
</blockquote>
</li>
</ol>
<h3 id="detect"><a class="markdownIt-Anchor" href="#detect"></a> Detect</h3>
<img src="/2025/05/04/2025%E5%B9%B45%E6%9C%884%E6%97%A5-yolo%E6%B1%87%E6%80%BB/image-20250507234632382.png" alt="image-20250507234632382" style="zoom:33%;">
<p><strong>组成:</strong></p>
<ol>
<li>边界框预测</li>
<li>类别预测:</li>
</ol>
<h4 id="loss-2"><a class="markdownIt-Anchor" href="#loss-2"></a> loss</h4>
<p><strong>区别</strong>: 无ancher</p>
<p>YOLOv8 head 输出 <code>[B, H, W, 4+cls+obj]</code></p>
<h3 id="问题"><a class="markdownIt-Anchor" href="#问题"></a> 问题:</h3>
<p>anchor-free ：如果多个目标落在 <strong>同一个 cell（像素）中心</strong>：</p>
<ol>
<li>
<p><strong>center prior</strong>（中心先验）:</p>
<blockquote>
<p>只有 <strong>中心点附近的 cell（通常限定一个 radius 区域）</strong> 才有资格被分配为“正样本”</p>
</blockquote>
</li>
<li>
<p>YOLOv8 的 <strong>dynamic-k matching：</strong></p>
<blockquote>
<p>计算每个候选正样本（中心附近 cell）的 IoU<br>
按 IoU 排序<br>
通过 <strong>ΣIoU 确定动态的 k（正样本数）</strong><br>
选择 top-k 作为正样本</p>
</blockquote>
</li>
<li>
<p>👉 如果两个目标中心点落在 <strong>同一个 cell</strong>：</p>
<blockquote>
<p>按照 <strong>center prior → 同一个 cell 确实是两个目标都“想分配”的 cell</strong></p>
<p>但！<strong>dynamic-k matching 会用 IoU 排序决定“更合适”分配给谁</strong><br>
→ 更高 IoU 的目标会“赢” → 另一个目标需要“找附近 cell”作为正样本</p>
</blockquote>
</li>
</ol>
<h2 id="yolo-v11"><a class="markdownIt-Anchor" href="#yolo-v11"></a> YOLO V11</h2>
<blockquote>
<p>特点: 改进</p>
</blockquote>
<img src="/2025/05/04/2025%E5%B9%B45%E6%9C%884%E6%97%A5-yolo%E6%B1%87%E6%80%BB/image-20250507230522250.png" alt="image-20250507230522250">
<p><img src="/2025/05/04/2025%E5%B9%B45%E6%9C%884%E6%97%A5-yolo%E6%B1%87%E6%80%BB/yolo_v11.png" alt="yolo_v11"></p>
<h3 id="21-c3k2"><a class="markdownIt-Anchor" href="#21-c3k2"></a> 2.1 C3K2</h3>
<p><strong>定义</strong>:  将bottlenetck改为C3K</p>
<blockquote>
<p>作用:   <strong>特征融合</strong>和__<strong>模型压缩</strong></p>
</blockquote>
<p><strong>组成:</strong></p>
<ol>
<li>CBS</li>
<li>n个Bottleneck = <strong>C3K</strong></li>
<li>concat</li>
<li>CBS</li>
</ol>
<h4 id="c3k"><a class="markdownIt-Anchor" href="#c3k"></a> C3K:</h4>
<ol>
<li>CBS</li>
<li>1 bottleneck = CBSx2 + res</li>
<li>concat: 12+ CBS</li>
<li>CBS</li>
</ol>
<p><strong>优点:</strong></p>
<p>1️⃣ 增加网络的深度和非线性表达能力<br>
→ 更深的路径可以学到更复杂的特征</p>
<p>2️⃣ 更高的参数复用效率<br>
→ 通过残差和重复结构，避免纯深度带来的退化问题</p>
<p>3️⃣ 提高局部感受野<br>
→ 套娃 bottleneck 可以让局部更“通透”，减少信息屏蔽</p>
<p><strong>区别:</strong></p>
<blockquote>
<p>V8: 相当于套娃加了深度,多了一个bottleneck</p>
</blockquote>
<p><strong>细节:</strong></p>
<h3 id="22-c2psa"><a class="markdownIt-Anchor" href="#22-c2psa"></a> 2.2 C2PSA</h3>
<img src="/2025/05/04/2025%E5%B9%B45%E6%9C%884%E6%97%A5-yolo%E6%B1%87%E6%80%BB/image-20250507232147155.png" alt="image-20250507232147155" style="zoom: 25%;">
<p><strong>定义:</strong></p>
<blockquote>
<p>作用:**补充全局感知能力 **优化梯度传播和网络训练效果</p>
</blockquote>
<p><strong>组成:</strong></p>
<ol>
<li>类似C2f模块</li>
<li>bottleneck=PSAbBlock</li>
</ol>
<p><strong>PSA:</strong></p>
<h3 id="23-head"><a class="markdownIt-Anchor" href="#23-head"></a> 2.3 head</h3>
<p><strong>区别</strong>:深度可分离的方法</p>
<h2 id="yolov12"><a class="markdownIt-Anchor" href="#yolov12"></a> YOLOv12</h2>
<blockquote>
<p>将注意力机制直接融入目标检测框架的核心设计</p>
</blockquote>
<p><img src="/2025/05/04/2025%E5%B9%B45%E6%9C%884%E6%97%A5-yolo%E6%B1%87%E6%80%BB/v2-f1284a6e6c67a40fd67266ce81c06a39_r.jpg" alt="img"></p>
<h3 id="1-关键技术改进"><a class="markdownIt-Anchor" href="#1-关键技术改进"></a> 1 关键技术改进</h3>
<blockquote>
<h4 id="11-区域注意力area-attention"><a class="markdownIt-Anchor" href="#11-区域注意力area-attention"></a> <strong>1.1 区域注意力（Area Attention）</strong></h4>
<ul>
<li><strong>原理</strong>：将特征图划分为相等区域（默认 4 部分）以降低计算复杂度，同时保持较大的感受野。</li>
<li><strong>优势</strong>：相比传统自注意力机制的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 复杂度，区域注意力将计算成本降低 <strong>50%</strong>。</li>
</ul>
<hr>
<h4 id="12-残差高效层聚合网络r-elan"><a class="markdownIt-Anchor" href="#12-残差高效层聚合网络r-elan"></a> 1.2 残差高效层聚合网络（R-ELAN）</h4>
<ul>
<li><strong>改进点</strong>：在原始 ELAN 架构基础上，引入 <strong>块级残差连接</strong> 与 <strong>缩放技术</strong>（缩放因子 0.01）。</li>
<li><strong>解决问题</strong>：提升大规模模型训练的稳定性。</li>
<li><strong>效果</strong>：
<ul>
<li>参数减少 <strong>18%</strong></li>
<li>FLOPs 降低 <strong>24%</strong></li>
</ul>
</li>
</ul>
<hr>
<h4 id="13-flashattention-集成"><a class="markdownIt-Anchor" href="#13-flashattention-集成"></a> 1.3 FlashAttention 集成</h4>
<ul>
<li><strong>优化</strong>：改进内存访问模式，显著加速注意力计算。</li>
<li><strong>硬件适配</strong>：针对现代 GPU 架构（Turing / Ampere / Ada Lovelace / Hopper）优化。</li>
<li><strong>实测结果</strong>（RTX 3080）：加速 <strong>0.3-0.4 ms</strong>。</li>
</ul>
</blockquote>
<h3 id="2-具体细节"><a class="markdownIt-Anchor" href="#2-具体细节"></a> 2 具体细节</h3>
<h4 id="21-区域注意力area-attention机制"><a class="markdownIt-Anchor" href="#21-区域注意力area-attention机制"></a> 2.1 区域注意力（Area Attention）机制</h4>
<p>YOLOv12 的核心创新之一，主要解决传统自注意力的两大瓶颈：</p>
<p><strong>1. 计算复杂度优化</strong></p>
<ul>
<li><strong>传统自注意力</strong>：对 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo>×</mo><mi>W</mi></mrow><annotation encoding="application/x-tex">H \times W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span> 特征图的计算复杂度为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>H</mi><mn>2</mn></msup><msup><mi>W</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(H^2 W^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></li>
<li><strong>区域注意力</strong>：将特征图按水平方向或垂直方向划分为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi></mrow><annotation encoding="application/x-tex">l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span></span></span></span> 个相等部分（默认 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">l=4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">4</span></span></span></span>），复杂度降至：
<ul>
<li>水平划分：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo fence="true">(</mo><mfrac><msup><mi>H</mi><mn>2</mn></msup><msup><mi>l</mi><mn>2</mn></msup></mfrac><msup><mi>W</mi><mn>2</mn></msup><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">O\left(\frac{H^2}{l^2} W^2\right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.80002em;vertical-align:-0.65002em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size2">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01792em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7463142857142857em;"><span style="top:-2.786em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913142857142857em;"><span style="top:-2.931em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size2">)</span></span></span></span></span></span></li>
<li>垂直划分：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo fence="true">(</mo><msup><mi>H</mi><mn>2</mn></msup><mfrac><msup><mi>W</mi><mn>2</mn></msup><msup><mi>l</mi><mn>2</mn></msup></mfrac><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">O\left(H^2 \frac{W^2}{l^2}\right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.80002em;vertical-align:-0.65002em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size2">(</span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01792em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7463142857142857em;"><span style="top:-2.786em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913142857142857em;"><span style="top:-2.931em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size2">)</span></span></span></span></span></span></li>
</ul>
</li>
<li><strong>优势</strong>：无需复杂窗口划分，降低计算成本同时保持较大感受野。</li>
</ul>
<p><strong>2. 内存访问效率提升</strong></p>
<ul>
<li>结合 <strong>FlashAttention</strong> 技术，通过 I/O 优化减少内存访问次数，充分利用 GPU 内存层次结构。</li>
<li>测试表明，<strong>纯卷积实现</strong> 比线性替代方案速度更快。</li>
</ul>
<p><strong>实现方式</strong></p>
<ol>
<li>将特征图按指定方向划分为多个区域</li>
<li>在各区域内分别计算注意力</li>
<li>合并结果得到全局特征</li>
</ol>
<ul>
<li>在固定分辨率（如 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>640</mn></mrow><annotation encoding="application/x-tex">n=640</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">6</span><span class="mord">4</span><span class="mord">0</span></span></span></span>）下可实现实时检测</li>
<li>与 <strong>轴向注意力</strong> 对比：
<ul>
<li>感受野扩大 <strong>4 倍</strong></li>
<li>速度下降（实测 YOLOv12 ≈ 180 FPS vs YOLOv11 ≈ 250 FPS）</li>
</ul>
</li>
</ul>
<hr>
<h4 id="22-残差高效层聚合网络r-elan"><a class="markdownIt-Anchor" href="#22-残差高效层聚合网络r-elan"></a> 2.2 残差高效层聚合网络（R-ELAN）</h4>
<p>R-ELAN 是对原始 ELAN 架构的重要改进，主要针对以下两个问题：</p>
<p><strong>1. 训练不稳定性</strong></p>
<ul>
<li>在较大模型（YOLOv12-L / YOLOv12-X）中，原始 ELAN 容易出现 <strong>梯度阻塞</strong> 与 <strong>优化困难</strong>。</li>
<li><strong>改进方法</strong>：
<ul>
<li>添加 <strong>输入到输出的残差连接（跳跃连接）</strong></li>
<li>使用 <strong>层缩放技术</strong>（缩放因子 0.01）</li>
</ul>
</li>
<li><strong>效果</strong>：显著提升大规模模型训练稳定性。</li>
</ul>
<p><strong>2. 特征聚合效率</strong></p>
<ul>
<li><strong>原始 ELAN</strong>：输入 → 过渡层 → 拆分多路并行处理 → 拼接</li>
<li><strong>R-ELAN</strong>：输入 → 过渡层调整通道 → 单一特征图 → 并行处理 → 拼接成瓶颈结构</li>
<li><strong>优势</strong>：
<ul>
<li>降低计算成本</li>
<li>提升特征聚合效率</li>
</ul>
</li>
</ul>
<h4 id="23-其他架构优化"><a class="markdownIt-Anchor" href="#23-其他架构优化"></a> 2.3 其他架构优化</h4>
<p>YOLOv12 在核心模块之外，还进行了多项精细化的结构改进：</p>
<p><strong>1. MLP 比例调整</strong></p>
<ul>
<li>将典型 Transformer 中 MLP 扩展比例 <strong>从 4 降至 1.2~2</strong></li>
<li>目的：平衡 <strong>注意力层</strong> 与 <strong>前馈层</strong> 的计算量，减少冗余计算。</li>
</ul>
<p><strong>2. 位置编码替代</strong></p>
<ul>
<li>去除显式位置编码</li>
<li>引入 <strong>7×7 深度可分离卷积</strong>（“位置感知器”）来隐式建模位置信息</li>
<li><strong>优势</strong>：架构更简洁、速度更快。</li>
</ul>
<p><strong>3. 块数量减少</strong></p>
<ul>
<li>减少堆叠块的数量</li>
<li>简化优化过程、降低推理时间。</li>
</ul>
<p><strong>4. 卷积算子集成</strong></p>
<ul>
<li>在适当位置保留高效卷积操作</li>
<li>降低整体参数数量与计算成本。</li>
</ul>
<hr>
<h2 id="yolov13"><a class="markdownIt-Anchor" href="#yolov13"></a> YOLOv13</h2>
<blockquote>
<p>属于社区开发的衍生版本（非 Ultralytics 官方版本）</p>
</blockquote>
<p><img src="/2025/05/04/2025%E5%B9%B45%E6%9C%884%E6%97%A5-yolo%E6%B1%87%E6%80%BB/framework-1754701975877-3.png" alt="framework"></p>
<h3 id="1-核心技术创新"><a class="markdownIt-Anchor" href="#1-核心技术创新"></a> 1 核心技术创新</h3>
<blockquote>
<h4 id="11-hyperace基于超图的自适应关联增强"><a class="markdownIt-Anchor" href="#11-hyperace基于超图的自适应关联增强"></a> 1.1 HyperACE：基于超图的自适应关联增强</h4>
<ul>
<li>
<p><strong>技术原理</strong>：<br>
将多尺度特征图中的像素视为超图顶点，通过可学习的超边构建模块自适应探索顶点间的高阶关联，<br>
并利用线性复杂度的消息传递模块聚合特征，提升复杂场景下的视觉感知能力。</p>
</li>
<li>
<p><strong>作用</strong>：<br>
强化不同尺度特征间的语义关联，尤其对小目标和密集目标检测效果显著。</p>
</li>
</ul>
<hr>
<h4 id="12-fullpad全流程聚合-分布范式"><a class="markdownIt-Anchor" href="#12-fullpad全流程聚合-分布范式"></a> 1.2 FullPAD：全流程聚合 - 分布范式</h4>
<ul>
<li>
<p><strong>技术原理</strong>：<br>
通过 HyperACE 聚合骨干网络的多尺度特征，再通过三条独立“隧道”将增强后的特征分别传递到：</p>
<ul>
<li>骨干与颈部连接处</li>
<li>颈部内部</li>
<li>颈部与头部连接处<br>
实现全流程细粒度信息流协同。</li>
</ul>
</li>
<li>
<p><strong>作用</strong>：<br>
改善梯度传播效率，提升模型整体检测性能。</p>
</li>
</ul>
<hr>
<h4 id="13-轻量级卷积替换"><a class="markdownIt-Anchor" href="#13-轻量级卷积替换"></a> 1.3 轻量级卷积替换</h4>
<ul>
<li><strong>技术原理</strong>：<br>
使用深度可分离卷积（DSConv、DS-Bottleneck 等）替代大核卷积，<br>
在保持感受野的同时大幅减少参数和计算量。</li>
</ul>
</blockquote>
<h3 id="2-hyperace"><a class="markdownIt-Anchor" href="#2-hyperace"></a> 2 HyperACE</h3>
<h2 id="yolo-world"><a class="markdownIt-Anchor" href="#yolo-world"></a> YOLO-World</h2>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://github.com/AILab-CVC/YOLO-World">https://github.com/AILab-CVC/YOLO-World</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/690233858">https://zhuanlan.zhihu.com/p/690233858</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_47151388/article/details/137424184">https://blog.csdn.net/weixin_47151388/article/details/137424184</a></p>
</blockquote>
<p><img src="/2025/05/04/2025%E5%B9%B45%E6%9C%884%E6%97%A5-yolo%E6%B1%87%E6%80%BB/image-20250814145602937.png" alt="image-20250814145602937"></p>
<h3 id="1-关键技术-开放词汇目标检测ovd"><a class="markdownIt-Anchor" href="#1-关键技术-开放词汇目标检测ovd"></a> 1 关键技术 ------开放词汇目标检测（OVD）</h3>
<blockquote>
<p>旨在识别超出预先建立类别范围之外的对象。</p>
</blockquote>
<p>能够解释提示的上下文，以进行准确的检测，而<strong>无需进行特定的类别训练</strong>。它利用大量的图像-文本对和基础图像进行训练，以理解和响应各种提示，例如“穿着白色衬衫的人”。提升了YOLO系列检测器在<strong>零样本场景中的泛化能力。</strong></p>
<p><img src="/2025/05/04/2025%E5%B9%B45%E6%9C%884%E6%97%A5-yolo%E6%B1%87%E6%80%BB/image-20250814145222371.png" alt="image-20250814145222371"></p>
<p>引入“提示-然后检测”的方法论，YOLO-World避开了即时文本编码的需要，而是利用用户提示生成的离线词汇来进行检测。</p>
<h3 id="2-具体细节-2"><a class="markdownIt-Anchor" href="#2-具体细节-2"></a> 2 具体细节</h3>
<h4 id="21-可重新参数化的视觉-语言路径聚合网络repvl-pan"><a class="markdownIt-Anchor" href="#21-可重新参数化的视觉-语言路径聚合网络repvl-pan"></a> 2.1 可重新参数化的视觉-语言路径聚合网络（RepVL-PAN）</h4>
<img src="/2025/05/04/2025%E5%B9%B45%E6%9C%884%E6%97%A5-yolo%E6%B1%87%E6%80%BB/image-20250814150028346.png" alt="image-20250814150028346" style="zoom:67%;">
<p>通过多尺度图像特征{C3, C4, C5}建立特征金字塔{P3, P4, P5}。此外，我们提出了文本引导CSPLayer (T-CSPLayer)和图像池注意(I-Pooling Attention)，进一步增强图像特征和文本特征之间的交互，从而提高开放词汇的视觉语义表示能力。在推理过程中，离线词汇嵌入可以重新参数化为卷积层或线性层的权重，以便部署。</p>
<h4 id="22-区域-文本对比损失"><a class="markdownIt-Anchor" href="#22-区域-文本对比损失"></a> 2.2 区域-文本对比损失</h4>
<h2 id="yoloe"><a class="markdownIt-Anchor" href="#yoloe"></a> YOLOE</h2>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/29944215233">https://zhuanlan.zhihu.com/p/29944215233</a></p>
</blockquote>
<p><img src="/2025/05/04/2025%E5%B9%B45%E6%9C%884%E6%97%A5-yolo%E6%B1%87%E6%80%BB/image-20250814143715287.png" alt="image-20250814143715287"></p>
<h3 id="1-关键技术改进-2"><a class="markdownIt-Anchor" href="#1-关键技术改进-2"></a> 1 关键技术改进</h3>
<p>传统模型YOLO系列虽以高效著称，却受限于预定义类别. YOLOE支持文本、视觉、无提示全场景的实时视觉全能模型。</p>
<ul>
<li>YOLOE的架构基于经典的YOLO系列模型，主要由以下几个部分组成：
<ul>
<li><strong>骨干网络（Backbone）</strong>：负责提取图像的多尺度特征。</li>
<li><strong>PAN（Path Aggregation Network）</strong>：用于进一步融合多尺度特征，增强特征表达能力。</li>
<li><strong>回归头（Regression Head）</strong>：预测目标边界框的位置和大小。</li>
<li><strong>分割头（Segmentation Head）</strong>：生成目标的分割掩码。</li>
<li><strong>对象嵌入头（Object Embedding Head）</strong>：生成每个锚点的对象嵌入，用于与提示嵌入进行对比。</li>
</ul>
</li>
</ul>
<p>YOLOE的核心创新在于其能<strong>够处理多种提示机制，包括文本提示、视觉提示和无提示场景。对于文本提示.</strong></p>
<p>YOLOE通过**可重参数化区域-文本对齐（RepRTA）<strong>策略来增强文本嵌入与视觉特征的对齐。对于视觉提示，YOLOE设计了</strong>语义激活视觉提示编码器（SAVPE）<strong>通过解耦的语义和激活分支生成高效的视觉提示嵌入。对于无提示场景，YOLOE引入了</strong>懒惰区域-提示对比（LRPC）**策略，通过内置的大词汇表检索类别名称，避免了依赖昂贵的语言模型。</p>
<h3 id="2-具体细节-3"><a class="markdownIt-Anchor" href="#2-具体细节-3"></a> 2 具体细节</h3>
<h4 id="21-可重参数化区域-文本对齐re-parameterizable-region-text-alignment-reprta"><a class="markdownIt-Anchor" href="#21-可重参数化区域-文本对齐re-parameterizable-region-text-alignment-reprta"></a> 2.1 可重参数化区域-文本对齐（<strong>Re-parameterizable region-text alignment:</strong> RepRTA）</h4>
<img src="/2025/05/04/2025%E5%B9%B45%E6%9C%884%E6%97%A5-yolo%E6%B1%87%E6%80%BB/image-20250814143815369.png" alt="image-20250814143815369" style="zoom: 67%;"> 
<p>在开放集场景中，文本提示与视觉特征的对齐至关重要。传统的跨模态融合方法虽然能够提升对齐效果，但计算开销较大，尤其是在处理大量文本时。为此，YOLOE提出了RepRTA策略，通过一个轻量级的辅助网络来改进预训练的文本嵌入。</p>
<p>具体来说，RepRTA首先使用CLIP文本编码器生成预训练的文本嵌入。然后，通过一个轻量级的辅助网络（仅包含一个前馈块）对这些嵌入进行增强。训练时，辅助网络仅需处理文本提示，计算开销较低。推理时，辅助网络可以无缝重参数化为分类头，使得模型在推理时与标准的YOLO模型完全一致，实现了零开销。</p>
<h4 id="22-语义激活视觉提示编码器semantic-activated-visual-prompt-encoder-savpe"><a class="markdownIt-Anchor" href="#22-语义激活视觉提示编码器semantic-activated-visual-prompt-encoder-savpe"></a> 2.2 语义激活视觉提示编码器（<strong>Semantic-activated visual prompt encoder:</strong> SAVPE）</h4>
<p>视觉提示通过视觉线索（如边界框或掩码）指示感兴趣的对象类别。为了高效处理视觉提示，YOLOE设计了SAVPE，它包含两个解耦的分支：</p>
<ul>
<li><strong>语义分支</strong>：输出与提示无关的语义特征，保持低计算开销。</li>
<li><strong>激活分支</strong>：通过与图像特征的交互生成分组提示感知权重，用于加权语义特征。</li>
</ul>
<p>具体来说，SAVPE首先将视觉提示形式化为掩码，并通过卷积操作生成提示特征。然后，语义分支从PAN的多尺度特征中提取语义特征，激活分支通过卷积操作生成提示感知权重。最后，通过加权语义特征生成视觉提示嵌入，用于与对象嵌入进行对比。</p>
<h4 id="23-懒惰区域-提示对比lazy-region-prompt-contrast-lrpc"><a class="markdownIt-Anchor" href="#23-懒惰区域-提示对比lazy-region-prompt-contrast-lrpc"></a> 2.3 懒惰区域-提示对比（<strong>Lazy region-prompt contrast:</strong> LRPC）</h4>
<p>在无提示场景中，模型需要识别图像中的所有对象并生成类别名称。传统的生成式方法依赖于大型语言模型，计算开销较大。为此，YOLOE提出了LRPC策略，将无提示场景重新定义为检索问题。</p>
<p>具体来说，LRPC首先通过一个专用的提示嵌入找到所有对象的锚点。然后，仅将这些锚点与内置的大词汇表进行匹配，检索类别名称。通过这种方式，LRPC避免了处理大量无关锚点的计算开销，显著提升了效率。</p>
<h1 id="2结构分析"><a class="markdownIt-Anchor" href="#2结构分析"></a> 2.结构分析</h1>
<h2 id="3-检测头"><a class="markdownIt-Anchor" href="#3-检测头"></a> 3 检测头</h2>
<p>解耦头</p>
<blockquote>
<p>即分开计算BboxLoss和ClsLoss</p>
</blockquote>
<p><strong>reg_max</strong></p>
<p>定义:</p>
<blockquote>
<p>作用</p>
</blockquote>
<h1 id="结果分析"><a class="markdownIt-Anchor" href="#结果分析"></a> 结果分析</h1>
<h2 id="0-labels"><a class="markdownIt-Anchor" href="#0-labels"></a> 0 labels</h2>
<p><strong>labels</strong></p>
<p>定义: 混淆矩阵。矩阵的每一列代表一个类的实例预测，而每一行表示一个实际的类的实例。</p>
<blockquote>
<p>作用: 是否将两个不同的类混淆了</p>
<p>分析:</p>
<ol>
<li>第一个图是训练集得数据量，每个类别有多少个</li>
<li>第二个是框的尺寸和数量</li>
<li>第三个框的center点的位置。</li>
<li>第四个是labeld的高宽相比于整个图片</li>
</ol>
</blockquote>
<img src="/2025/05/04/2025%E5%B9%B45%E6%9C%884%E6%97%A5-yolo%E6%B1%87%E6%80%BB/labels.jpg" alt="labels" style="zoom:25%;">   
<p><strong>labels_correlogram</strong></p>
<p>定义: <strong>类别相关性热力图（correlation heatmap）</strong>。</p>
<blockquote>
<p>作用: 是否将两个不同的类混淆了</p>
<p>分析: 颜色越深，表示对应标签之间的相关性越强；颜色越浅，表示相关性越弱</p>
<ol>
<li>哪些<strong>标签之间具有较强的相关性</strong></li>
<li>如果我们发现某些标签之间的相关性过强，可以考虑将它们合并成一个标签，从而简化模型并提高效率。</li>
</ol>
</blockquote>
<img src="/2025/05/04/2025%E5%B9%B45%E6%9C%884%E6%97%A5-yolo%E6%B1%87%E6%80%BB/labels_correlogram.jpg" alt="labels_correlogram" style="zoom:25%;">  
<h2 id="1-confusion_matrix"><a class="markdownIt-Anchor" href="#1-confusion_matrix"></a> 1 confusion_matrix</h2>
<p>定义: 混淆矩阵。矩阵的每一列代表一个类的实例预测，而每一行表示一个实际的类的实例。</p>
<blockquote>
<p>作用: 是否将两个不同的类混淆了</p>
<p>分析:</p>
<ol>
<li>left4 容易认为是background</li>
</ol>
</blockquote>
<img src="/2025/05/04/2025%E5%B9%B45%E6%9C%884%E6%97%A5-yolo%E6%B1%87%E6%80%BB/confusion_matrix.png" alt="confusion_matrix" style="zoom: 15%;"> 
<h2 id="2-prprf1_curve"><a class="markdownIt-Anchor" href="#2-prprf1_curve"></a> 2  P&amp;R&amp;PR&amp;F1_curve</h2>
<p><strong>2.1 P_curve</strong></p>
<p>定义: 当我设置置信度为某一数值的时候，各个类别识别的准确率。</p>
<blockquote>
<p>作用:</p>
</blockquote>
<img src="/2025/05/04/2025%E5%B9%B45%E6%9C%884%E6%97%A5-yolo%E6%B1%87%E6%80%BB/P_curve.png" alt="P_curve" style="zoom:13%;"> 
<p><strong>2.2  R_curve</strong></p>
<p>定义: 当我设置置信度为某一数值的时候，各个类别识别的召回率（查全率）和置信度的关系图。</p>
<blockquote>
<p>作用:  越全面</p>
</blockquote>
<p><img src="/2025/05/04/2025%E5%B9%B45%E6%9C%884%E6%97%A5-yolo%E6%B1%87%E6%80%BB/_%E6%A1%8C%E9%9D%A2/train2/PR_curve.png" alt="PR_curve"></p>
<p><strong>3. PR_curve</strong></p>
<p>定义: 精度与召回率曲线</p>
<blockquote>
<p>作用:  精度与召回率的关系</p>
<p>分析: 希望mAP曲线的面积尽可能接近1</p>
</blockquote>
<img src="/2025/05/04/2025%E5%B9%B45%E6%9C%884%E6%97%A5-yolo%E6%B1%87%E6%80%BB/PR_curve.png" alt="PR_curve" style="zoom: 15%;"> 
<p><strong>4 F1_curve</strong></p>
<p>定义: <img src="/2025/05/04/2025%E5%B9%B45%E6%9C%884%E6%97%A5-yolo%E6%B1%87%E6%80%BB/image-20250508182738579.png" alt="image-20250508182738579" style="zoom:33%;"></p>
<blockquote>
<p>作用: <strong>精确率和召回率的调和平均数</strong></p>
<p>分析:</p>
</blockquote>
<img src="/2025/05/04/2025%E5%B9%B45%E6%9C%884%E6%97%A5-yolo%E6%B1%87%E6%80%BB/F1_curve.png" alt="F1_curve" style="zoom:15%;"> 
<h2 id="3-other"><a class="markdownIt-Anchor" href="#3-other"></a> 3 other</h2>
<p><strong>3.1 loss functions</strong></p>
<blockquote>
<p>定义: 各类信息</p>
</blockquote>
<p><strong>3.2 result.csv</strong></p>
<blockquote>
<p>定义: 训练的输出 results.txt中最后三列是验证集结果，前面的是训练集结果</p>
</blockquote>
<p><strong>3.3 train_batchx|val_batchx_labels|val_batchx_pred</strong></p>
<blockquote>
<p>batchsize’分析</p>
</blockquote>
<p>ok</p>
<h1 id="消融实验"><a class="markdownIt-Anchor" href="#消融实验"></a> 消融实验</h1>
<table>
<thead>
<tr>
<th>Model</th>
<th>Layers</th>
<th>Params(M)</th>
<th>GFLOPs</th>
<th>mAP@0.5</th>
<th>mAP@0.5:0.95</th>
<th>Precision</th>
<th>Recall</th>
<th>Val Box</th>
<th>Val Cls</th>
<th>Val DFL</th>
</tr>
</thead>
<tbody>
<tr>
<td>wire（YOLOv8s）</td>
<td>129</td>
<td>11.142</td>
<td>28.7</td>
<td><strong>0.97932</strong></td>
<td><strong>0.61878</strong></td>
<td>0.96333</td>
<td>0.95744</td>
<td>1.34446</td>
<td>0.68130</td>
<td>0.98080</td>
</tr>
<tr>
<td>wire-CBAM2</td>
<td>149</td>
<td>4.461</td>
<td>19.8</td>
<td>0.98499</td>
<td>0.65787</td>
<td>0.96523</td>
<td>0.96764</td>
<td>1.23725</td>
<td>0.60513</td>
<td>0.96018</td>
</tr>
<tr>
<td>wire-ghost</td>
<td>137</td>
<td>2.824</td>
<td>7.8</td>
<td>0.97932</td>
<td>0.61878</td>
<td>0.96333</td>
<td>0.95744</td>
<td>1.34446</td>
<td>0.68130</td>
<td>0.98080</td>
</tr>
<tr>
<td>wire-Ghost-C3ghost-GSCSP</td>
<td>291</td>
<td>5.857</td>
<td>17.4</td>
<td>0.97649</td>
<td>0.61437</td>
<td>0.95304</td>
<td>0.94231</td>
<td>1.34232</td>
<td>0.67661</td>
<td>0.97089</td>
</tr>
<tr>
<td>wire-Ghost-x-x-CBAM</td>
<td>162</td>
<td>3.399</td>
<td>15.5</td>
<td>0.97125</td>
<td>0.63970</td>
<td>0.94271</td>
<td>0.95166</td>
<td>1.28386</td>
<td>0.64999</td>
<td>0.96416</td>
</tr>
<tr>
<td>wire-Ghost-C3ghost-GSCSP-CBAM3</td>
<td>311</td>
<td>7.222</td>
<td>28.8</td>
<td>0.97058</td>
<td>0.61882</td>
<td>0.94713</td>
<td>0.95569</td>
<td>1.34975</td>
<td>0.69796</td>
<td>0.96739</td>
</tr>
<tr>
<td>wire-Ghost-C3ghost</td>
<td>259</td>
<td>3.136</td>
<td>8.9</td>
<td>0.96228</td>
<td>0.58690</td>
<td>0.87856</td>
<td>0.93795</td>
<td>1.43447</td>
<td>0.77993</td>
<td>0.96887</td>
</tr>
<tr>
<td>wire-C2Ghost-o</td>
<td>251</td>
<td>0.999</td>
<td>3.2</td>
<td>0.77162</td>
<td>0.34710</td>
<td>0.63183</td>
<td>0.79332</td>
<td>1.94767</td>
<td>1.21099</td>
<td>1.09901</td>
</tr>
</tbody>
</table>
<p>总结</p>
<table>
<thead>
<tr>
<th>模型</th>
<th>Params(M)</th>
<th>↓参数压缩</th>
<th>GFLOPs</th>
<th>↓GFLOPs</th>
<th>mAP@0.5</th>
<th>ΔmAP@0.5</th>
<th>mAP@0.5:0.95</th>
<th>ΔmAP@0.5:95</th>
<th>结论</th>
</tr>
</thead>
<tbody>
<tr>
<td>wire</td>
<td>11.142</td>
<td>-</td>
<td>28.7</td>
<td>-</td>
<td>0.97932</td>
<td>0</td>
<td>0.61878</td>
<td>0</td>
<td>基准</td>
</tr>
<tr>
<td>wire-ghost</td>
<td>2.824</td>
<td><strong>74.6%↓</strong></td>
<td>7.8</td>
<td><strong>72.8%↓</strong></td>
<td>0.97932</td>
<td>+0.00000</td>
<td>0.61878</td>
<td>+0.00000</td>
<td>极优</td>
</tr>
<tr>
<td>wire-CBAM2</td>
<td>4.461</td>
<td>59.9%↓</td>
<td>19.8</td>
<td>31.0%↓</td>
<td><strong>0.98499</strong></td>
<td><strong>+0.00567</strong></td>
<td><strong>0.65787</strong></td>
<td><strong>+0.03909</strong></td>
<td>准确性最优</td>
</tr>
<tr>
<td>wire-Ghost-C3ghost</td>
<td>3.136</td>
<td>71.8%↓</td>
<td>8.9</td>
<td>69.0%↓</td>
<td>0.96228</td>
<td>−0.01704</td>
<td>0.58690</td>
<td>−0.03188</td>
<td>稳健但略降</td>
</tr>
<tr>
<td>wire-Ghost-C3ghost-GSCSP</td>
<td>5.857</td>
<td>47.4%↓</td>
<td>17.4</td>
<td>39.3%↓</td>
<td>0.97649</td>
<td>−0.00283</td>
<td>0.61437</td>
<td>−0.00441</td>
<td>平衡最优</td>
</tr>
<tr>
<td>wire-Ghost-C3ghost-GSCSP-CBAM</td>
<td>7.222</td>
<td>35.2%↓</td>
<td>28.8</td>
<td>≈持平</td>
<td>0.97068</td>
<td>−0.00864</td>
<td>0.63409</td>
<td>+0.01531</td>
<td>精度提升轻度补偿</td>
</tr>
<tr>
<td>wire-Ghost-x-x-CBAM</td>
<td>3.399</td>
<td>69.5%↓</td>
<td>15.5</td>
<td>46.0%↓</td>
<td>0.97125</td>
<td>−0.00807</td>
<td>0.63970</td>
<td>+0.02092</td>
<td>轻量+CBAM 平衡型</td>
</tr>
<tr>
<td>wire-C2Ghost-o</td>
<td>0.999</td>
<td><strong>91.0%↓</strong></td>
<td>3.2</td>
<td><strong>88.9%↓</strong></td>
<td>0.77162</td>
<td>−0.20770</td>
<td>0.34710</td>
<td>−0.27168</td>
<td>极端轻量，损失大</td>
</tr>
</tbody>
</table>
<blockquote>
<p>在保持较低参数量（&lt;5M）和计算复杂度（&lt;20 GFLOPs）前提下，<code>Ghost + C3ghost + GSCSP + CBAM</code> 的组合在 mAP@0.5 上基本保持在 <strong>0.97 以上</strong>，性能与原始结构相当甚至更优。</p>
</blockquote>
<blockquote>
<p>实际推理速度和资源占用显著优于原版 YOLOv8。量化后的模型在 mAP@0.5 上仅有 <strong>0.5%-1% 左右的下降</strong>，但带来了 <strong>20%-40% 的加速提升</strong>，更适合在嵌入式或边缘设备上部署。</p>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://shakewely.github.io/2025/05/04/2025%E5%B9%B45%E6%9C%884%E6%97%A5-yolo%E6%B1%87%E6%80%BB/" data-id="cmanj8o28000ylcv48vhi5odp" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/YOLO/" rel="tag">YOLO</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-2025年4月24日-训练优化" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2025/04/24/2025%E5%B9%B44%E6%9C%8824%E6%97%A5-%E8%AE%AD%E7%BB%83%E4%BC%98%E5%8C%96/" class="article-date">
  <time class="post-time" datetime="2025-04-24T05:39:41.000Z" itemprop="datePublished">
    <span class="post-month">4月</span><br/>
    <span class="post-day">24</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/04/24/2025%E5%B9%B44%E6%9C%8824%E6%97%A5-%E8%AE%AD%E7%BB%83%E4%BC%98%E5%8C%96/">2025年4月24日 训练优化</a>
    </h1>
  

        <div>
          
          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="数据"><a class="markdownIt-Anchor" href="#数据"></a> 数据</h1>
<h2 id="1-标注优化"><a class="markdownIt-Anchor" href="#1-标注优化"></a> 1 标注优化</h2>
<p>✅<strong>1. 常见问题汇总</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">相连的两个数据是一起标还是?</span><br><span class="line">制造进行标定数据可以把之前完成的暂时先设置为未完成状态</span><br><span class="line">有很相似的物体 但是相对位置是明显不同的</span><br><span class="line">AI辅助标注后,要纠正一些错误标注,AI的错误样本和很多相同相似的数据要删除,分析哪几个类别比较容易出错,保留一些难样本,增加这类的样本,和少见比如有遮挡的情景，出现遮挡如何处理?需不需要重新标注呢?</span><br><span class="line">模型效果比我想象的要好很多啊,很多想不到的都能识别正确,关键是别标错数据了</span><br><span class="line">对于变化不大,泛化能力要求不高的场景,将val的比例提高是否有效?</span><br><span class="line">如果发现缺漏的标签需要再添加，可以训练只标定新标签的小model辅助后续标注而不用全部标签</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th>问题</th>
<th>建议</th>
</tr>
</thead>
<tbody>
<tr>
<td>相连物体一起标 or 分开标？</td>
<td>根据任务需求，区分 instance 则分开，不区分则合并标</td>
</tr>
<tr>
<td>标定数据状态管理</td>
<td>已完成数据可切换为未完成，用状态标记做复查</td>
</tr>
<tr>
<td>相似物体不同位置 → class 设计</td>
<td>推荐用不同 class，如有后处理能力也可统一 class 后区分</td>
</tr>
<tr>
<td>AI 标注后纠错与困难样本采集</td>
<td>分析高误差类别、保留难样本、增加少见情景、遮挡需重新标注</td>
</tr>
<tr>
<td>遮挡样本标注</td>
<td>标可见部分并加 occluded 属性，完全遮挡则不标</td>
</tr>
<tr>
<td>val 占比调整</td>
<td>变化小的场景可适当增大 val 比例（<s>20</s>30%），注意别太高</td>
</tr>
</tbody>
</table>
<h1 id="训练相关"><a class="markdownIt-Anchor" href="#训练相关"></a> 训练相关</h1>
<h2 id="1-训练数据指标"><a class="markdownIt-Anchor" href="#1-训练数据指标"></a> 1 训练数据指标</h2>
<p>✅ <strong>1. layers（层数）</strong></p>
<ul>
<li>表示网络中包含的层的数量（卷积层、BN层、激活层、C3ghost 模块等）。</li>
<li><strong>和显存关系：</strong><br>
→ 层数本身不直接决定显存占用，但更深的网络往往需要更多参数和中间特征，这会增加显存需求。</li>
</ul>
<hr>
<p>✅ <strong>2. parameters（参数量）</strong></p>
<ul>
<li>网络中的总参数数量，比如卷积核的权重、偏置等。</li>
<li><strong>和显存关系：</strong><br>
→ 需要显存存储这些参数，通常参数量较小（百万级）时，这部分显存占用较低（几十 MB）。<br>
→ 主要影响模型加载到 GPU 后的静态显存占用。</li>
</ul>
<hr>
<p>✅ <strong>3. gradients（梯度数）</strong></p>
<ul>
<li>训练时需要保存每个参数的梯度，用于反向传播。</li>
<li><strong>和显存关系：</strong><br>
→ 在训练时会额外占用和参数量几乎相同的显存。<br>
→ 如果只推理（inference），梯度不计算，不占用显存。</li>
</ul>
<hr>
<p>✅ <strong>4. GFLOPs（每秒十亿次浮点运算）</strong></p>
<ul>
<li>每次前向推理所需的大约计算量。</li>
<li><strong>和显存关系：</strong><br>
→ 不直接决定显存占用，但通常 GFLOPs 高的模型也会有更大中间特征图（feature map），导致更高的显存占用。</li>
</ul>
<hr>
<h3 id="总结和-gpu-显存相关的部分"><a class="markdownIt-Anchor" href="#总结和-gpu-显存相关的部分"></a> 📌 总结：和 GPU 显存相关的部分</h3>
<table>
<thead>
<tr>
<th>组件</th>
<th>训练时显存占用</th>
<th>推理时显存占用</th>
</tr>
</thead>
<tbody>
<tr>
<td>parameters</td>
<td>是</td>
<td>是</td>
</tr>
<tr>
<td>gradients</td>
<td>是（主要是训练时）</td>
<td>否</td>
</tr>
<tr>
<td>feature maps（中间特征图，未在 summary 中列出，但由 layers 结构和输入大小决定）</td>
<td>是，主要占用</td>
<td>是，主要占用</td>
</tr>
<tr>
<td>GFLOPs</td>
<td>间接相关（高 GFLOPs 模型通常特征图也大）</td>
<td>间接相关</td>
</tr>
</tbody>
</table>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://shakewely.github.io/2025/04/24/2025%E5%B9%B44%E6%9C%8824%E6%97%A5-%E8%AE%AD%E7%BB%83%E4%BC%98%E5%8C%96/" data-id="cmanj8o26000mlcv4fgg5hrf3" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/dl/" rel="tag">dl</a></li></ul>

    </footer>
  </div>
  
</article>




  


  <nav id="page-nav">
    <a class="extend prev" rel="prev" href="/page/3/">&amp;laquo; pre</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><a class="page-number" href="/page/6/">6</a><span class="space">&hellip;</span><a class="page-number" href="/page/12/">12</a><a class="extend next" rel="next" href="/page/5/">next &amp;raquo;</a>
  </nav>
</section>
        
          <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title"></h3>
  <div class="widget">
    <h1 class="blog-title">Weakliy_Blog</h1>
    <h2 class="blog-subtitle"></h2>
    <ul class="blog-link">
     
          <a href="/" title="Home">
            <li>主页</li>
          </a>
        
          <a href="/archives" title="Archives">
            <li>归档</li>
          </a>
        
          <a href="/categories" title="Categories">
            <li>分类</li>
          </a>
        
          <a href="/tags" title="Tags">
            <li>标签</li>
          </a>
        
    </ul>
  </div>
</div>

  
    <div class="widget-wrap">
  <h3 class="widget-title"></h3>
  <div class="widget">
    <img class="avatar" src="https://raw.githubusercontent.com/ShakeWeLy/Weakliy.github.io/main/%E5%A4%B4%E5%83%8F/mmexport1683194148817.png">
    <h2 class="author">Weakliy</h2>
    <h3 class="description"></h3>
    <div class="count-box">
      <a href="/archives"><div><strong>116</strong><br>文章</div></a>
      <a href="/categories"><div><strong>0</strong><br>分类</div></a>
      <a href="/tags"><div><strong>34</strong><br>标签</div></a>
    </div>



    <div class="social-link">
      
        <a class="hvr-bounce-in" href="https://github.com/ShakeWeLy" target="_blank" title="Github">
          Github
        </a>
      
    </div>

    <div class="friend-link">
      <h2>联系我</h2>
      
        <a class="hvr-bounce-in" href="https://github.com/ShakeWeLy" target="_blank" title="ShanaMaid">
          ShanaMaid
        </a>
      
    </div>
  </div>
</div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy;2024 - 2025 Weakliy<br>
      由<a href="http://hexo.io/" target="_blank">Hexo</a>强力驱动 | 
      主题-<a target="_blank" rel="noopener" href="https://github.com/ShanaMaid/hexo-theme-shana">Shana</a>
      
    </div>
    
  </div>
</footer>
    </div>
    

<script src="//apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"></script>
<script src="//apps.bdimg.com/libs/wow/0.1.6/wow.min.js"></script>
<script>
new WOW().init();
</script>   


  
<link rel="stylesheet" href="/plugin/fancybox/jquery.fancybox.css">

  
<script src="/plugin/fancybox/jquery.fancybox.pack.js"></script>




  
<link rel="stylesheet" href="/plugin/galmenu/GalMenu.css">

  
<script src="/plugin/galmenu/GalMenu.js"></script>

  <div class="GalMenu GalDropDown">
      <div class="circle" id="gal">
        <div class="ring">
          
            <a href="/" title="" class="menuItem">首页</a>
          
            <a href="/tags" title="" class="menuItem">标签</a>
          
            <a href="/categories" title="" class="menuItem">分类</a>
          
            <a href="/archives" title="" class="menuItem">总览</a>
          
            <a href="/xxxxxxxxx" title="" class="menuItem">xxx</a>
          
            <a href="/xxxxxxx" title="" class="menuItem">xxxx</a>
          
        </div>
        
          <audio id="audio" src="#"></audio>
        
      </div> 
</div>
<div id="overlay" style="opacity: 1; cursor: pointer;"></div>
  <script type="text/javascript">var items = document.querySelectorAll('.menuItem');
    for (var i = 0,
    l = items.length; i < l; i++) {
      items[i].style.left = (50 - 35 * Math.cos( - 0.5 * Math.PI - 2 * (1 / l) * i * Math.PI)).toFixed(4) + "%";
      items[i].style.top = (50 + 35 * Math.sin( - 0.5 * Math.PI - 2 * (1 / l) * i * Math.PI)).toFixed(4) + "%"
    }</script>
<script type="text/javascript">
  $(document).ready(function() {
    $('body').GalMenu({
      'menu': 'GalDropDown'
    })
  });
</script>

  <section class="hidden-xs"> 
  <ul class="cb-slideshow"> 
    <li><span>苟利</span></li> 
    <li><span>国家</span></li> 
    <li><span>生死以</span></li> 
    <li><span>岂能</span></li> 
    <li><span>祸福</span></li> 
    <li><span>趋避之</span></li> 
  </ul>
</section>

<script src="/js/script.js"></script>




  </div>
</body>
</html>