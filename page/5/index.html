<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Weakliy_Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Weakliy_Blog">
<meta property="og:url" content="https://shakewely.github.io/page/5/index.html">
<meta property="og:site_name" content="Weakliy_Blog">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Weakliy">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Weakliy_Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

  
<link rel="stylesheet" href="/plugin/bganimation/bg.css">

  

  <link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css" rel="stylesheet" type="text/css">
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <div class="outer">
        <div class="widget-wrap mobile-header">
  <h3 class="widget-title"></h3>
  <div class="widget">
    <img class="avatar" src="https://raw.githubusercontent.com/ShakeWeLy/Weakliy.github.io/main/%E5%A4%B4%E5%83%8F/mmexport1683194148817.png">
    <h2 class="author">Weakliy</h2>
    <h3 class="description"></h3>
    <div class="count-box">
      <a href="/archives"><div><strong>92</strong><br>文章</div></a>
      <a href="/categories"><div><strong>0</strong><br>分类</div></a>
      <a href="/tags"><div><strong>29</strong><br>标签</div></a>
    </div>
    <ul class="blog-link">
     
          <a href="/" title="Home">
            <li>主页</li>
          </a>
        
          <a href="/archives" title="Archives">
            <li>归档</li>
          </a>
        
          <a href="/categories" title="Categories">
            <li>分类</li>
          </a>
        
          <a href="/tags" title="Tags">
            <li>标签</li>
          </a>
        
    </ul>
  </div>
</div>

        <section id="main">
  
    <article id="post-2025年2月5日-轻量化神经网络" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2025/02/05/2025%E5%B9%B42%E6%9C%885%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="article-date">
  <time class="post-time" datetime="2025-02-05T08:28:10.000Z" itemprop="datePublished">
    <span class="post-month">2月</span><br/>
    <span class="post-day">05</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/02/05/2025%E5%B9%B42%E6%9C%885%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">2025年2月5日 轻量化神经网络1</a>
    </h1>
  

        <div>
          
          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="简介"><a class="markdownIt-Anchor" href="#简介"></a> 简介</h1>
<p><img src="/2025/02/05/2025%E5%B9%B42%E6%9C%885%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20250205162820154.png" alt="image-20250205162820154"></p>
<h1 id="知识蒸馏"><a class="markdownIt-Anchor" href="#知识蒸馏"></a> 知识蒸馏</h1>
<h2 id="soft-target"><a class="markdownIt-Anchor" href="#soft-target"></a> soft target</h2>
<img src="/2025/02/05/2025%E5%B9%B42%E6%9C%885%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20250205163735260.png" alt="image-20250205163735260" style="zoom:33%;">
<blockquote>
<p>更多信息+</p>
</blockquote>
<h4 id="蒸馏温度"><a class="markdownIt-Anchor" href="#蒸馏温度"></a> 蒸馏温度</h4>
<img src="/2025/02/05/2025%E5%B9%B42%E6%9C%885%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20250205164432943.png" alt="image-20250205164432943" style="zoom:50%;">
<img src="/2025/02/05/2025%E5%B9%B42%E6%9C%885%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20250205164308480.png" alt="image-20250205164308480" style="zoom:67%;">
<h4 id="训练"><a class="markdownIt-Anchor" href="#训练"></a> 训练</h4>
<img src="/2025/02/05/2025%E5%B9%B42%E6%9C%885%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20250205164454289.png" alt="image-20250205164454289" style="zoom:33%;">
<h1 id="量化"><a class="markdownIt-Anchor" href="#量化"></a> 量化</h1>
<h1 id="剪枝"><a class="markdownIt-Anchor" href="#剪枝"></a> 剪枝</h1>
<h2 id="定义"><a class="markdownIt-Anchor" href="#定义"></a> 定义</h2>
<h3 id="剪枝方法总结表"><a class="markdownIt-Anchor" href="#剪枝方法总结表"></a> <strong>剪枝方法总结表</strong></h3>
<table>
<thead>
<tr>
<th>剪枝方法</th>
<th>主要剪枝对象</th>
<th>优点</th>
<th>缺点</th>
<th>示例方法/应用</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>权重剪枝（Weight Pruning）</strong></td>
<td>单个权重</td>
<td>剪枝率高，可应用于各种网络</td>
<td>需要稀疏矩阵优化，不适合硬件加速</td>
<td>TensorFlow <code>tf.sparsity</code>, PyTorch <code>torch.nn.utils.prune</code></td>
</tr>
<tr>
<td><strong>结构化剪枝（Structured Pruning）</strong></td>
<td>神经元、通道、卷积核</td>
<td>适合硬件加速</td>
<td>剪枝率受限，可能影响模型结构</td>
<td><code>Channel Pruning</code>, <code>Filter Pruning</code></td>
</tr>
<tr>
<td><strong>低秩分解剪枝（Low-Rank Approximation）</strong></td>
<td>整体权重矩阵</td>
<td>计算加速明显</td>
<td>需要额外的分解计算</td>
<td>SVD 分解, CP 分解, Tensor Train 分解</td>
</tr>
<tr>
<td><strong>剪枝 + 训练（Prune and Fine-tune）</strong></td>
<td>结合剪枝和微调</td>
<td>可恢复精度</td>
<td>训练时间增加</td>
<td>迭代剪枝（Iterative Pruning）, 一次性剪枝（One-shot Pruning）</td>
</tr>
<tr>
<td><strong>软剪枝（Soft Pruning）</strong></td>
<td>权重</td>
<td>更温和的剪枝方式</td>
<td>需要更多训练步骤</td>
<td>逐步缩小权重（Weight Decay），平滑剪枝（Gradual Magnitude Pruning）</td>
</tr>
<tr>
<td><strong>剪枝 + 蒸馏（Pruning with Distillation）</strong></td>
<td>剪枝后蒸馏</td>
<td>精度损失小</td>
<td>需要额外教师模型</td>
<td><code>Knowledge Distillation (KD)</code>, MobileBERT, TinyBERT</td>
</tr>
<tr>
<td><strong>幸运票剪枝（Lottery Ticket Hypothesis）</strong></td>
<td>子网络</td>
<td>保留重要子结构</td>
<td>训练步骤复杂</td>
<td>训练大模型后剪枝，重新初始化训练</td>
</tr>
<tr>
<td><strong>正则化剪枝（Regularization-based Pruning）</strong></td>
<td>L1/L2 约束</td>
<td>无需额外剪枝步骤</td>
<td>训练需额外超参数</td>
<td><code>L1 Regularization</code>, <code>Group Lasso Pruning</code></td>
</tr>
<tr>
<td><strong>动态剪枝（Movement Pruning）</strong></td>
<td>Transformer 模型</td>
<td>适合 NLP</td>
<td>计算复杂度高</td>
<td><code>BERT Pruning</code>, <code>MobileBERT</code></td>
</tr>
<tr>
<td><strong>自动剪枝（AutoML Pruning）</strong></td>
<td>NAS/强化学习</td>
<td>自动优化</td>
<td>计算成本高</td>
<td><code>AMC (AutoML for Model Compression)</code>, <code>Meta-Pruning</code></td>
</tr>
</tbody>
</table>
<img src="https://ucc.alicdn.com/yysinyik4knec/developer-article1644450/20241207/de25bb1591524da481677d9008ecc078.png?x-oss-process=image/resize,w_1400/format,webp" alt="image" style="zoom: 67%;">
<ul>
<li><strong>非结构化剪枝（Unstructured Pruning）</strong>：直接删除模型中的某些参数，通常基于参数的绝对值大小。这种方法可以实现较高的压缩比，但可能会破坏模型的整体结构。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 应用权重剪枝</span><br><span class="line">def apply_pruning(model, amount=0.2):</span><br><span class="line">    for name, module in model.named_modules():</span><br><span class="line">        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):</span><br><span class="line">            prune.l1_unstructured(module, name=&#x27;weight&#x27;, amount=amount)</span><br><span class="line">            print(f&quot;Applied pruning on &#123;name&#125;&quot;)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>结构化剪枝（Structured Pruning）</strong>：删除模型中的特定结构单元，如滤波器、通道或层。这种方法不会破坏模型的整体结构，更适合硬件加速。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">threshold = 0.01</span><br><span class="line">for name, module in model.named_modules():</span><br><span class="line">    if isinstance(module, nn.Conv2d):</span><br><span class="line">        # 计算每个卷积核的L2范数</span><br><span class="line">        kernel_norms = torch.norm(module.weight, dim=(1, 2, 3))</span><br><span class="line">        # 找到小于阈值的卷积核索引</span><br><span class="line">        prune_indices = torch.where(kernel_norms &lt; threshold)[0]</span><br><span class="line">        # 将这些卷积核的权重置零</span><br><span class="line">        module.weight[prune_indices] = 0</span><br></pre></td></tr></table></figure>
<ul>
<li>
<h4 id="基于梯度的剪枝"><a class="markdownIt-Anchor" href="#基于梯度的剪枝"></a> <strong>基于梯度的剪枝</strong></h4>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line">def gradient_magnitude_pruning(weight, gradient, percentile=0.5):</span><br><span class="line">    &quot;&quot;&quot;基于梯度幅值剪枝权重&quot;&quot;&quot;</span><br><span class="line">    num_zeros = round(weight.numel() * percentile)  # 计算剪枝元素数量</span><br><span class="line">    threshold = gradient.abs().view(-1).kthvalue(num_zeros).values  # 计算剪枝阈值</span><br><span class="line">    mask = gradient.abs() &gt; threshold  # 生成掩码</span><br><span class="line">    weight.mul_(mask.to(weight.device))  # 应用掩码剪枝</span><br><span class="line">    return weight</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="注意力迁移"><a class="markdownIt-Anchor" href="#注意力迁移"></a> 注意力迁移</h1>
<p><a target="_blank" rel="noopener" href="https://github.com/szagoruyko/attention-transfer?tab=readme-ov-file">https://github.com/szagoruyko/attention-transfer?tab=readme-ov-file</a></p>
<h2 id="定义-2"><a class="markdownIt-Anchor" href="#定义-2"></a> 定义</h2>
<ul>
<li>注意力迁移的思想来源于知识蒸馏，但与<strong>传统知识蒸馏主要关注最后层的知识不同，注意力迁移关注训练过程中特征图中的知识</strong>。</li>
<li>其目的是通过将教师网络的注意力图迁移到学生网络，提升学生网络的性能，同时实现模型的轻量化。</li>
</ul>
<h2 id="注意力机制"><a class="markdownIt-Anchor" href="#注意力机制"></a> 注意力机制</h2>
<ul>
<li><strong>空间域（Spatial Domain）</strong>：关注特征空间信息，决定空间中哪些区域重要。例如，通过动态注意力机制来选择性地关注图像中的特定区域。</li>
<li><strong>通道域（Channel Domain）</strong>：关注通道信息，如Squeeze-and-Excitation Networks（SENet）。SENet通过全局平均池化、降维再升维的方式为通道分配权重，增强重要通道的特征。</li>
<li><strong>混合域（Mixed Domain）</strong>：同时关注空间域和通道域，如CBAM等注意力机制，综合考虑特征空间和通道信息来生成注意力图。</li>
</ul>
<h2 id="算法部分"><a class="markdownIt-Anchor" href="#算法部分"></a> 算法部分</h2>
<ul>
<li><strong>基于激活的注意力迁移（Activation-based Attention Transfer）</strong>：
<ul>
<li>在前馈过程中，通过教师网络的激活特征图来引导学生网络的学习。</li>
<li>教师网络的激活特征图反映了输入数据在不同区域的重要性，<strong>学生网络通过模仿这些激活特征图</strong>来学习关注重要的区域。</li>
</ul>
</li>
<li><strong>基于梯度的注意力迁移（Gradient-based Attention Transfer）</strong>：
<ul>
<li>在反馈过程中，对教师网络和学生网络的交叉熵损失函数分别求梯度，<strong>将教师网络的梯度作为注意力图转移到学生网络</strong>。</li>
<li>关注那些对输出影响大的区域，通过构造损失函数，使得学生网络的梯度注意力图与教师网络的梯度注意力图接近，从而实现知识的迁移。</li>
</ul>
</li>
</ul>
<h1 id="低秩分解"><a class="markdownIt-Anchor" href="#低秩分解"></a> 低秩分解</h1>
<h1 id="轻量化网络结构"><a class="markdownIt-Anchor" href="#轻量化网络结构"></a> 轻量化网络结构</h1>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://shakewely.github.io/2025/02/05/2025%E5%B9%B42%E6%9C%885%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" data-id="cm6rncay50000xgv42v0x1l8p" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E3%80%81model/" rel="tag">轻量化神经网络、model</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-2025年1月9日-语言模型" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2025/01/08/2025%E5%B9%B41%E6%9C%889%E6%97%A5-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" class="article-date">
  <time class="post-time" datetime="2025-01-07T16:22:05.000Z" itemprop="datePublished">
    <span class="post-month">1月</span><br/>
    <span class="post-day">08</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/01/08/2025%E5%B9%B41%E6%9C%889%E6%97%A5-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/">2025年1月9日 语言模型</a>
    </h1>
  

        <div>
          
          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ol>
<li>预训练 浅层特征不变 高层改变
<ol>
<li>冻结</li>
<li>fine-tuning</li>
</ol>
</li>
</ol>
<h2 id="统计语言模型"><a class="markdownIt-Anchor" href="#统计语言模型"></a> 统计语言模型</h2>
<p><img src="/2025/01/08/2025%E5%B9%B41%E6%9C%889%E6%97%A5-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/image-20250103124916040.png" alt="image-20250103124916040"></p>
<ol>
<li>马尔科夫链</li>
<li></li>
</ol>
<h2 id="神经网络语言模型"><a class="markdownIt-Anchor" href="#神经网络语言模型"></a> 神经网络语言模型</h2>
<p>word embedding 例子</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
<p>word embedding 就是 预训练的frozen</p>
<p><img src="/2025/01/08/2025%E5%B9%B41%E6%9C%889%E6%97%A5-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/image-20250108002222562.png" alt></p>
<p><img src="/2025/01/08/2025%E5%B9%B41%E6%9C%889%E6%97%A5-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/image-20250108002349457.png" alt="image-20250108002349457" style="zoom:50%;"><img src="/2025/01/08/2025%E5%B9%B41%E6%9C%889%E6%97%A5-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/image-20250108002355983.png" alt="image-20250108002355983"><img src="/2025/01/08/2025%E5%B9%B41%E6%9C%889%E6%97%A5-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/image-20250108002359696.png" alt="image-20250108002359696" style="zoom:50%;"><img src="/2025/01/08/2025%E5%B9%B41%E6%9C%889%E6%97%A5-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/image-20250108002402739.png" alt="image-20250108002402739" style="zoom:50%;"></p>
<h2 id="改进的注意力机制"><a class="markdownIt-Anchor" href="#改进的注意力机制"></a> 改进的注意力机制</h2>
<h3 id><a class="markdownIt-Anchor" href="#"></a> </h3>
<h3 id="1-lora-低秩近似在-transformer-中的应用"><a class="markdownIt-Anchor" href="#1-lora-低秩近似在-transformer-中的应用"></a> 1 LORA 低秩近似在 Transformer 中的应用</h3>
<p>在 Transformer 中，低秩近似主要应用于自注意力计算，特别是通过优化注意力矩阵的存储和计算来提高效率。以下是两种常见的实现方式：</p>
<ol>
<li>
<p><strong>Linformer</strong>：Linformer 是一种基于低秩近似的优化方法。Linformer 的核心思想是将标准自注意力中的注意力矩阵近似为低秩矩阵。Linformer 假设注意力矩阵可以用低秩矩阵来近似，因此将其从一个全连接矩阵（n×n）降为一个n×k  的矩阵（其中 kkk 是较小的值，通常比 nnn 小得多）。通过这种低秩近似，计算复杂度从</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1141079999999999em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p>
<p>降低为</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mo>⋅</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n \cdot k)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mclose">)</span></span></span></span></span></p>
<p>大大减少了计算量，特别是在长序列时效果显著。</p>
</li>
<li>
<p><strong>Performer</strong>：Performer 是另一种采用低秩近似的 Transformer 变种，它通过引入一种叫做 <strong>线性注意力</strong> 的方法，采用<strong>随机特征来近似注意力矩阵</strong>的乘积。这种方法能够将原本的</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1141079999999999em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p>
<p>计算复杂度降低为</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal">n</span><span class="mclose">)</span></span></span></span></span></p>
<p>，使得 Transformer 可以处理非常长的序列。</p>
</li>
</ol>
<h4 id="低秩近似的优点"><a class="markdownIt-Anchor" href="#低秩近似的优点"></a> 低秩近似的优点</h4>
<ul>
<li><strong>减少计算复杂度</strong>：通过将高秩矩阵分解为低秩矩阵，降低了计算复杂度，从而加快了训练和推理速度。</li>
<li><strong>节省内存</strong>：低秩矩阵的存储通常比原始矩阵所需的内存少，因此节省了计算过程中的内存消耗。</li>
<li><strong>适用于长序列</strong>：对于长序列，低秩近似可以大大提高 Transformer 的效率，使其能够处理更长的输入序列而不遇到内存瓶颈。</li>
</ul>
<h4 id="低秩近似的缺点"><a class="markdownIt-Anchor" href="#低秩近似的缺点"></a> 低秩近似的缺点</h4>
<ul>
<li><strong>性能损失</strong>：低秩近似虽然可以减少计算复杂度，但可能会带来一定的性能损失，尤其是在近似效果不佳时。较低的秩可能无法捕捉到原矩阵中的复杂关系，因此可能影响模型的精度。</li>
<li><strong>近似精度问题</strong>：低秩近似依赖于如何选择低秩矩阵的秩（rank）。如果选择的秩过小，可能会导致较大的误差，从而影响模型的效果。</li>
</ul>
<p>、</p>
<h3 id="lora-的应用步骤"><a class="markdownIt-Anchor" href="#lora-的应用步骤"></a> LoRA 的应用步骤</h3>
<h4 id="1-选择适当的层"><a class="markdownIt-Anchor" href="#1-选择适当的层"></a> 1. <strong>选择适当的层</strong></h4>
<ul>
<li>通常，LoRA 应用在 Transformer 中的关键层，如注意力层和前馈层。选择这些层是因为它们通常是最有影响力的层，影响模型的表达能力。</li>
</ul>
<h4 id="2-添加低秩矩阵"><a class="markdownIt-Anchor" href="#2-添加低秩矩阵"></a> 2. <strong>添加低秩矩阵</strong></h4>
<ul>
<li>对于每个需要修改的层（如注意力层中的权重矩阵），LoRA 在其上加上低秩矩阵。通常，这些低秩矩阵的秩较小，因此不会显著增加模型的复杂度。</li>
</ul>
<h4 id="3-训练低秩矩阵"><a class="markdownIt-Anchor" href="#3-训练低秩矩阵"></a> 3. <strong>训练低秩矩阵</strong></h4>
<ul>
<li>在微调过程中，仅训练低秩矩阵的参数，而保持原始的预训练权重不变。这意味着只有少量参数需要更新，从而加速训练并降低存储需求。</li>
</ul>
<h4 id="4-合成模型"><a class="markdownIt-Anchor" href="#4-合成模型"></a> 4. <strong>合成模型</strong></h4>
<ul>
<li>在微调结束后，最终的模型将包括原始的预训练权重和经过 LoRA 微调的低秩矩阵</li>
</ul>
<h3 id="代码实现"><a class="markdownIt-Anchor" href="#代码实现"></a> 代码实现</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="adapter"><a class="markdownIt-Anchor" href="#adapter"></a> Adapter</h2>
<p>适</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://shakewely.github.io/2025/01/08/2025%E5%B9%B41%E6%9C%889%E6%97%A5-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" data-id="cm6rncay70001xgv40x8a6tc6" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/llm/" rel="tag">llm</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-2025年1月11日-视觉模型" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/" class="article-date">
  <time class="post-time" datetime="2024-12-31T07:20:05.000Z" itemprop="datePublished">
    <span class="post-month">12月</span><br/>
    <span class="post-day">31</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/">2025年1月2日 视觉模型</a>
    </h1>
  

        <div>
          
          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>端到端单阶段</p>
<p>两阶段的</p>
<p>RCNN</p>
<h2 id="rcnn"><a class="markdownIt-Anchor" href="#rcnn"></a> RCNN</h2>
<h3 id="rcnn-2"><a class="markdownIt-Anchor" href="#rcnn-2"></a> RCNN</h3>
<h4 id="0-原理"><a class="markdownIt-Anchor" href="#0-原理"></a> 0 原理</h4>
<p><strong><img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250104184356637.png" alt="image-20250104184356637" style="zoom: 33%;"></strong></p>
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250221144723650.png" alt="image-20250221144723650" style="zoom:50%;"> 
<h4 id="1-ss算法"><a class="markdownIt-Anchor" href="#1-ss算法"></a> 1 ss算法</h4>
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250104184501872.png" alt="image-20250104184501872" style="zoom: 33%;">
<h4 id="2-提取特征"><a class="markdownIt-Anchor" href="#2-提取特征"></a> 2 提取特征</h4>
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250104184859288.png" alt="image-20250104184859288" style="zoom: 33%;">
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250104184531836.png" alt="image-20250104184531836" style="zoom:50%;">
<h4 id="3-svm"><a class="markdownIt-Anchor" href="#3-svm"></a> 3 SVM</h4>
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250104184713372.png" alt="image-20250104184713372" style="zoom: 33%;">
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250104185007538.png" alt="image-20250104185007538" style="zoom: 33%;">
<p><strong>IOU</strong></p>
<blockquote>
<p><img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250104185250090.png" alt="image-20250104185250090" style="zoom: 50%;"><img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250104185256627.png" alt="image-20250104185256627" style="zoom: 25%;"></p>
</blockquote>
<blockquote>
<p><img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250104185308808.png" alt="image-20250104185308808" style="zoom: 33%;"> <img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250104185322806.png" alt="image-20250104185322806" style="zoom:33%;"></p>
</blockquote>
<h4 id="4-回归分类器"><a class="markdownIt-Anchor" href="#4-回归分类器"></a> 4 回归分类器</h4>
<p>pass</p>
<h4 id="优缺"><a class="markdownIt-Anchor" href="#优缺"></a> 优缺</h4>
<ul>
<li>冗余</li>
<li>慢</li>
<li>空间大</li>
</ul>
<h3 id="fast-rcnn"><a class="markdownIt-Anchor" href="#fast-rcnn"></a> Fast RCNN</h3>
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250104190121582.png" alt="image-20250104190121582" style="zoom:50%;">
<h4 id="0-原理-2"><a class="markdownIt-Anchor" href="#0-原理-2"></a> 0 原理</h4>
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250221152009699.png" alt="image-20250221152009699" style="zoom: 80%;">  
 <img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250221144938969.png" alt="image-20250221144938969" style="zoom:50%;"> 
<h4 id="1-减少重复计算特征"><a class="markdownIt-Anchor" href="#1-减少重复计算特征"></a> 1 减少重复计算特征</h4>
<p>通过全连接层进行</p>
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250104191656317.png" alt="image-20250104191656317" style="zoom:33%;">
<p><strong>正负样本</strong></p>
<blockquote></blockquote>
<h4 id="2-分类器"><a class="markdownIt-Anchor" href="#2-分类器"></a> 2 分类器</h4>
<p>全连接层实现</p>
<h4 id="3-边界框回归器"><a class="markdownIt-Anchor" href="#3-边界框回归器"></a> 3 边界框回归器</h4>
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250104192439278.png" alt="image-20250104192439278" style="zoom: 33%;">
<h4 id="4-损失计算"><a class="markdownIt-Anchor" href="#4-损失计算"></a> 4 损失计算</h4>
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250104192659858.png" alt="image-20250104192659858" style="zoom:33%;">
<h3 id="faster-rcnn"><a class="markdownIt-Anchor" href="#faster-rcnn"></a> Faster RCNN</h3>
<h4 id="0-原理-3"><a class="markdownIt-Anchor" href="#0-原理-3"></a> 0 原理</h4>
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250104194051582.png" alt="image-20250104194051582" style="zoom:33%;"> 
<p><strong>就是用RPN替换SS算法</strong></p>
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250104194210537.png" alt="image-20250104194210537" style="zoom:33%;">  
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250104194406137.png" alt="image-20250104194406137" style="zoom: 33%;">  
<h4 id="1-rpn"><a class="markdownIt-Anchor" href="#1-rpn"></a> 1 RPN</h4>
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250104194441284.png" alt="image-20250104194441284" style="zoom: 50%;">	
<ul>
<li>3x3 卷积</li>
<li>在并联两个全连接层 生成:背景和前景 + 4个参数</li>
<li></li>
<li></li>
</ul>
<h4 id="2-anchor"><a class="markdownIt-Anchor" href="#2-anchor"></a> 2 anchor</h4>
<p><strong>实现原理</strong></p>
<p><img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250104194722542.png" alt="image-20250104194722542" style="zoom:33%;"><img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250104194733782.png" alt="image-20250104194733782" style="zoom:33%;"></p>
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250104194922010.png" alt="image-20250104194922010" style="zoom: 25%;"> 
<p>anchor 个数</p>
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250104195441622.png" alt="image-20250104195441622" style="zoom: 25%;">  
<p><strong>小感受野也可以预测大特征</strong></p>
<blockquote>
 <img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250104195137512.png" alt="image-20250104195137512" style="zoom:33%;">  
</blockquote>
<h4 id="3-损失计算"><a class="markdownIt-Anchor" href="#3-损失计算"></a> 3 损失计算</h4>
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250104200305676.png" alt="image-20250104200305676" style="zoom:33%;">
<h2 id="ssd"><a class="markdownIt-Anchor" href="#ssd"></a> SSD</h2>
<h3 id="原理"><a class="markdownIt-Anchor" href="#原理"></a> 原理</h3>
<p>在<strong>六个不同的层上预测不同大小的目标</strong>[ 因为:卷积程度越深 感受野越大 抽象的特征越大 那么细节就越少]</p>
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250221160156493.png" alt="image-20250221160156493" style="zoom:67%;">
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250104171147389.png" alt="image-20250104171147389" style="zoom:33%;">
<ul>
<li>b预测小目标 🐱</li>
<li>C预测大目标 🐕</li>
</ul>
<h4 id="default-box"><a class="markdownIt-Anchor" href="#default-box"></a> default box</h4>
<p>从不同的特征层中产生 每个特征图（feature map）上的像素点为中心生成default box。中心点坐标为特征图位置的映射值，计算公式</p>
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250104171745323.png" alt="image-20250104171745323" style="zoom:67%;">
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250104172055346.png" alt="image-20250104172055346" style="zoom:67%;">
<h3 id="训练"><a class="markdownIt-Anchor" href="#训练"></a> 训练</h3>
<h4 id="模型"><a class="markdownIt-Anchor" href="#模型"></a> 模型</h4>
<p><img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250104170756080.png" alt="image-20250104170756080"><img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250104170840590.png" alt="image-20250104170840590" style="zoom: 25%;"></p>
<h4 id="数据处理"><a class="markdownIt-Anchor" href="#数据处理"></a> 数据处理</h4>
<h4 id="损失计算"><a class="markdownIt-Anchor" href="#损失计算"></a> 损失计算</h4>
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250104172613364.png" alt="image-20250104172613364" style="zoom:50%;">
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250104172751053.png" alt="image-20250104172751053" style="zoom:33%;">
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250104172759925.png" alt="image-20250104172759925" style="zoom: 33%;">
<h4 id="预测"><a class="markdownIt-Anchor" href="#预测"></a> 预测</h4>
<p><img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250104172201462.png" alt="image-20250104172201462"> ]</p>
<p>参数数目</p>
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250104172401969.png" alt="image-20250104172401969" style="zoom: 33%;"> 
<h3 id="优缺-2"><a class="markdownIt-Anchor" href="#优缺-2"></a> 优缺</h3>
<h3 id="qa"><a class="markdownIt-Anchor" href="#qa"></a> QA</h3>
<h1 id="yolo"><a class="markdownIt-Anchor" href="#yolo"></a> YOLO</h1>
<h2 id="yolov1"><a class="markdownIt-Anchor" href="#yolov1"></a> <a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_43334693/article/details/129011644?spm=1001.2014.3001.5501">YOLOv1</a></h2>
<ul>
<li>端到端单阶段</li>
<li>目标检测问题看作回归问题</li>
</ul>
<h4 id="bounding-box"><a class="markdownIt-Anchor" href="#bounding-box"></a> bounding box</h4>
<ul>
<li>x——框中心的横坐标</li>
<li>y——框中心的纵坐标</li>
<li>w——框的宽度</li>
<li>h——框的高度</li>
<li>置信度（confidence）</li>
</ul>
<p>在YOLOV1中S = 7 , B = 2即将每个输入图片分成7 × 7 个网格，每个网格将生成2 个预测框，用来框出图片中的物体。 每个框会预测出5个变量值，所以一个网格生成两个框，一个框带有5个属性，所以一个格就需要预测出5 × 2 = 10 个变量值。<br>
在YOLOv1中有<strong>20个类别的物体的条件概率</strong>，所以输出结果的后20个值就表示每一个小网格（grid cell）对应每一类物体的概率，即由该网格生产的两个预测框对应每一类物体的概率。 由此，输出的7 × 7 × 30 向量的每一项含义便清楚了。<br>
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250104151446113.png" alt="image-20250104151446113" style="zoom: 67%;"></p>
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250104154104316.png" alt="image-20250104154104316" style="zoom: 33%;">
<h4 id="训练-2"><a class="markdownIt-Anchor" href="#训练-2"></a> 训练</h4>
<h5 id="网络结构"><a class="markdownIt-Anchor" href="#网络结构"></a> 网络结构</h5>
<p><img src="https://i-blog.csdnimg.cn/blog_migrate/64c2116c3e9f6d55b5aa2a8fab982a26.png" alt="img"></p>
<p>YOLOV1的输入为448 × 448 × 3 图像，输出大小为7 × 7 × 30 向量</p>
<p>在网络结构的最后一层使用的是线性激活函数，其他层使用的是****leaky ReLU****激活函数，YOLOv1中采用的leaky ReLU定义公式如下</p>
<p><img src="https://i-blog.csdnimg.cn/direct/dba9296aa0d24233abe7b02e75e4b7e9.png" alt="img"></p>
<h5 id="数据处理-2"><a class="markdownIt-Anchor" href="#数据处理-2"></a> 数据处理</h5>
<h5 id="损失计算-2"><a class="markdownIt-Anchor" href="#损失计算-2"></a> 损失计算</h5>
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250104155000144.png" alt="image-20250104155000144" style="zoom:67%;">
<p><img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250104155310820.png" alt="image-20250104155310820" style="zoom:33%;"> 大小目标对偏移影响不同</p>
<h5 id="预测极大值抑制"><a class="markdownIt-Anchor" href="#预测极大值抑制"></a> 预测–极大值抑制</h5>
<p>7X7X2个bounding box 进行 过滤与非极大值抑制</p>
<p><img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250104152002611.png" alt></p>
<p><img src="https://i-blog.csdnimg.cn/blog_migrate/d358fa901b47a00baa710246af0ea197.gif" alt="img"></p>
<h4 id="优缺-3"><a class="markdownIt-Anchor" href="#优缺-3"></a> 优缺</h4>
<p>无法很好地处理小目标和重叠目标</p>
<h4 id="qa-2"><a class="markdownIt-Anchor" href="#qa-2"></a> QA</h4>
<ol>
<li>
<h5 id="为什么两个bounding-box"><a class="markdownIt-Anchor" href="#为什么两个bounding-box"></a> 为什么两个bounding box</h5>
</li>
</ol>
<p>每个 bounding box 都有一组独立的参数，包括中心坐标、宽度、高度和置信度，能够分别拟合不同的目标。</p>
<h2 id="yolov2"><a class="markdownIt-Anchor" href="#yolov2"></a> YOLOV2</h2>
<h4 id="1-bn-加入"><a class="markdownIt-Anchor" href="#1-bn-加入"></a> 1 BN 加入</h4>
<h4 id="2-anchor-2"><a class="markdownIt-Anchor" href="#2-anchor-2"></a> 2 anchor</h4>
<p>K-mean聚类实现</p>
<p>模型学习如何调整预定义的 anchor，而不是从零开始预测边界框的所有参数。</p>
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250104164542277.png" alt="image-20250104164542277" style="zoom: 67%;"> 
<h4 id="3-pass-through-layer"><a class="markdownIt-Anchor" href="#3-pass-through-layer"></a> 3 pass through layer</h4>
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250104160704522.png" alt="image-20250104160704522" style="zoom:67%;"> 
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250104160931404.png" alt="image-20250104160931404" style="zoom:50%;"> 
<h5 id="4-多尺度数据输入"><a class="markdownIt-Anchor" href="#4-多尺度数据输入"></a> 4 多尺度数据输入</h5>
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250104161259611.png" alt="image-20250104161259611" style="zoom:50%;"> 
<h4 id="faster"><a class="markdownIt-Anchor" href="#faster"></a> Faster</h4>
<p>1</p>
<h2 id="yolov3"><a class="markdownIt-Anchor" href="#yolov3"></a> YOLOV3</h2>
<h4 id="单标签分类改进为多标签分类"><a class="markdownIt-Anchor" href="#单标签分类改进为多标签分类"></a> 单标签分类改进为多标签分类</h4>
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250105125615278.png" alt="image-20250105125615278" style="zoom:50%;"> 
<h4 id="pan"><a class="markdownIt-Anchor" href="#pan"></a> PAN</h4>
<h3 id="yolov4"><a class="markdownIt-Anchor" href="#yolov4"></a> YOLOV4</h3>
<h3 id="1-原理"><a class="markdownIt-Anchor" href="#1-原理"></a> 1 原理</h3>
<p><img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250105142450863.png" alt="image-20250105142450863"><img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250105142605476.png" alt="image-20250105142605476" style="zoom:67%;"></p>
<h3 id="2-输入端"><a class="markdownIt-Anchor" href="#2-输入端"></a> 2 输入端</h3>
<h5 id="sat自对抗训练"><a class="markdownIt-Anchor" href="#sat自对抗训练"></a> SAT自对抗训练</h5>
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250105151656124.png" alt="image-20250105151656124" style="zoom:67%;">
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250105151712577.png" alt="image-20250105151712577" style="zoom:67%;">
<h5 id="label-smoothing类标签平滑"><a class="markdownIt-Anchor" href="#label-smoothing类标签平滑"></a> Label Smoothing类标签平滑</h5>
<h3 id="3-网络结构"><a class="markdownIt-Anchor" href="#3-网络结构"></a> 3 网络结构</h3>
<h4 id="1-backbone"><a class="markdownIt-Anchor" href="#1-backbone"></a> 1 Backbone</h4>
<h5 id="31-csp"><a class="markdownIt-Anchor" href="#31-csp"></a> 3.1 CSP</h5>
<h5 id="img-src2025年1月11日-视觉模型image-20250105130506026png-altimage-20250105130506026-stylezoom50"><a class="markdownIt-Anchor" href="#img-src2025年1月11日-视觉模型image-20250105130506026png-altimage-20250105130506026-stylezoom50"></a> <img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250105130506026.png" alt="image-20250105130506026" style="zoom:50%;"></h5>
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250105152215756.png" alt="image-20250105152215756" style="zoom:50%;"> 
<h4 id="4-neck"><a class="markdownIt-Anchor" href="#4-neck"></a> 4 Neck</h4>
<p>一个颈部neck由几个自下而上的路径和几个自上而下的路径组成。具有该机制的网络包括特征金字塔网络(FPN)、路径汇聚网络(PAN)、BiFPN和NAS-FPN。</p>
<h5 id="41-fpn"><a class="markdownIt-Anchor" href="#41-fpn"></a> 4.1 FPN</h5>
<p>引入了自底向上的路径，使得底层信息更容易传到顶部</p>
<p><img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250105150718121.png" alt="image-20250105150718121"></p>
<p><img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250105150726295.png" alt="image-20250105150726295"></p>
<h5 id="42-pan"><a class="markdownIt-Anchor" href="#42-pan"></a> 4.2 PAN</h5>
<p>特征层之间融合时是直接通过addition的方式进行融合的，而Yolov4中则采用在通道方向concat拼接操作融合的</p>
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250105150948205.png" alt="image-20250105150948205" style="zoom:67%;">
<p><img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250105150823004.png" alt="image-20250105150823004"></p>
<h5 id="43-spp"><a class="markdownIt-Anchor" href="#43-spp"></a> 4.3 SPP</h5>
<p>空间金字塔池化 特征提取池化</p>
<p>通过在输入特征图上<strong>采用不同尺度的池化窗口</strong>（如1×1、2×2、4×4 等），SPP 能够从不同的空间范围内有效地捕获图像中不同大小和比例的目标特征</p>
<blockquote>
<p>旨在解决卷积神经网络中固定大小输入的限制。它能够对任意大小的 输入特征图进行处理，并在不同尺度下提取特征，从而显著增强模型对不同大小目标的感 知能力</p>
</blockquote>
<p><img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250303181539006.png" alt="image-20250303181539006"></p>
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250105150606229.png" alt="image-20250105150606229" style="zoom:50%;">
<h3 id="4-bof"><a class="markdownIt-Anchor" href="#4-bof"></a> 4 BoF</h3>
<p>我们把这些只会改变培训策略或只增加培训成本的方法称为“bag of freebies”。</p>
<h4 id="1-数据增强"><a class="markdownIt-Anchor" href="#1-数据增强"></a> 1 数据增强</h4>
<h4 id="2-解决数据集中语义分布偏差问题"><a class="markdownIt-Anchor" href="#2-解决数据集中语义分布偏差问题"></a> 2 解决数据集中语义分布偏差问题</h4>
<p>不同类之间存在数据不平衡的问题</p>
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250105143153315.png" alt="image-20250105143153315" style="zoom: 67%;"> 
<h4 id="3-边界框bbox回归的目标函数"><a class="markdownIt-Anchor" href="#3-边界框bbox回归的目标函数"></a> 3 边界框(BBox)回归的目标函数 ??</h4>
<p>直接估计BBox中每个点的坐标值是要将这些点作为自变量来处理，但实际上并没有考虑对象本身的完整性</p>
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250105143449548.png" alt="image-20250105143449548" style="zoom:67%;">
<h3 id="5-bos"><a class="markdownIt-Anchor" href="#5-bos"></a> 5 BoS</h3>
<p>对于那些只增加少量推理成本但又能显著提高目标检测精度的插件模块和后处理方法，我们称它们为“bag of specials&quot;</p>
<h4 id="1-增强感受野"><a class="markdownIt-Anchor" href="#1-增强感受野"></a> 1 增强感受野</h4>
<p><strong>①改进的SPP模块</strong></p>
<p><img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250105143750945.png" alt="image-20250105143750945"></p>
<p><strong>②ASPP模块</strong></p>
<p><img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250105143836518.png" alt="image-20250105143836518"></p>
<p><strong>③RFB模块</strong></p>
<p><img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250105143846925.png" alt="image-20250105143846925"></p>
<h4 id="2-注意力机制"><a class="markdownIt-Anchor" href="#2-注意力机制"></a> 2 注意力机制</h4>
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250105143912123.png" alt="image-20250105143912123" style="zoom:50%;"> 
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250105143920723.png" alt="image-20250105143920723" style="zoom: 50%;"> 
<h4 id="3-特征融合"><a class="markdownIt-Anchor" href="#3-特征融合"></a> 3 特征融合</h4>
<p><strong>①SFAM：</strong> 主要思想是利用SE模块在多尺度的拼接特征图上进行信道级重加权。</p>
<p><strong>②ASFF：</strong> 使用softmax对多尺度拼接特征图在点维度进行加权。</p>
<p><strong>③BiFPN：</strong> 提出了多输入加权剩余连接来执行按比例的水平重加权，然后添加不同比例的特征图。</p>
<h4 id="4-激活函数"><a class="markdownIt-Anchor" href="#4-激活函数"></a> 4 激活函数</h4>
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250105144012738.png" alt="image-20250105144012738" style="zoom:50%;"> 
<h3 id="6-注意力机制"><a class="markdownIt-Anchor" href="#6-注意力机制"></a> 6 注意力机制</h3>
<h4 id="61-cbamconvolutional-block-attention-module注意力机制"><a class="markdownIt-Anchor" href="#61-cbamconvolutional-block-attention-module注意力机制"></a> 6.1 **CBAM(Convolutional Block Attention Module)**注意力机制</h4>
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250105151035854.png" alt="image-20250105151035854" style="zoom: 50%;">
<h4 id="62-channel-attention-module通道注意力模块"><a class="markdownIt-Anchor" href="#62-channel-attention-module通道注意力模块"></a> 6.2 <strong>Channel attention module(通道注意力模块)</strong></h4>
<p><img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250105151238032.png" alt="image-20250105151238032"></p>
<h4 id="63-attention-module空间注意力模块"><a class="markdownIt-Anchor" href="#63-attention-module空间注意力模块"></a> 6.3 <strong>attention module(空间注意力模块)</strong></h4>
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250105151404902.png" alt="image-20250105151404902" style="zoom:80%;">
<h3 id="dropblock正则化"><a class="markdownIt-Anchor" href="#dropblock正则化"></a> Dropblock正则化</h3>
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250105130645146.png" alt="image-20250105130645146" style="zoom: 50%;"> 
<blockquote>
<p>Q：全连接层上效果很好的Dropout在卷积层上效果并不好？</p>
<pre><code>中间Dropout的方式会随机的删减丢弃一些信息，但Dropblock的研究者认为，卷积层对于这种随机丢弃并不敏感，因为卷积层通常是三层连用：卷积+激活+池化层，池化层本身就是对相邻单元起作用。

而且即使随机丢弃，卷积层仍然可以从相邻的激活单元学习到相同的信息。因此，在全连接层上效果很好的Dropout在卷积层上效果并不好。所以右图Dropblock的研究者则干脆整个局部区域进行删减丢弃。
</code></pre>
</blockquote>
<h2 id="yolov5"><a class="markdownIt-Anchor" href="#yolov5"></a> <a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_43334693/article/details/129312409">YOLOV5</a></h2>
<h3 id="1-原理-2"><a class="markdownIt-Anchor" href="#1-原理-2"></a> 1 原理</h3>
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250105140339482.png" alt="image-20250105140339482" style="zoom: 80%;"> 
<h3 id="2-输入"><a class="markdownIt-Anchor" href="#2-输入"></a> 2 输入</h3>
<p>数据增强</p>
<h3 id="3-网络结构-2"><a class="markdownIt-Anchor" href="#3-网络结构-2"></a> 3 网络结构</h3>
<p><img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250303181011801.png" alt></p>
<h4 id="31-backbone"><a class="markdownIt-Anchor" href="#31-backbone"></a> 3.1 Backbone</h4>
<h5 id="311-focus-结构"><a class="markdownIt-Anchor" href="#311-focus-结构"></a> 3.1.1 Focus 结构</h5>
<p>4×4的3通道图 像切片后变成2×2的12通道的特征图</p>
<p><img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250105141242384.png" alt="image-20250105141242384"></p>
<p><img src="https://i-blog.csdnimg.cn/blog_migrate/009d2d69aae2b1cb6ecede41530f82bf.png" alt="img"></p>
<p><img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250105141341837.png" alt="image-20250105141341837"></p>
<p><img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250105141346413.png" alt="image-20250105141346413"></p>
<ul>
<li>高分辨率 图像的信息从空间维度转化到通道维度，这一转化不仅保留了大量输入信息，还缩减了输 入尺寸，有助于提升网络的训练和推理速度</li>
</ul>
<h5 id="312-csb"><a class="markdownIt-Anchor" href="#312-csb"></a> 3.1.2 CSB</h5>
<p>结合了卷积、批量归一化以及SiLU 激活函数，这种配置使得该模块能够有效地提取和传递特征信息，加快模型的收敛速度并 增强检测能力。</p>
<p><img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250303180658523.png" alt></p>
<h5 id="313-csp"><a class="markdownIt-Anchor" href="#313-csp"></a> 3.1.3 CSP</h5>
<p>一种用于构建特征提取网络的重要组件。通过引入<strong>跨阶段部分连接</strong>和通道分割来加强特征的传播和利 用，从而提高了模型的性能和效率。</p>
<p><img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250303180919670.png" alt="image-20250303180919670"></p>
<p><img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250105141515679.png" alt="image-20250105141515679"></p>
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250105141525595.png" alt="image-20250105141525595" style="zoom:50%;"> 
<h5 id="314-sppf"><a class="markdownIt-Anchor" href="#314-sppf"></a> 3.1.4 SPPF</h5>
<p>SPPF将SPP原来并行的结构<strong>改进为串行结构</strong>，通过指定单一卷积核，<strong>每次池化后的输出直接作为下一个池化的输入</strong></p>
<p><img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250303182202107.png" alt="image-20250303182202107"></p>
<h4 id="32-neck"><a class="markdownIt-Anchor" href="#32-neck"></a> 3.2 Neck</h4>
<p><img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250105141614203.png" alt="image-20250105141614203"></p>
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250105141619565.png" alt="image-20250105141619565" style="zoom: 50%;">
<h5 id="321-路径聚合网络panet"><a class="markdownIt-Anchor" href="#321-路径聚合网络panet"></a> 3.2.1 路径聚合网络（PANet）</h5>
<p>PAN 通过增加<strong>自下而上</strong>的路径来增强 FPN 的结构，</p>
<p>其主要目的是改善信息的流动和特征图的利用效率。</p>
<h5 id="322-特征金字塔网络fpnet"><a class="markdownIt-Anchor" href="#322-特征金字塔网络fpnet"></a> 3.2.2 特征金字塔网络（FPNet）</h5>
<p>FPNet多尺度特征融合的方法，它在提取的特征层之间建立<strong>自上而下</strong>的路径</p>
<p>高层次的语义信息能够与低层次的细节<strong>信息结合</strong> 改进对<strong>小尺寸</strong>目标的检测能力</p>
<p><img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250303182750649.png" alt="image-20250303182750649"></p>
<h4 id="33-输出端"><a class="markdownIt-Anchor" href="#33-输出端"></a> 3.3 输出端</h4>
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250303183300954.png" alt="image-20250303183300954" style="zoom:80%;">
<p>当真实目标的中心点位于网格的左上角或右下角附近时，σ(tx)与σ(ty)的 值可能会接近0或1，这种极端值通常是网络难以实现的</p>
<p>对偏移量进行调整，将其<strong>从（0,1）缩放到（-0.5,1.5）</strong>，确保 模型输出的偏移量保持在0到1的范围</p>
<h3 id="4-损失函数"><a class="markdownIt-Anchor" href="#4-损失函数"></a> 4 损失函数</h3>
<p>总损失是定 位损失、分类损失和置信度损失的加权和，各损失的权重是通过调整超参数进行优化，以 平衡分类、定位和置信度之间的重要性</p>
<h3 id="5-训练策略"><a class="markdownIt-Anchor" href="#5-训练策略"></a> 5 训练策略</h3>
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250105123929037.png" alt="image-20250105123929037" style="zoom: 50%;"> 
<h3 id="6-魔改"><a class="markdownIt-Anchor" href="#6-魔改"></a> 6 魔改</h3>
<h4 id="61-网络结构"><a class="markdownIt-Anchor" href="#61-网络结构"></a> 6.1 网络结构</h4>
<img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250303184319327.png" alt="image-20250303184319327" style="zoom:67%;">
<h1 id="fcos"><a class="markdownIt-Anchor" href="#fcos"></a> FCOS</h1>
<h2 id="fcos-2"><a class="markdownIt-Anchor" href="#fcos-2"></a> FCOS</h2>
<p>参考： <a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_46142822/article/details/123958529%E3%80%81">https://blog.csdn.net/weixin_46142822/article/details/123958529、</a></p>
<h3 id="2-fcos-网络框架"><a class="markdownIt-Anchor" href="#2-fcos-网络框架"></a> 2 FCOS 网络框架</h3>
<p><img src="/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/image-20250309153327923.png" alt="image-20250309153327923"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://shakewely.github.io/2024/12/31/2025%E5%B9%B41%E6%9C%8811%E6%97%A5-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/" data-id="cm8q1tfhe000bpcv4hr161vro" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/model/" rel="tag">model</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-深度学习问题总结" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2024/06/07/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/" class="article-date">
  <time class="post-time" datetime="2024-06-07T08:37:27.321Z" itemprop="datePublished">
    <span class="post-month">6月</span><br/>
    <span class="post-day">07</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="交叉熵为什么常用于机器学习中的分类问题"><a class="markdownIt-Anchor" href="#交叉熵为什么常用于机器学习中的分类问题"></a> 交叉熵为什么常用于机器学习中的分类问题</h3>
<p>交叉熵在机器学习中的分类问题中广泛使用，尤其是在多分类和二分类任务中，这是由于交叉熵具有以下几个重要特性和优势：</p>
<h4 id="1-衡量真实分布和预测分布之间的差异"><a class="markdownIt-Anchor" href="#1-衡量真实分布和预测分布之间的差异"></a> 1. 衡量真实分布和预测分布之间的差异</h4>
<p>交叉熵能够很好地度量模型的预测概率分布与真实概率分布之间的差异。其定义为：</p>
<p class="katex-block katex-error" title="ParseError: KaTeX parse error: Undefined control sequence: \[ at position 1: \̲[̲ H(p, q) = - \s…">\[ H(p, q) = - \sum_{x} p(x) \log q(x) \]
</p>
<p>其中，( p(x) ) 是真实标签的概率分布，通常是one-hot编码（对于正确类别概率为1，其余类别概率为0），( q(x) ) 是模型的预测概率分布。</p>
<h4 id="2-对错误分类有较大惩罚"><a class="markdownIt-Anchor" href="#2-对错误分类有较大惩罚"></a> 2. 对错误分类有较大惩罚</h4>
<p>交叉熵损失函数对错误分类的惩罚较大，能够更有效地引导模型进行优化。举个例子，对于一个二分类问题，假设真实标签 ( p = 1 )，而模型预测的概率 ( q ) 很接近0（完全错误的预测），则交叉熵损失值会非常高。这种特性使得交叉熵在训练过程中能够更快速地减少错误分类的概率。</p>
<h4 id="3-可微性"><a class="markdownIt-Anchor" href="#3-可微性"></a> 3. 可微性</h4>
<p>交叉熵损失函数是可微的，这使得它非常适合梯度下降等优化算法。通过计算损失函数相对于模型参数的梯度，模型可以逐步更新参数以最小化损失，从而提高分类性能。</p>
<h4 id="4-与softmax结合良好"><a class="markdownIt-Anchor" href="#4-与softmax结合良好"></a> 4. 与Softmax结合良好</h4>
<p>在多分类问题中，交叉熵损失函数通常与Softmax激活函数一起使用。Softmax函数将模型的输出转化为一个概率分布：</p>
<p class="katex-block katex-error" title="ParseError: KaTeX parse error: Undefined control sequence: \[ at position 1: \̲[̲ \sigma(z_i) = …">\[ \sigma(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}} \]
</p>
<p>结合Softmax和交叉熵，损失函数的梯度计算更为简便，优化过程更稳定。</p>
<h4 id="5-适合概率解释"><a class="markdownIt-Anchor" href="#5-适合概率解释"></a> 5. 适合概率解释</h4>
<p>交叉熵损失函数直接基于概率，因此它能够提供一种自然的方式来解释模型的输出。例如，输出值可以直接解释为某个类别的概率，这在很多实际应用中是非常有用的，如图像分类、自然语言处理等。</p>
<h4 id="6-数学上的凸性"><a class="markdownIt-Anchor" href="#6-数学上的凸性"></a> 6. 数学上的凸性</h4>
<p>在二分类问题中，交叉熵损失函数是凸的，这意味着它有唯一的全局最优解。尽管在多分类问题中，交叉熵损失函数可能不是严格凸的，但在实践中通过适当的优化方法（如SGD，Adam等）依然能有效地找到较优解。</p>
<h3 id="实际例子"><a class="markdownIt-Anchor" href="#实际例子"></a> 实际例子</h3>
<p>以二分类问题为例，假设真实标签 ( y ) 为1（即正类），模型预测正类的概率为 ( \hat{y} )。交叉熵损失函数为：</p>
<p class="katex-block katex-error" title="ParseError: KaTeX parse error: Undefined control sequence: \[ at position 1: \̲[̲ L = -[y \log(\…">\[ L = -[y \log(\hat{y}) + (1 - y) \log(1 - \hat{y})] \]
</p>
<p>当 ( y = 1 ) 时，损失函数简化为 ( L = -\log(\hat{y}) )。如果模型预测 ( \hat{y} ) 很小，则损失会非常大，从而迫使模型在训练过程中提高对正类的预测概率。</p>
<p>总的来说，交叉熵损失函数由于其良好的数学性质和对分类问题的适用性，成为机器学习中分类问题的标准选择。</p>
<h2 id="高维而带来的数据稀疏性问题"><a class="markdownIt-Anchor" href="#高维而带来的数据稀疏性问题"></a> 高维而带来的数据稀疏性问题</h2>
<p><strong>距离度量失效</strong>：在高维空间中，数据点之间的距离度量（如欧氏距离）可能失去区分性。随着维度增加，不同数据点之间的距离变得越来越相似，从而导致传统的距离度量方法失效。这对基于距离的算法（如K近邻算法、聚类算法等）影响尤为显著。</p>
<p><strong>计算复杂度增加</strong>：高维数据需要处理的特征数增多，计算复杂度随之增加。这对存储和计算资源都是巨大的挑战，尤其是在处理大规模数据集时。</p>
<p><strong>维度诅咒</strong>：高维数据通常伴随着“维度诅咒”问题，即随着维度增加，数据点需要的样本量指数级增长，才能维持同样的统计显著性和精度。这导致在高维空间中进行数据建模和分析变得困难，模型容易过拟合。</p>
<p><strong>模型解释性降低</strong>：高维数据中的特征较多，模型的解释性会降低。人类难以理解和解释高维空间中的特征关系，从而影响决策和分析的透明度。</p>
<h3 id="解决办法"><a class="markdownIt-Anchor" href="#解决办法"></a> 解决办法：</h3>
<p><strong>特征选择</strong>：通过选择与目标变量高度相关的特征，去除冗余或不相关的特征，从而降低数据的维度，提高模型的性能。</p>
<p><strong>特征提取</strong>：通过技术如主成分分析（PCA）、线性判别分析（LDA）和非负矩阵分解（NMF）等方法，将原始高维数据映射到低维空间，同时尽可能保留数据的主要信息。</p>
<p><strong>稀疏表示</strong>：利用稀疏编码、L1正则化等方法，使得数据在较少的特征上具有较大的表示，从而减少模型的复杂度和过拟合风险。</p>
<p><strong>核方法</strong>：如支持向量机（SVM）的核技巧，通过在高维空间中进行操作而不显式地计算高维特征，来处理非线性问题。</p>
<p><strong>深度学习</strong>：深度神经网络，尤其是自动编码器（Autoencoder），可以有效地学习数据的低维表示，从而在高维数据处理中表现出色</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://shakewely.github.io/2024/06/07/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/" data-id="cly1c03750000wwv438464fxl" class="article-share-link">分享</a>
      
      
    </footer>
  </div>
  
</article>




  
    <article id="post-2024年4月18日-深度学习自制框架" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2024/04/18/2024%E5%B9%B44%E6%9C%8818%E6%97%A5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%88%B6%E6%A1%86%E6%9E%B6/" class="article-date">
  <time class="post-time" datetime="2024-04-18T12:15:29.000Z" itemprop="datePublished">
    <span class="post-month">4月</span><br/>
    <span class="post-day">18</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2024/04/18/2024%E5%B9%B44%E6%9C%8818%E6%97%A5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%88%B6%E6%A1%86%E6%9E%B6/">2024年4月18日 深度学习自制框架</a>
    </h1>
  

        <div>
          
          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>b</p>
<p>本书结构</p>
<img src="/2024/04/18/2024%E5%B9%B44%E6%9C%8818%E6%97%A5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%88%B6%E6%A1%86%E6%9E%B6/image-20240418201605770.png" alt="image-20240418201605770" style="zoom:67%;">
<h1 id="第一阶段-自动微分"><a class="markdownIt-Anchor" href="#第一阶段-自动微分"></a> 第一阶段 自动微分</h1>
<h2 id="1-变量"><a class="markdownIt-Anchor" href="#1-变量"></a> 1 变量</h2>
<p>pass</p>
<h2 id="2-函数"><a class="markdownIt-Anchor" href="#2-函数"></a> 2 函数</h2>
<p>pass</p>
<h2 id="3-函数连续调用"><a class="markdownIt-Anchor" href="#3-函数连续调用"></a> 3 函数连续调用</h2>
<p>pass</p>
<h2 id="4-数值微分"><a class="markdownIt-Anchor" href="#4-数值微分"></a> 4 数值微分</h2>
<h3 id="1-导数"><a class="markdownIt-Anchor" href="#1-导数"></a> 1 导数</h3>
<p>导数表示</p>
<p><img src="/2024/04/18/2024%E5%B9%B44%E6%9C%8818%E6%97%A5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%88%B6%E6%A1%86%E6%9E%B6/image-20240418235609679.png" alt="image-20240418235609679"></p>
<h3 id="2-数值微分"><a class="markdownIt-Anchor" href="#2-数值微分"></a> 2 数值微分</h3>
<p>计算机不能处理极限值 。 因此，这里的 h 表示一个近似值来计算 式 4. 1 就叫做数值微分</p>
<h4 id="21前向差分近似"><a class="markdownIt-Anchor" href="#21前向差分近似"></a> 2.1前向差分近似</h4>
<p><img src="/2024/04/18/2024%E5%B9%B44%E6%9C%8818%E6%97%A5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%88%B6%E6%A1%86%E6%9E%B6/image-20240418235758166.png" alt="image-20240418235758166"></p>
<h4 id="22中心差分近似"><a class="markdownIt-Anchor" href="#22中心差分近似"></a> 2.2中心差分近似</h4>
<p><img src="/2024/04/18/2024%E5%B9%B44%E6%9C%8818%E6%97%A5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%88%B6%E6%A1%86%E6%9E%B6/image-20240418235805977.png" alt="image-20240418235805977"></p>
<img src="/2024/04/18/2024%E5%B9%B44%E6%9C%8818%E6%97%A5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%88%B6%E6%A1%86%E6%9E%B6/image-20240418235810650.png" alt="image-20240418235810650" style="zoom:67%;">
<h2 id="5-反向传播"><a class="markdownIt-Anchor" href="#5-反向传播"></a> 5 反向传播</h2>
<p>y 对 z 的导数 11J 以用式子 5.1 表示：</p>
<p><img src="/2024/04/18/2024%E5%B9%B44%E6%9C%8818%E6%97%A5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%88%B6%E6%A1%86%E6%9E%B6/image-20240418235920234.png" alt="image-20240418235920234"></p>
<p>也可写成：</p>
<p><img src="/2024/04/18/2024%E5%B9%B44%E6%9C%8818%E6%97%A5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%88%B6%E6%A1%86%E6%9E%B6/image-20240418235944251.png" alt="image-20240418235944251"></p>
<p>求导流程</p>
<img src="/2024/04/18/2024%E5%B9%B44%E6%9C%8818%E6%97%A5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%88%B6%E6%A1%86%E6%9E%B6/image-20240419000026551.png" alt="image-20240419000026551" style="zoom:50%;">
<p>求导过程：</p>
<img src="/2024/04/18/2024%E5%B9%B44%E6%9C%8818%E6%97%A5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%88%B6%E6%A1%86%E6%9E%B6/image-20240418214342085.png" alt="image-20240418214342085" style="zoom:80%;">
<img src="/2024/04/18/2024%E5%B9%B44%E6%9C%8818%E6%97%A5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%88%B6%E6%A1%86%E6%9E%B6/image-20240418212406502.png" alt="image-20240418212406502" style="zoom:80%;">
<p><img src="/2024/04/18/2024%E5%B9%B44%E6%9C%8818%E6%97%A5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%88%B6%E6%A1%86%E6%9E%B6/image-20240418214212099.png" alt="image-20240418214212099"></p>
<p>正向传播</p>
<p><img src="/2024/04/18/2024%E5%B9%B44%E6%9C%8818%E6%97%A5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%88%B6%E6%A1%86%E6%9E%B6/image-20240418213741377.png" alt="image-20240418213741377"></p>
<p>反向传播</p>
<p><img src="/2024/04/18/2024%E5%B9%B44%E6%9C%8818%E6%97%A5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%88%B6%E6%A1%86%E6%9E%B6/image-20240418214157026.png" alt="image-20240418214157026"></p>
<p><img src="/2024/04/18/2024%E5%B9%B44%E6%9C%8818%E6%97%A5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%88%B6%E6%A1%86%E6%9E%B6/image-20240418220753163.png" alt="image-20240418220753163"></p>
<p><img src="/2024/04/18/2024%E5%B9%B44%E6%9C%8818%E6%97%A5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%88%B6%E6%A1%86%E6%9E%B6/image-20240418221125070.png" alt="image-20240418221125070"></p>
<p><img src="/2024/04/18/2024%E5%B9%B44%E6%9C%8818%E6%97%A5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%88%B6%E6%A1%86%E6%9E%B6/image-20240418221120878.png" alt="image-20240418221120878"></p>
<p><img src="/2024/04/18/2024%E5%B9%B44%E6%9C%8818%E6%97%A5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%88%B6%E6%A1%86%E6%9E%B6/image-20240418221115733.png" alt="image-20240418221115733"></p>
<p>\</p>
<p><img src="/2024/04/18/2024%E5%B9%B44%E6%9C%8818%E6%97%A5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%88%B6%E6%A1%86%E6%9E%B6/image-20240418235104955.png" alt="image-20240418235104955"></p>
<p><img src="/2024/04/18/2024%E5%B9%B44%E6%9C%8818%E6%97%A5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%88%B6%E6%A1%86%E6%9E%B6/image-20240419220559301.png" alt="image-20240419220559301"></p>
<p><img src="/2024/04/18/2024%E5%B9%B44%E6%9C%8818%E6%97%A5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%88%B6%E6%A1%86%E6%9E%B6/image-20240419224326395.png" alt="image-20240419224326395"></p>
<p><img src="/2024/04/18/2024%E5%B9%B44%E6%9C%8818%E6%97%A5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%88%B6%E6%A1%86%E6%9E%B6/image-20240419224320883.png" alt="image-20240419224320883"></p>
<p><img src="/2024/04/18/2024%E5%B9%B44%E6%9C%8818%E6%97%A5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%88%B6%E6%A1%86%E6%9E%B6/image-20240419224730866.png" alt="image-20240419224730866"></p>
<p><img src="/2024/04/18/2024%E5%B9%B44%E6%9C%8818%E6%97%A5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%88%B6%E6%A1%86%E6%9E%B6/image-20240419225947503.png" alt="image-20240419225947503"></p>
<p><img src="/2024/04/18/2024%E5%B9%B44%E6%9C%8818%E6%97%A5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%88%B6%E6%A1%86%E6%9E%B6/image-20240420153251259.png" alt="image-20240420153251259"></p>
<h1 id="2"><a class="markdownIt-Anchor" href="#2"></a> 2</h1>
<h2 id="217-内存管理和循环引用"><a class="markdownIt-Anchor" href="#217-内存管理和循环引用"></a> 2.17 内存管理和循环引用</h2>
<h3 id="1-内存管理"><a class="markdownIt-Anchor" href="#1-内存管理"></a> 1 内存管理</h3>
<ul>
<li>
<p>一种是引用计数</p>
</li>
<li>
<p>一种是分代垃圾凹收</p>
</li>
</ul>
<h3 id="2-计数方式"><a class="markdownIt-Anchor" href="#2-计数方式"></a> 2 计数方式</h3>
<p>​	每个对象在被创建时的引用计数为0，当它被另一个对象引用时引用计数加1，当引用停止时，引用计数减1。最终，当引用计数变为0时 python解释器会回收该对象。</p>
<img src="/2024/04/18/2024%E5%B9%B44%E6%9C%8818%E6%97%A5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%88%B6%E6%A1%86%E6%9E%B6/image-20240420155229195.png" alt="image-20240420155229195" style="zoom:67%;">
<p>当a = b = c = None时，对象之间的关系发生变化 此时a的引用计数变为O(b和c的引用计数为1) 对此. a立即被删除 删除 a 导致b的引用计数从1变成O. 所以b也被删除。</p>
<h3 id="3-循环引用"><a class="markdownIt-Anchor" href="#3-循环引用"></a> 3 循环引用</h3>
<img src="/2024/04/18/2024%E5%B9%B44%E6%9C%8818%E6%97%A5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%88%B6%E6%A1%86%E6%9E%B6/image-20240420155157415.png" alt="image-20240420155157415" style="zoom:67%;">
<p>采用 <code>分代垃圾回收</code>处理</p>
<h3 id="4-弱引用"><a class="markdownIt-Anchor" href="#4-弱引用"></a> 4 弱引用</h3>
<p>​	用weakref.ref函数来创建弱引用</p>
<img src="/2024/04/18/2024%E5%B9%B44%E6%9C%8818%E6%97%A5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%88%B6%E6%A1%86%E6%9E%B6/image-20240420155447235.png" alt="image-20240420155447235" style="zoom:67%;">
<img src="/2024/04/18/2024%E5%B9%B44%E6%9C%8818%E6%97%A5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%88%B6%E6%A1%86%E6%9E%B6/image-20240420155452283.png" alt="image-20240420155452283" style="zoom:67%;">
<p>a=None 时，b虽然用了这个对象，但由于是弱引用，所以对引用计数没有影响</p>
<h3 id="5修改"><a class="markdownIt-Anchor" href="#5修改"></a> 5修改</h3>
<p><strong>对比：</strong></p>
<p>之前：</p>
<p><img src="/2024/04/18/2024%E5%B9%B44%E6%9C%8818%E6%97%A5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%88%B6%E6%A1%86%E6%9E%B6/image-20240420160208866.png" alt="image-20240420160208866"></p>
<p>之后：</p>
<img src="/2024/04/18/2024%E5%B9%B44%E6%9C%8818%E6%97%A5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%88%B6%E6%A1%86%E6%9E%B6/image-20240420160154381.png" alt="image-20240420160154381" style="zoom:67%;">
<h2 id="218减少内存使用量的模式"><a class="markdownIt-Anchor" href="#218减少内存使用量的模式"></a> 2.18减少内存使用量的模式</h2>
<p>第1项改进是减少反向传播消耗的内存使用址， 这项改进提供了立即清除元用导数的机制。</p>
<p><img src="/2024/04/18/2024%E5%B9%B44%E6%9C%8818%E6%97%A5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%88%B6%E6%A1%86%E6%9E%B6/image-20240420160709236.png" alt="image-20240420160709236"></p>
<p><img src="/2024/04/18/2024%E5%B9%B44%E6%9C%8818%E6%97%A5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%88%B6%E6%A1%86%E6%9E%B6/image-20240420161151386.png" alt="image-20240420161151386"></p>
<p>第2项改进是提供&quot;不需要反向 传播时的模式&quot;</p>
<p><img src="/2024/04/18/2024%E5%B9%B44%E6%9C%8818%E6%97%A5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%88%B6%E6%A1%86%E6%9E%B6/image-20240420161534975.png" alt="image-20240420161534975"></p>
<p><img src="/2024/04/18/2024%E5%B9%B44%E6%9C%8818%E6%97%A5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%88%B6%E6%A1%86%E6%9E%B6/image-20240420161630351.png" alt="image-20240420161630351"></p>
<p>是否 <code>creator</code>也不需要了呢？</p>
<p>Constant</p>
<p><img src="/2024/04/18/2024%E5%B9%B44%E6%9C%8818%E6%97%A5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%88%B6%E6%A1%86%E6%9E%B6/image-20240420154131602.png" alt="image-20240420154131602"></p>
<p><img src="/2024/04/18/2024%E5%B9%B44%E6%9C%8818%E6%97%A5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%88%B6%E6%A1%86%E6%9E%B6/image-20240420154150170.png" alt="image-20240420154150170"></p>
<p><img src="/2024/04/18/2024%E5%B9%B44%E6%9C%8818%E6%97%A5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%88%B6%E6%A1%86%E6%9E%B6/image-20240420154244033.png" alt="image-20240420154244033"></p>
<p><img src="/2024/04/18/2024%E5%B9%B44%E6%9C%8818%E6%97%A5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%88%B6%E6%A1%86%E6%9E%B6/image-20240420154246451.png" alt="image-20240420154246451"></p>
<p><img src="/2024/04/18/2024%E5%B9%B44%E6%9C%8818%E6%97%A5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%88%B6%E6%A1%86%E6%9E%B6/image-20240420154312677.png" alt="image-20240420154312677"></p>
<p><img src="/2024/04/18/2024%E5%B9%B44%E6%9C%8818%E6%97%A5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%88%B6%E6%A1%86%E6%9E%B6/image-20240420154416108.png" alt="image-20240420154416108"></p>
<p><img src="/2024/04/18/2024%E5%B9%B44%E6%9C%8818%E6%97%A5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%88%B6%E6%A1%86%E6%9E%B6/image-20240420154432668.png" alt="image-20240420154432668"></p>
<h1 id="3-高阶导数"><a class="markdownIt-Anchor" href="#3-高阶导数"></a> 3 高阶导数</h1>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://shakewely.github.io/2024/04/18/2024%E5%B9%B44%E6%9C%8818%E6%97%A5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%88%B6%E6%A1%86%E6%9E%B6/" data-id="clx4fkxnj000dn0v43cmx8t28" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/project/" rel="tag">project</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-2024年3月29日-llm2" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2024/03/29/2024%E5%B9%B43%E6%9C%8829%E6%97%A5-llm2/" class="article-date">
  <time class="post-time" datetime="2024-03-29T04:40:45.000Z" itemprop="datePublished">
    <span class="post-month">3月</span><br/>
    <span class="post-day">29</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2024/03/29/2024%E5%B9%B43%E6%9C%8829%E6%97%A5-llm2/">2024年3月29日 llm2</a>
    </h1>
  

        <div>
          
          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="2-llm2"><a class="markdownIt-Anchor" href="#2-llm2"></a> 2 LLM2</h1>
<h2 id="21处理流程"><a class="markdownIt-Anchor" href="#21处理流程"></a> 2.1处理流程</h2>
<h3 id="输入数据"><a class="markdownIt-Anchor" href="#输入数据"></a> <strong>输入数据</strong>：</h3>
<p>LLM的输入数据是一段文本，可以是一个句子或一段话。文本通常被表示成单词或字符的序列。</p>
<h3 id="tokenization"><a class="markdownIt-Anchor" href="#tokenization"></a> <strong>Tokenization</strong>：</h3>
<p>将文本进行Tokenization，将其切分成单词或字符，形成Token序列。</p>
<p>再将文本映射成模型可理解的输入形式，将文本序列转换为整数索引序列</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">序列化-&gt; </span><br><span class="line">[&#x27;BOS&#x27;,&#x27;君&#x27;,&#x27;不&#x27;,&#x27;见&#x27;,&#x27;黄&#x27;,&#x27;河&#x27;,&#x27;之&#x27;,&#x27;水&#x27;,&#x27;天&#x27;,&#x27;上&#x27;,&#x27;来&#x27;,&#x27;，&#x27; ,&#x27;奔&#x27;,&#x27;流&#x27;,&#x27;到&#x27;...&#x27;与&#x27;,&#x27;尔&#x27;,&#x27;同&#x27;,&#x27;销&#x27;,&#x27;万&#x27;,&#x27;古&#x27;,&#x27;愁&#x27;,&#x27;EOS&#x27;]</span><br><span class="line"></span><br><span class="line">假设语料库索引化-&gt;</span><br><span class="line">[&#x27;BOS&#x27;,&#x27;10&#x27;,&#x27;3&#x27;,&#x27;67&#x27;,&#x27;89&#x27;,&#x27;21&#x27;,&#x27;45&#x27;,&#x27;55&#x27;,&#x27;61&#x27;,&#x27;4&#x27;,&#x27;324&#x27;,&#x27;565&#x27; ,&#x27;789&#x27;,&#x27;6567&#x27;,&#x27;786&#x27;...&#x27;7869&#x27;,&#x27;9&#x27;,&#x27;3452&#x27;,&#x27;563&#x27;,&#x27;56&#x27;,&#x27;66&#x27;,&#x27;77&#x27;,&#x27;EOS&#x27;]</span><br></pre></td></tr></table></figure>
<h3 id="embedding"><a class="markdownIt-Anchor" href="#embedding"></a> <strong>Embedding</strong>：</h3>
<p>将每个Token映射为一个实数向量，为Embeding Vector</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#x27;BOS&#x27;-&gt; [p_&#123;00&#125;,p_&#123;01&#125;,p_&#123;02&#125;,...,p_&#123;0d-1&#125;]</span><br><span class="line">&#x27;10&#x27; -&gt; [p_&#123;10&#125;,p_&#123;11&#125;,p_&#123;12&#125;,...,p_&#123;1d-1&#125;]</span><br><span class="line">&#x27;3&#x27;  -&gt; [p_&#123;20&#125;,p_&#123;21&#125;,p_&#123;22&#125;,...,p_&#123;2d-1&#125;]</span><br><span class="line">...</span><br><span class="line">&#x27;EOS&#x27;-&gt; [p_&#123;n0&#125;,p_&#123;n1&#125;,p_&#123;n2&#125;,...,p_&#123;nd-1&#125;]</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="位置编码"><a class="markdownIt-Anchor" href="#位置编码"></a> <strong>位置编码</strong>：</h3>
<p>对于Token序列中的每个位置，添加位置编码（Positional Encoding）向量，以提供关于Token在序列中位置的信息。</p>
<p>位置编码是为了区分不同位置的Token，并为模型提供上下文关系的信息。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[p_&#123;00&#125;,p_&#123;01&#125;,p_&#123;02&#125;,...,p_&#123;0d-1&#125;]       [pe_&#123;00&#125;,pe_&#123;01&#125;,pe_&#123;02&#125;,...,pe_&#123;0d-1&#125;]</span><br><span class="line">[p_&#123;10&#125;,p_&#123;11&#125;,p_&#123;12&#125;,...,p_&#123;1d-1&#125;]       [pe_&#123;10&#125;,pe_&#123;11&#125;,pe_&#123;12&#125;,...,pe_&#123;1d-1&#125;]</span><br><span class="line">[p_&#123;20&#125;,p_&#123;21&#125;,p_&#123;22&#125;,...,p_&#123;2d-1&#125;]    +  [pe_&#123;20&#125;,pe_&#123;21&#125;,pe_&#123;22&#125;,...,pe_&#123;2d-1&#125;]</span><br><span class="line">...                                       ...  </span><br><span class="line">[p_&#123;n0&#125;,p_&#123;n1&#125;,p_&#123;n2&#125;,...,p_&#123;nd-1&#125;]       [pe_&#123;n0&#125;,pe_&#123;n1&#125;,pe_&#123;n2&#125; ,...,pe_&#123;nd-1&#125;]</span><br></pre></td></tr></table></figure>
<h3 id="transformer"><a class="markdownIt-Anchor" href="#transformer"></a> <strong>Transformer</strong> ：</h3>
<p>在生成任务中，模型只需要用到Transformer 的decoder阶段，即Decoder-Only，比如GPT、LLaMA 都是。</p>
<p><img src="/2024/03/29/2024%E5%B9%B43%E6%9C%8829%E6%97%A5-llm2/image-20240329125213908.png" alt="image-20240329125213908"></p>
<h3 id="自回归生成"><a class="markdownIt-Anchor" href="#自回归生成"></a> <strong>自回归生成</strong>：</h3>
<p>在生成任务中，使用自回归（Autoregressive）方式，逐个生成输出序列中的每个Token。</p>
<p>在解码过程中，每次生成一个Token时，使用前面已生成的内容作为上下文，来帮助预测下一个Token。</p>
<p><img src="/2024/03/29/2024%E5%B9%B43%E6%9C%8829%E6%97%A5-llm2/image-20240329125500103.png" alt="image-20240329125500103"></p>
<h2 id="22-相关技术"><a class="markdownIt-Anchor" href="#22-相关技术"></a> 2.2 相关技术</h2>
<h3 id="rope"><a class="markdownIt-Anchor" href="#rope"></a> <strong>RoPE</strong></h3>
<p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2104.09864.pdf">RoPE位置编码</a></p>
<p><img src="/2024/03/29/2024%E5%B9%B43%E6%9C%8829%E6%97%A5-llm2/image-20240329131119065.png" alt="image-20240329131119065"></p>
<h3 id="kv-cache-gqa"><a class="markdownIt-Anchor" href="#kv-cache-gqa"></a> <strong>KV Cache &amp; GQA</strong></h3>
<p>Attention计算时的KV</p>
<p>通过将每次计算的K和V缓存下来，之后新的序列进来时只需要从KV Cache中读取之前的KV值即可，就不需要再去重复计算之前的KV了。</p>
<p><img src="/2024/03/29/2024%E5%B9%B43%E6%9C%8829%E6%97%A5-llm2/image-20240329131434662.png" alt="image-20240329131434662"></p>
<p>至于为什么不用缓存Q？</p>
<p><strong>GQA</strong></p>
<p><img src="/2024/03/29/2024%E5%B9%B43%E6%9C%8829%E6%97%A5-llm2/image-20240329131658979.png" alt="image-20240329131658979"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://shakewely.github.io/2024/03/29/2024%E5%B9%B43%E6%9C%8829%E6%97%A5-llm2/" data-id="clx4fkxni0007n0v4cpjj3kck" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/llm/" rel="tag">llm</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-2024年3月3日-pythons刷题" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2024/03/13/2024%E5%B9%B43%E6%9C%883%E6%97%A5-pythons%E5%88%B7%E9%A2%98/" class="article-date">
  <time class="post-time" datetime="2024-03-13T12:51:35.000Z" itemprop="datePublished">
    <span class="post-month">3月</span><br/>
    <span class="post-day">13</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2024/03/13/2024%E5%B9%B43%E6%9C%883%E6%97%A5-pythons%E5%88%B7%E9%A2%98/">2024年3月3日 python内存管理</a>
    </h1>
  

        <div>
          
          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="python-内存管理"><a class="markdownIt-Anchor" href="#python-内存管理"></a> python 内存管理</h1>
<h2 id="1-内存管理"><a class="markdownIt-Anchor" href="#1-内存管理"></a> 1 内存管理</h2>
<h2 id="2-记数引用"><a class="markdownIt-Anchor" href="#2-记数引用"></a> 2 记数引用</h2>
<h2 id="3-弱引用"><a class="markdownIt-Anchor" href="#3-弱引用"></a> 3 弱引用</h2>
<p>在不增加引用计数的情况下引用另一个对象的功能</p>
<img src="/2024/03/13/2024%E5%B9%B43%E6%9C%883%E6%97%A5-pythons%E5%88%B7%E9%A2%98/image-20240419235445781.png" alt="image-20240419235445781" style="zoom:67%;">
<img src="/2024/03/13/2024%E5%B9%B43%E6%9C%883%E6%97%A5-pythons%E5%88%B7%E9%A2%98/image-20240419235504536.png" alt="image-20240419235504536" style="zoom:67%;">
<p><img src="/2024/03/13/2024%E5%B9%B43%E6%9C%883%E6%97%A5-pythons%E5%88%B7%E9%A2%98/image-20240419235820695.png" alt="image-20240419235820695"></p>
<h2 id="4-memory-profiler"><a class="markdownIt-Anchor" href="#4-memory-profiler"></a> 4 memory profiler</h2>
<p>外部库来监测 Python 中的内存使用情况</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://shakewely.github.io/2024/03/13/2024%E5%B9%B43%E6%9C%883%E6%97%A5-pythons%E5%88%B7%E9%A2%98/" data-id="clx4fkxni0009n0v46qrdakx6" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/" rel="tag">python</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-2024年3月3日-面试更新——代码模型" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2024/03/03/2024%E5%B9%B43%E6%9C%883%E6%97%A5-%E9%9D%A2%E8%AF%95%E6%9B%B4%E6%96%B0%E2%80%94%E2%80%94%E4%BB%A3%E7%A0%81%E6%A8%A1%E5%9E%8B/" class="article-date">
  <time class="post-time" datetime="2024-03-03T06:16:37.000Z" itemprop="datePublished">
    <span class="post-month">3月</span><br/>
    <span class="post-day">03</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2024/03/03/2024%E5%B9%B43%E6%9C%883%E6%97%A5-%E9%9D%A2%E8%AF%95%E6%9B%B4%E6%96%B0%E2%80%94%E2%80%94%E4%BB%A3%E7%A0%81%E6%A8%A1%E5%9E%8B/">2024年3月3日 面试更新——代码模型</a>
    </h1>
  

        <div>
          
          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="代码模型"><a class="markdownIt-Anchor" href="#代码模型"></a> 代码模型</h1>
<h2 id="模型加密"><a class="markdownIt-Anchor" href="#模型加密"></a> 模型加密</h2>
<p>模型加密的几个方式：</p>
<ul>
<li>模型文件直接加密，把模型的二进制文件直接加密，一般都会有加密算法，对模型的前xx字节进行加密，解密的时候对前xx字节进行解密即可，这种加密只在load时候做，会影响模型加载速度，但是不影响运行速度</li>
<li>模型结构不加密，模型权重加密，有的可以采用类似于模型压缩的方法把权重压缩了，推理时以自定义协议读取加载推理</li>
<li>也需要对解密模型的代码进行加密，代码混淆要做，不过这样会影响解密代码也就是load模型过程，会变慢，就看你代码混淆做到什么程度</li>
</ul>
<h2 id="模型转换"><a class="markdownIt-Anchor" href="#模型转换"></a> 模型转换</h2>
<p>模型可能是各种格式的：</p>
<ul>
<li>Caffe</li>
<li>ONNX</li>
<li>Pytorch</li>
<li>TFLITE</li>
<li>NCNN、MNN</li>
</ul>
<h4 id="模型转换后一般要做"><a class="markdownIt-Anchor" href="#模型转换后一般要做"></a> 模型转换后一般要做</h4>
<ul>
<li>转换后看一下模型的输入输出类型、维度、名称、数量啥的是否和之前一致</li>
<li>转换后首先跑一张训练时的图（或者一批图），看下新模型的输出和旧模型差多少，一定要保证最终输入到模型的tensor一致（也可以使用random输入或者ones输入测试，不过对于模型权重分布特殊的模型来说，对于这种输入可能评测不是很准确）</li>
<li>批量跑测试集测一下精度是否一致</li>
<li>benchmark转换后模型的速度是否符合预期</li>
</ul>
<h4 id="常见的问题"><a class="markdownIt-Anchor" href="#常见的问题"></a> 常见的问题</h4>
<ul>
<li>转换后模型精度问题，输出为nan、精度完全错乱</li>
<li>转换后模型batch=1正常，多batch结果错乱</li>
<li>转换后输入/输出类型维度啥的错误</li>
<li>转换后模型速度未达到预期</li>
<li>未完待续</li>
</ul>
<h2 id="模型优化"><a class="markdownIt-Anchor" href="#模型优化"></a> 模型优化</h2>
<p>小到优化一个<a target="_blank" rel="noopener" href="https://blog.csdn.net/agq358/article/details/125095432"><strong>op</strong></a>（Op就是Kernel的集合，一个Op代表的是有一定共性的多个Kernel便于在<a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/2306404">计算图和图优化</a>中进行表示），大概优化整个推理pipeline，可以干的事情很多。</p>
<h3 id="合并-替换op算子"><a class="markdownIt-Anchor" href="#合并-替换op算子"></a> 合并、替换op算子</h3>
<p>很多框架在导出的时候就会<strong>自动合并一些操作</strong>，比如torch.onnx在导出<strong>conv+bn</strong>的时候会将bn吸到前面的conv中，比较常见了。<br>
但也有一些<strong>可以合并的操作，<strong>假如框架代码还没有实现该pattern则不会合并，不过我们也可以自己合并，这里需要经验了，需要我们熟知各种op的合并方式。<br>
可以自己合并一些操作，比如下图中的</strong>convTranspose+Add+BN</strong>，是不常见的Squence(序列)的pattern，如果自己愿意的话，可以直接在ONNX中进行合并，把权重关系搞对就行</p>
<p><img src="/2024/03/03/2024%E5%B9%B43%E6%9C%883%E6%97%A5-%E9%9D%A2%E8%AF%95%E6%9B%B4%E6%96%B0%E2%80%94%E2%80%94%E4%BB%A3%E7%A0%81%E6%A8%A1%E5%9E%8B/image-20240303145240993.png" alt="image-20240303145240993"></p>
<p>列举一些合并的例子，总之就是将多个op合成大op，节省计算量以及数据搬运的时间：</p>
<ul>
<li>conv/transposeConv+bn</li>
<li>多个支路的conv合并为group conv</li>
<li>gemm + bias -&gt; conv</li>
</ul>
<p><img src="/2024/03/03/2024%E5%B9%B43%E6%9C%883%E6%97%A5-%E9%9D%A2%E8%AF%95%E6%9B%B4%E6%96%B0%E2%80%94%E2%80%94%E4%BB%A3%E7%A0%81%E6%A8%A1%E5%9E%8B/image-20240303145305875.png" alt="image-20240303145305875"></p>
<p>多路合并</p>
<p><img src="/2024/03/03/2024%E5%B9%B43%E6%9C%883%E6%97%A5-%E9%9D%A2%E8%AF%95%E6%9B%B4%E6%96%B0%E2%80%94%E2%80%94%E4%BB%A3%E7%A0%81%E6%A8%A1%E5%9E%8B/image-20240303145254628.png" alt="image-20240303145254628"></p>
<p>既可以融合一些算子，当然也可以替换一些算子：</p>
<ul>
<li>relu6替换为max(0,6)</li>
</ul>
<h3 id="蒸馏-剪枝"><a class="markdownIt-Anchor" href="#蒸馏-剪枝"></a> 蒸馏、剪枝</h3>
<p>剪枝后的模型比未剪枝的同等size大小精度更高。<br>
具体的可以参考这篇：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1710.01878">https://arxiv.org/abs/1710.01878</a></p>
<p>剪枝的方法可以看zomi总结的ppt(来自 <a target="_blank" rel="noopener" href="https://github.com/chenzomi12/DeepLearningSystem">https://github.com/chenzomi12/DeepLearningSystem</a>)：</p>
<p><img src="/2024/03/03/2024%E5%B9%B43%E6%9C%883%E6%97%A5-%E9%9D%A2%E8%AF%95%E6%9B%B4%E6%96%B0%E2%80%94%E2%80%94%E4%BB%A3%E7%A0%81%E6%A8%A1%E5%9E%8B/image-20240303145428861.png" alt="image-20240303145428861"></p>
<p>​</p>
<p>蒸馏可以使同结构的小模型精度提升接近大模型的精度。</p>
<h4 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结</h4>
<p>剪枝类似于模型搜索，如果直接<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/127194745">NAS</a>的话，就没有必要剪枝了。</p>
<h4 id="参考"><a class="markdownIt-Anchor" href="#参考"></a> 参考</h4>
<ul>
<li>to prune or not to prune exploring the efficacy of pruning for model compression</li>
<li>DAMO-YOLO</li>
</ul>
<h3 id="量化"><a class="markdownIt-Anchor" href="#量化"></a> 量化</h3>
<p>量化目前比较成熟，也有很好的教程（PPQ），很成熟的库（PPQ）</p>
<ul>
<li>量化基本概念，与硬件的关系</li>
<li>PTQ量化和QAT量化，两种量化还有很多方法。PTQ有  EasyQuant（EQ）、；QAT有LSQ(Learned Step Size Quantization)、DSQ(Differentiable Soft Quantization)</li>
</ul>
<p>不是所有模型、所有op都适合量化：</p>
<ul>
<li>重参数量化 <a target="_blank" rel="noopener" href="https://tech.meituan.com/2022/09/22/yolov6-quantization-in-meituan.html">https://tech.meituan.com/2022/09/22/yolov6-quantization-in-meituan.html</a></li>
<li>量化模型直接输入int8 进行测试</li>
</ul>
<p>注意点：</p>
<ul>
<li>有些模型量化后，虽然整体指标没有变化（某个评价标准，比如coco的mAP），但是实际使用中，发现之前的一些效果变差了，这种情况大多是调用模型的策略效果与这个模型的耦合度比较高了。举个例子，比如之前这个模型对小目标检测效果好，但是量化后，小目标检测效果差了（然而中目标效果好了）所以导致与小目标耦合度比较高的策略兼容度不高，导致算法的整体精度下降。这种情况就比较尴尬，你可以调整策略，或者重新量化模型，加上一些约束使其在某些场景下尽可能和原始模型表现一致，但这个需要时间去优化了，有较高的时间成本。</li>
</ul>
<h4 id="参考-2"><a class="markdownIt-Anchor" href="#参考-2"></a> 参考</h4>
<ul>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=Mzg3ODU2MzY5MA==&amp;mid=2247488318&amp;idx=1&amp;sn=048c1b78f3b2cb25c05abb115f20d6c6&amp;chksm=cf108b3bf867022d1b214928102d65ed691c81955b59ca02bccdee92584ad9aa8e390e1d2978&amp;token=1097456929&amp;lang=zh_CN&amp;scene=21#wechat_redirect">必看部署系列~懂你的神经网络量化教程：第一讲！</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=Mzg3ODU2MzY5MA==&amp;mid=2247488838&amp;idx=1&amp;sn=56107c468d5b683a574e6046af3a541f&amp;chksm=cf108d43f8670455736a83546eb5ed81abc9194d7d4c2359af393f26e3bddd1379f777e35f35&amp;token=1097456929&amp;lang=zh_CN&amp;scene=21#wechat_redirect">量化番外篇——TensorRT-8的量化细节</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=Mzg3ODU2MzY5MA==&amp;mid=2247489317&amp;idx=1&amp;sn=797e32276bd4f55948d992f455415943&amp;chksm=cf108f20f8670636b27be2431d5a4fdef1689eaa672e59ffc7d6181d2afb13a9a050d9b6b4a5&amp;token=1097456929&amp;lang=zh_CN&amp;scene=21#wechat_redirect">实践torch.fx第二篇——基于FX的PTQ量化实操</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/openppl-public/ppq">https://github.com/openppl-public/ppq</a></li>
</ul>
<h3 id="算子op优化"><a class="markdownIt-Anchor" href="#算子op优化"></a> 算子op优化</h3>
<p>性能计算的一些入门问题，可以看下这个的回答：</p>
<ul>
<li>想进大厂的高性能计算岗位需要做哪些准备？</li>
</ul>
<p>大部分优化op的场景，很多原因是<strong>原生op的实现可能不是最优</strong>的，有很大的优化空间。<br>
比如LayerNorm这个操作，Pytorch原生的实现比较慢，于是就有了优化空间：</p>
<ul>
<li>CUDA优化之LayerNorm性能优化实践</li>
</ul>
<p>同理，很多CUDA实现的OP可能不是最优的，只有你有精力，就可以进行优化。也要考虑是这个优化值不值，在整个模型中的占比大不大，投入产出比怎么样blabla。</p>
<p>对于CUDA还好些，资料很多，很多op网上都有开源的不错的实现（尤其是gemm），抄抄改改就可以了。</p>
<p>不过对于一些没有CUDA那么火的平台或者语言，比如arm平台的neon或者npu，这些开源的算子实现少一些，大部分需要自己手写。分析模型哪些op慢之后，手动优化一下。</p>
<p>知乎上也有很多优化系列的教程，跟着一步一步来吧：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/478846788">cuda 入门的正确姿势：how-to-optimize-gemm</a></li>
<li>深入浅出GPU优化系列：elementwise优化及CUDA工具链介绍</li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/441146275">[施工中] CUDA GEMM 理论性能分析与 kernel 优化</a></li>
<li>深入浅出GPU优化系列：reduce优化</li>
</ul>
<p>有几种可以自动生成op的框架：</p>
<ul>
<li>TVM</li>
<li>triton（此triton非彼triton）</li>
</ul>
<h3 id="调优可视化工具"><a class="markdownIt-Anchor" href="#调优可视化工具"></a> 调优可视化工具</h3>
<p>可视化工具</p>
<ul>
<li>画模型图的工具，graphvis</li>
<li>NVIDIA的nsight system和nsight compute</li>
<li>pytorch的profiler</li>
</ul>
<h2 id="更多推理框架ai编译器"><a class="markdownIt-Anchor" href="#更多推理框架ai编译器"></a> 更多推理框架/AI编译器</h2>
<p><a target="_blank" rel="noopener" href="https://github.com/merrymercy/awesome-tensor-compilers">https://github.com/merrymercy/awesome-tensor-compilers</a></p>
<h3 id="onnx"><a class="markdownIt-Anchor" href="#onnx"></a> ONNX</h3>
<h4 id="相关文章"><a class="markdownIt-Anchor" href="#相关文章"></a> 相关文章：</h4>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/daquexian/onnx-simplifier">https://github.com/daquexian/onnx-simplifier</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/ZhangGe6/onnx-modifier">https://github.com/ZhangGe6/onnx-modifier</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/tree/master/tools/onnx-graphsurgeon">https://github.com/NVIDIA/TensorRT/tree/master/tools/onnx-graphsurgeon</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/onnx/onnx">https://github.com/onnx/onnx</a></li>
<li>模型部署入门教程（五）：ONNX 模型的修改与调试</li>
<li><a target="_blank" rel="noopener" href="https://github.com/onnx/onnx/blob/main/docs/Operators.md">https://github.com/onnx/onnx/blob/main/docs/Operators.md</a></li>
</ul>
<h3 id="tvm"><a class="markdownIt-Anchor" href="#tvm"></a> TVM</h3>
<ul>
<li>如何评测模型的速度</li>
<li>如何benchmark模型各层的错误</li>
<li>解析trtexec中的benchmark</li>
<li>解析TVM中的graph和VM</li>
</ul>
<h3 id="tensorrt"><a class="markdownIt-Anchor" href="#tensorrt"></a> TensorRT</h3>
<ul>
<li>python端口调用</li>
<li>C++端口调用</li>
<li>多线程调用</li>
<li>各种模型转换TensorRT（ONNX、Pytorch、TensorFLow）</li>
<li>各种和TensorRT相关的转换库</li>
</ul>
<h4 id="自定义插件"><a class="markdownIt-Anchor" href="#自定义插件"></a> 自定义插件</h4>
<p>如果模型中保存TensorRT不支持的算子，就需要自己实现cuda操作并且集成到TensorRT中。<br>
现在也有很多可以生成插件的工具：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA-AI-IOT/tensorrt_plugin_generator">https://github.com/NVIDIA-AI-IOT/tensorrt_plugin_generator</a></li>
</ul>
<p>相关资料：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/trt-samples-for-hackathon-cn">https://github.com/NVIDIA/trt-samples-for-hackathon-cn</a></li>
</ul>
<h3 id="libtorch"><a class="markdownIt-Anchor" href="#libtorch"></a> libtorch</h3>
<p>简单好用的对Pytorch模型友好的C++推理库。</p>
<ul>
<li>torch.jit.trace</li>
<li>torch.jit.script</li>
<li>python导出libtorch模型，C++加载libtorch模型</li>
</ul>
<h3 id="aitemplate"><a class="markdownIt-Anchor" href="#aitemplate"></a> AITemplate</h3>
<p>AITemplate 加速Stable Diffusion的效果比TensorRT要好不少。<br>
测试了一个res50的模型，利用<code>TensorRT-8.5.1.7</code>和<code>AITemplate-0.1dev</code>转化后简单测试了下速度，精度都是FP16，显卡是A4000。</p>
<table>
<thead>
<tr>
<th style="text-align:left"></th>
<th style="text-align:left">4x3x224x224</th>
<th style="text-align:left">8x3x224x224</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">TensorRT</td>
<td style="text-align:left">2.07885 ms</td>
<td style="text-align:left">3.49877 ms</td>
</tr>
<tr>
<td style="text-align:left">AITemplate</td>
<td style="text-align:left">1.36401 ms</td>
<td style="text-align:left">2.38946 ms</td>
</tr>
</tbody>
</table>
<h2 id="高性能计算"><a class="markdownIt-Anchor" href="#高性能计算"></a> 高性能计算</h2>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://shakewely.github.io/2024/03/03/2024%E5%B9%B43%E6%9C%883%E6%97%A5-%E9%9D%A2%E8%AF%95%E6%9B%B4%E6%96%B0%E2%80%94%E2%80%94%E4%BB%A3%E7%A0%81%E6%A8%A1%E5%9E%8B/" data-id="cltgreey5000rw8v4e1zv3zw6" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E9%9D%A2%E8%AF%95/" rel="tag">面试</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-2024年3月3日-面试更新" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2024/03/03/2024%E5%B9%B43%E6%9C%883%E6%97%A5-%E9%9D%A2%E8%AF%95%E6%9B%B4%E6%96%B0/" class="article-date">
  <time class="post-time" datetime="2024-03-03T04:42:53.000Z" itemprop="datePublished">
    <span class="post-month">3月</span><br/>
    <span class="post-day">03</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2024/03/03/2024%E5%B9%B43%E6%9C%883%E6%97%A5-%E9%9D%A2%E8%AF%95%E6%9B%B4%E6%96%B0/">2024年3月3日 面试更新——大模型</a>
    </h1>
  

        <div>
          
          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="大模型面试总结"><a class="markdownIt-Anchor" href="#大模型面试总结"></a> 大模型面试总结</h1>
<h2 id="大模型"><a class="markdownIt-Anchor" href="#大模型"></a> 大模型</h2>
<h3 id="项目"><a class="markdownIt-Anchor" href="#项目"></a> 项目</h3>
<p>现在简历上搞一些什么使用unet训练一个分割网络实现某个任务，或者说使用yolov7检测某个目标已经不是什么亮点了。不过这种也不是不行，但你需要<strong>更多的深度</strong>我才会感兴趣：</p>
<ul>
<li>网络结构有无值得说明的改进</li>
<li>为什么这样做可以明确说出原因和数据证明</li>
<li>对使用这个方法以及和其他方法做过比较详细的对比，选择这个模型是有理由的</li>
</ul>
<h3 id="推理相关"><a class="markdownIt-Anchor" href="#推理相关"></a> 推理相关</h3>
<p>要求：</p>
<ul>
<li>搞上层编译器的（类似于torch-tensorrt的利用pytorch生态和TensorRT生态的在nvidia显卡加速的编译器，不需要自己写codegen），会针对不同的后端（比如onnx和torchscript）写parser，针对计算图写一些pass；也有搞基于MLIR的编译器的，在自己的公司硬件上跑，前端中端后端需要都搞</li>
<li>搞推理框架的，就是优化训练和部署中的一些性能问题、精度溢出问题；有些公司喜欢搞统一的框架（训练和部署都解决了），不喜欢用现有的轮子，要自己造；对于加速类的推理框架，会实现比如模拟量化功能、精度对比功能等等</li>
<li>搞加速的，就是对任务中各种瓶颈的算子进行加速，C<ins>转cuda，python转c</ins>等等，使用C++封装一些项目blabla</li>
<li>项目优化op的细节</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://shakewely.github.io/2024/03/03/2024%E5%B9%B43%E6%9C%883%E6%97%A5-%E9%9D%A2%E8%AF%95%E6%9B%B4%E6%96%B0/" data-id="cltgreey3000lw8v49zyk0xav" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E9%9D%A2%E8%AF%95/" rel="tag">面试</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-2024年2月12日-转载文章" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2024/02/12/2024%E5%B9%B42%E6%9C%8812%E6%97%A5-%E8%BD%AC%E8%BD%BD%E6%96%87%E7%AB%A0/" class="article-date">
  <time class="post-time" datetime="2024-02-12T14:16:29.000Z" itemprop="datePublished">
    <span class="post-month">2月</span><br/>
    <span class="post-day">12</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2024/02/12/2024%E5%B9%B42%E6%9C%8812%E6%97%A5-%E8%BD%AC%E8%BD%BD%E6%96%87%E7%AB%A0/">2024年2月12日 转载文章</a>
    </h1>
  

        <div>
          
          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="转载ai文章"><a class="markdownIt-Anchor" href="#转载ai文章"></a> 转载AI文章</h1>
<h2 id="python"><a class="markdownIt-Anchor" href="#python"></a> python</h2>
<h3 id="100-个-python-练手项目"><a class="markdownIt-Anchor" href="#100-个-python-练手项目"></a> <strong>100 个 Python 练手项目</strong></h3>
<p>1、【Python 图片转字符画】</p>
<p>2、【200行Python代码实现2048】</p>
<p>3、【Python3 实现火车票查询工具】</p>
<p>4、【高德API+Python解决租房问题】</p>
<p>5、【Python3色情图片识别】</p>
<p>6、【Python 破解验证码】</p>
<p>7、【Python实现简单的Web服务器】</p>
<p>8、【pygame开发打飞机游戏】</p>
<p>9、【Django 搭建简易博客】</p>
<p>10、【Python基于共现提取《釜山行》人物关系】</p>
<p>11、【基于scrapy爬虫的天气数据采集（python）】</p>
<p>12、【Flask 开发轻博客】</p>
<p>13、【Python3图片隐写术】</p>
<p>14、【Python 实现简易Shell】</p>
<p>15、【使用Python解数学方程】</p>
<p>16、【PyQt 实现简易浏览器】</p>
<p>17、【神经网络实现手写字符识别系统】</p>
<p>18、【Python 实现简单画板】</p>
<p>19、【Python实现3D建模工具】</p>
<p>20、【NBA常规寒结果预测——利用Python进行比赛数据分析】</p>
<p>21、【神经网络实现人脸识别任务】</p>
<p>22、【Python文本解析器】</p>
<p>23、【Python3&amp;；amp；OpenCV 视频转字符动画】</p>
<p>24、【Python3 实现淘女郎照片爬虫】</p>
<p>25、【Python3实现简单的FTP认证服务器】</p>
<p>26、【基于Flask 与MySQL 实现番剧推荐系统】</p>
<p>27、【Python 实现端口扫描器】</p>
<p>28、【使用Python3编写系列实用脚本】</p>
<p>29、【Python 实现康威生命游戏】</p>
<p>30、【川普撞脸希拉里（基于OpenCV的面部特征交换）】</p>
<p>31、【Python 3实现Markdown 解析器】</p>
<p>32、【Python气象数据分析–《Python 数据分析实战》】</p>
<p>33、【Python实现键值数据库】</p>
<p>34、【k-近邻算法实现手写数字识别系统】</p>
<p>35、【ebay在线拍卖数据分析】</p>
<p>36、【Python 实现英文新闻摘要自动提取】</p>
<p>37、【Python实现简易局域网视频聊天工具】</p>
<p>38、【基于Flask及爬虫实现微信娱乐机器人】</p>
<p>39、【Python实现Python解释器】</p>
<p>40、【Python3基于Scapy实现DDos】</p>
<p>41、【Python 实现密码强度检测器】</p>
<p>42、【使用Python 实现深度神经网络】</p>
<p>43、【Python实现从excel读取数据并绘制成精美图像】</p>
<p>44、【人机对战初体验：Python基于Pygame实现四子棋游戏】</p>
<p>45、【Python3实现可控制肉鸡的反向Shell】</p>
<p>46、【Python打造漏洞扫描器】</p>
<p>47、【Python应用马尔可夫链算法实现随机文本生成】</p>
<p>48、【数独游戏的Python实现与破解】</p>
<p>49、【使用Python定制词云】</p>
<p>50、【Python开发简单计算器】</p>
<p>50、【Python开发简单计算器】</p>
<p>51、【Python 实现FTP弱口令扫描器】</p>
<p>52、【Python实现Huffman编码解压缩文件】</p>
<p>53、【Python实现Zip文件的暴力破解】</p>
<p>54、【Python3智能裁切图片】</p>
<p>55、【Python实现网站模拟登陆】</p>
<p>56、【给Python3爬虫做一个界面.妹子图网实战】</p>
<p>57、【Python3实现图片转彩色字符】</p>
<p>58、【自联想器的Python 实现】</p>
<p>59、【Python 实现简单流镜】</p>
<p>60、【Flask实现简单聊天室】</p>
<p>61、【基于PyQt5实现地图中定位相片拍摄位置】</p>
<p>62、【Python实现模板引擎】</p>
<p>63、【Python实现遗传算法求解n-queens问题】</p>
<p>64、【Python3实现命令行动态进度条】</p>
<p>65、【Python获取挂号信息并邮件通知】</p>
<p>66、【Python实现java web项目远端自动化更新部署】</p>
<p>67、【使用Python3编写Github 自动周报生成器】</p>
<p>68、【使用Python生成分形图片】</p>
<p>69、【Python 实现Redis 异步客户端】</p>
<p>70、【Python 实现中文错别字高亮系统】</p>
<p>71、【Python自动获取小说工具】</p>
<p>72、【python自动获取酷狗音乐工具】</p>
<p>73、【python自动获取海量IP工具】</p>
<p>74、【Python自动化开发-制作名片卡】</p>
<p>75、【Python自动化开发-微信统计】</p>
<p>76、【Python自动化开发-批量发邮件通知】</p>
<p>77、【Python自动化开发-考勒处理】</p>
<p>78、【Python双色球系统】</p>
<p>79、【Python批量获取王者荣耀皮肤】</p>
<p>80、【Python获取阴阳师壁纸】</p>
<p>81、【Python获取小说数据并分析】</p>
<p>82、【python获取拉钩工具】</p>
<p>83、【Python获取房天下数据】</p>
<p>84、【Python获取彩票信息】</p>
<p>85、【Python获取NBA数据】</p>
<p>86、【Python合成女神图片】</p>
<p>87、【Python法拍网数据】</p>
<p>88、【Python操作Excel自动化开发】</p>
<p>89、【python百行制作登录系统】</p>
<p>90、【python百行制作查询工具】</p>
<p>91、【Python百行代码实现点赞系统】</p>
<p>92、【Python百行代码实现抽奖系统】</p>
<p>93、【python自动工资条】</p>
<p>94、【python抓取相亲网数据</p>
<p>95、【python抓取百合网数据</p>
<p>96、【python制作12306查票工具</p>
<p>97、【python游戏开发公开课</p>
<p>98、【python协程详解公开课</p>
<p>99、【python吧中30行拿LOL皮肤</p>
<p>100、【python快速获取斗图表情</p>
<h3 id="数据分析"><a class="markdownIt-Anchor" href="#数据分析"></a> <a target="_blank" rel="noopener" href="https://blog.csdn.net/qazwsxpy/article/details/128192090?spm=1001.2100.3001.7377&amp;utm_medium=distribute.pc_feed_blog_category.none-task-blog-classify_tag-6-128192090-null-null.nonecase&amp;depth_1-utm_source=distribute.pc_feed_blog_category.none-task-blog-classify_tag-6-128192090-null-null.nonecase">数据分析</a></h3>
<h2 id="机器学习"><a class="markdownIt-Anchor" href="#机器学习"></a> 机器学习</h2>
<h3 id="机器学习-梯度下降"><a class="markdownIt-Anchor" href="#机器学习-梯度下降"></a> <a target="_blank" rel="noopener" href="https://techlead.blog.csdn.net/article/details/133819132?spm=1001.2014.3001.5502">机器学习-梯度下降</a></h3>
<p>目录<br>
一、简介<br>
什么是梯度下降？<br>
为什么梯度下降重要？<br>
二、梯度下降的数学原理<br>
代价函数（Cost Function）<br>
梯度（Gradient）<br>
更新规则<br>
代码示例：基础的梯度下降更新规则<br>
三、批量梯度下降（Batch Gradient Descent）<br>
基础算法<br>
代码示例<br>
四、随机梯度下降（Stochastic Gradient Descent）<br>
基础算法<br>
代码示例<br>
优缺点<br>
五、小批量梯度下降（Mini-batch Gradient Descent）<br>
基础算法<br>
代码示例<br>
优缺点<br>
————————————————</p>
<p>原文链接：<a target="_blank" rel="noopener" href="https://blog.csdn.net/magicyangjay111/article/details/133819132">https://blog.csdn.net/magicyangjay111/article/details/133819132</a></p>
<h3 id><a class="markdownIt-Anchor" href="#"></a> </h3>
<h3 id="遗传算法可选"><a class="markdownIt-Anchor" href="#遗传算法可选"></a> 遗传算法（可选）</h3>
<h2 id="深度学习"><a class="markdownIt-Anchor" href="#深度学习"></a> <a target="_blank" rel="noopener" href="https://blog.csdn.net/chenlycly/article/details/134043297?spm=1001.2100.3001.7377&amp;utm_medium=distribute.pc_feed_blog_category.none-task-blog-classify_tag-3-134043297-null-null.nonecase&amp;depth_1-utm_source=distribute.pc_feed_blog_category.none-task-blog-classify_tag-3-134043297-null-null.nonecase">深度学习</a></h2>
<h3 id="前馈神经网络"><a class="markdownIt-Anchor" href="#前馈神经网络"></a> <a target="_blank" rel="noopener" href="https://techlead.blog.csdn.net/article/details/132421817?spm=1001.2014.3001.5502">前馈神经网络</a></h3>
<p>目录<br>
一、前馈神经网络概述<br>
什么是前馈神经网络<br>
前馈神经网络的工作原理<br>
应用场景及优缺点<br>
二、前馈神经网络的基本结构<br>
输入层、隐藏层和输出层<br>
激活函数的选择与作用<br>
网络权重和偏置<br>
三、前馈神经网络的训练方法<br>
损失函数与优化算法<br>
反向传播算法详解<br>
避免过拟合的策略<br>
四、使用Python和PyTorch实现FNN<br>
4.1 准备数据集<br>
选择合适的数据集<br>
数据预处理<br>
PyTorch数据加载器<br>
4.2 构建模型结构<br>
定义网络架构<br>
选择激活函数<br>
权重初始化<br>
构建与任务相匹配的损失函数<br>
4.3 训练模型<br>
选择优化器<br>
训练循环<br>
模型验证<br>
调整学习率<br>
保存和加载模型<br>
可视化训练过程<br>
4.4 模型评估与可视化<br>
评估指标<br>
模型验证<br>
混淆矩阵<br>
ROC和AUC<br>
特征重要性和模型解释<br>
可视化隐藏层<br>
五、前馈神经网络的先进变体与应用<br>
多层感知器（MLP）<br>
卷积神经网络（CNN）<br>
循环神经网络（RNN）<br>
Transformer结构<br>
强化学习中的FNN<br>
生成对抗网络（GAN）<br>
FNN在医学图像分析中的应用<br>
六、总结与未来展望<br>
总结<br>
未来展望<br>
结语<br>
————————————————</p>
<p>原文链接：<a target="_blank" rel="noopener" href="https://blog.csdn.net/magicyangjay111/article/details/132421817">https://blog.csdn.net/magicyangjay111/article/details/132421817</a></p>
<p>损失函数和优化算法是神经网络训练的基石，决定了网络如何学习和调整其权重。</p>
<ul>
<li><strong>损失函数：</strong> 用于衡量网络预测与实际目标之间的差异，常见的损失函数包括均方误差（MSE）、交叉熵损失等。</li>
<li><strong>优化算法：</strong> 通过最小化损失函数来更新网络权重，常见的优化算法包括随机梯度下降（SGD）、Adam、RMSProp等。</li>
</ul>
<h3 id="长短时记忆网络lstm"><a class="markdownIt-Anchor" href="#长短时记忆网络lstm"></a> <a target="_blank" rel="noopener" href="https://techlead.blog.csdn.net/article/details/132480035">长短时记忆网络（LSTM）</a></h3>
<p>目录</p>
<ol>
<li>LSTM的背景<br>
人工神经网络的进化<br>
循环神经网络（RNN）的局限性<br>
LSTM的提出背景</li>
<li>LSTM的基础理论<br>
2.1 LSTM的数学原理<br>
遗忘门（Forget Gate）<br>
输入门（Input Gate）<br>
记忆单元（Cell State）<br>
输出门（Output Gate）<br>
2.2 LSTM的结构逻辑<br>
遗忘门：决定丢弃的信息<br>
输入门：选择性更新记忆单元<br>
更新单元状态<br>
输出门：决定输出的隐藏状态<br>
门的相互作用<br>
逻辑结构的实际应用<br>
总结<br>
2.3 LSTM与GRU的对比</li>
<li>结构<br>
LSTM<br>
GRU</li>
<li>数学表达<br>
LSTM<br>
GRU</li>
<li>性能和应用<br>
小结</li>
<li>LSTM在实际应用中的优势<br>
处理长期依赖问题<br>
遗忘门机制<br>
梯度消失问题的缓解<br>
广泛的应用领域<br>
灵活的架构选项<br>
成熟的开源实现<br>
小结</li>
<li>LSTM的实战演示<br>
4.1 使用PyTorch构建LSTM模型<br>
定义LSTM模型<br>
训练模型<br>
评估和预测</li>
<li>LSTM总结<br>
<strong>解决长期依赖问题</strong><br>
<strong>广泛的应用领域</strong><br>
<strong>灵活与强大</strong><br>
<strong>开源支持</strong><br>
<strong>持战与展望</strong><br>
总结反思<br>
————————————————</li>
</ol>
<p>原文链接：<a target="_blank" rel="noopener" href="https://blog.csdn.net/magicyangjay111/article/details/132480035">https://blog.csdn.net/magicyangjay111/article/details/132480035</a></p>
<h4 id="lstm的提出背景"><a class="markdownIt-Anchor" href="#lstm的提出背景"></a> LSTM的提出背景</h4>
<p>长短时记忆网络（LSTM）是一种特殊类型的RNN，由Hochreiter和Schmidhuber于1997年提出，目的是解决传统RNN的问题。</p>
<ul>
<li><strong>解决梯度消失问题</strong>: 通过引入“记忆单元”，LSTM能够在长序列中保持信息的流动。</li>
<li><strong>捕捉长依赖性</strong>: LSTM结构允许网络捕捉和理解长序列中的复杂依赖关系。</li>
<li><strong>广泛应用</strong>: 由于其强大的性能和灵活性，LSTM已经被广泛应用于许多序列学习任务，如语音识别、机器翻译和时间序列分析等。</li>
</ul>
<h4 id="记忆单元的结构"><a class="markdownIt-Anchor" href="#记忆单元的结构"></a> “记忆单元”的结构</h4>
<h3 id="深度信念网络dbn"><a class="markdownIt-Anchor" href="#深度信念网络dbn"></a> <a target="_blank" rel="noopener" href="https://techlead.blog.csdn.net/article/details/132531255?spm=1001.2014.3001.5502">深度信念网络（DBN）</a></h3>
<p>一、概述<br>
1.1 深度信念网络的概述<br>
1.2 深度信念网络与其他深度学习模型的比较<br>
结构层次<br>
学习方式<br>
训练和优化<br>
应用领域<br>
1.3 应用领域<br>
图像识别与处理<br>
自然语言处理<br>
推荐系统<br>
语音识别<br>
无监督学习与异常检测<br>
药物发现与生物信息学<br>
二、结构<br>
2.1 受限玻尔兹曼机（RBM）<br>
结构与组成<br>
工作原理<br>
学习算法<br>
应用<br>
2.2 DBN的结构和组成<br>
层次结构<br>
网络连接<br>
训练过程<br>
应用领域<br>
2.3 训练和学习算法<br>
预训练<br>
微调<br>
优化方法<br>
评估和验证<br>
三、实战<br>
3.1 DBN模型的构建<br>
定义RBM层<br>
构建DBN模型<br>
定义DBN的超参数<br>
3.2 预训练<br>
RBM的逐层训练<br>
对比散度（CD）算法<br>
3.3 微调<br>
监督训练<br>
微调训练<br>
模型验证和测试<br>
3.4 应用<br>
分类或回归任务<br>
特征学习<br>
转移学习<br>
在线应用<br>
四、总结<br>
————————————————</p>
<p>原文链接：<a target="_blank" rel="noopener" href="https://blog.csdn.net/magicyangjay111/article/details/132531255">https://blog.csdn.net/magicyangjay111/article/details/132531255</a></p>
<h2 id="项目实战"><a class="markdownIt-Anchor" href="#项目实战"></a> 项目实战</h2>
<h3 id="图像分类发展历史-技术全解与实战"><a class="markdownIt-Anchor" href="#图像分类发展历史-技术全解与实战"></a> <a target="_blank" rel="noopener" href="https://blog.csdn.net/magicyangjay111/article/details/134698103">图像分类：发展历史、技术全解与实战</a></h3>
<ul>
<li>自动化的神经网络架构搜索（NAS）技术</li>
<li>轻量级模型和少样本学习也成为研究的热点</li>
<li>图像分类有望实现更复杂的应用，如情感分析、自动化标注等</li>
</ul>
<p><img src="/2024/02/12/2024%E5%B9%B42%E6%9C%8812%E6%97%A5-%E8%BD%AC%E8%BD%BD%E6%96%87%E7%AB%A0/image-20240212222002730.png" alt="image-20240212222002730"></p>
<h3 id="计算机视觉五大核心研究任务分类识别-检测分割-人体分析-三维视觉-视频分析"><a class="markdownIt-Anchor" href="#计算机视觉五大核心研究任务分类识别-检测分割-人体分析-三维视觉-视频分析"></a> <a target="_blank" rel="noopener" href="https://techlead.blog.csdn.net/article/details/132321843?spm=1001.2014.3001.5502">计算机视觉五大核心研究任务：分类识别、检测分割、人体分析、三维视觉、视频分析</a></h3>
<p>目录<br>
一、引言<br>
1.1 计算机视觉的定义<br>
1.1.1 核心技术<br>
1.1.2 应用场景<br>
1.2 历史背景及发展<br>
1.2.1 1960s-1980s: 初期阶段<br>
1.2.2 1990s-2000s: 机器学习时代<br>
1.2.3 2010s-现在: 深度学习的革命<br>
1.3 应用领域概览<br>
1.3.1 工业自动化<br>
1.3.2 医疗图像分析<br>
1.3.3 自动驾驶<br>
1.3.4 虚拟现实与增强现实<br>
二、计算机视觉五大核心任务<br>
2.1 图像分类与识别<br>
2.1.1 图像分类与识别的基本概念<br>
2.1.2 早期方法与技术演进<br>
2.1.3 深度学习的引入与革新<br>
卷积神经网络在图像分类中的应用<br>
总结<br>
2.2 物体检测与分割<br>
2.2.1 物体检测<br>
早期方法<br>
深度学习方法<br>
2.2.2 物体分割<br>
语义分割<br>
实例分割<br>
总结<br>
2.3 人体分析<br>
2.3.1 人脸识别<br>
2.3.2 人体姿态估计<br>
2.3.3 动作识别<br>
2.3.4 人体分割<br>
2.4 三维计算机视觉<br>
2.4.1 三维重建<br>
立体视觉<br>
多视图几何<br>
点云生成和融合<br>
2.4.2 3D物体检测和识别<br>
基于2D图像的方法<br>
基于点云的方法<br>
2.4.3 三维语义分割<br>
基于体素的方法<br>
基于点云的方法<br>
2.4.4 三维姿态估计<br>
单视图方法<br>
多视图方法<br>
总结<br>
2.5 视频理解与分析<br>
2.5.1 视频分类<br>
2.5.2 动作识别<br>
2.5.3 视频物体检测与分割<br>
2.5.4 视频摘要与高亮检测<br>
2.5.5 视频生成和编辑<br>
总结<br>
三、无监督学习与自监督学习在计算机视觉中的应用<br>
3.1 无监督学习<br>
聚类<br>
降维与表示学习<br>
3.2 自监督学习<br>
对比学习<br>
预训练任务设计<br>
3.3 跨模态学习</p>
<ol start="4">
<li>总结</li>
</ol>
<p>原文链接：<a target="_blank" rel="noopener" href="https://blog.csdn.net/magicyangjay111/article/details/132321843">https://blog.csdn.net/magicyangjay111/article/details/132321843</a></p>
<p><img src="/2024/02/12/2024%E5%B9%B42%E6%9C%8812%E6%97%A5-%E8%BD%AC%E8%BD%BD%E6%96%87%E7%AB%A0/image-20240212224535849.png" alt="image-20240212224535849"></p>
<h3 id="人脸识别"><a class="markdownIt-Anchor" href="#人脸识别"></a> <a target="_blank" rel="noopener" href="https://techlead.blog.csdn.net/article/details/134762161">人脸识别</a>：</h3>
<p>目录<br>
一、人脸识别技术的发展历程<br>
早期探索：20世纪60至80年代<br>
技术价值点：<br>
自动化与算法化：20世纪90年代<br>
技术价值点：<br>
深度学习的革命：21世纪初至今<br>
技术价值点：<br>
二、几何特征方法详解与实战<br>
几何特征方法的原理<br>
几何特征方法的局限性<br>
实战案例：简单的几何特征人脸识别<br>
环境配置<br>
代码实现<br>
代码说明<br>
三、自动化与算法化详解与实战<br>
自动化与算法化的进展<br>
技术创新点：<br>
实战案例：基于特征匹配的人脸识别<br>
环境配置<br>
代码实现<br>
代码说明<br>
四、深度学习方法<br>
深度学习方法的核心概念<br>
技术创新点<br>
实战案例：使用深度学习进行人脸识别<br>
环境配置<br>
代码实现<br>
代码说明<br>
总结<br>
————————————————</p>
<p>原文链接：<a target="_blank" rel="noopener" href="https://blog.csdn.net/magicyangjay111/article/details/134762161">https://blog.csdn.net/magicyangjay111/article/details/134762161</a></p>
<p><img src="/2024/02/12/2024%E5%B9%B42%E6%9C%8812%E6%97%A5-%E8%BD%AC%E8%BD%BD%E6%96%87%E7%AB%A0/image-20240212232302240.png" alt="image-20240212232302240"></p>
<p><img src="/2024/02/12/2024%E5%B9%B42%E6%9C%8812%E6%97%A5-%E8%BD%AC%E8%BD%BD%E6%96%87%E7%AB%A0/image-20240212232432194.png" alt="image-20240212232432194"></p>
<h3 id="时间序列预测"><a class="markdownIt-Anchor" href="#时间序列预测"></a> <a target="_blank" rel="noopener" href="https://snu77.blog.csdn.net/article/details/133272874?spm=1001.2014.3001.5502">时间序列预测：</a></h3>
<h2 id="深度学习-前沿"><a class="markdownIt-Anchor" href="#深度学习-前沿"></a> 深度学习 前沿</h2>
<h3 id="可变形卷积deformable-conv"><a class="markdownIt-Anchor" href="#可变形卷积deformable-conv"></a> <a target="_blank" rel="noopener" href="https://blog.csdn.net/panghuzhenbang/article/details/129816869?spm=1001.2100.3001.7377&amp;utm_medium=distribute.pc_feed_blog_category.none-task-blog-classify_tag-13-129816869-null-null.nonecase&amp;depth_1-utm_source=distribute.pc_feed_blog_category.none-task-blog-classify_tag-13-129816869-null-null.nonecase">可变形卷积(Deformable Conv)</a></h3>
<p><img src="/2024/02/12/2024%E5%B9%B42%E6%9C%8812%E6%97%A5-%E8%BD%AC%E8%BD%BD%E6%96%87%E7%AB%A0/image-20240212235532908.png" alt="image-20240212235532908"></p>
<h3 id="llm"><a class="markdownIt-Anchor" href="#llm"></a> LLM</h3>
<h3 id="sd"><a class="markdownIt-Anchor" href="#sd"></a> SD</h3>
<h2 id="深度学习-项目优化"><a class="markdownIt-Anchor" href="#深度学习-项目优化"></a> 深度学习 项目优化</h2>
<h3 id="量化"><a class="markdownIt-Anchor" href="#量化"></a> 量化</h3>
<h3 id="cuda"><a class="markdownIt-Anchor" href="#cuda"></a> CUDA</h3>
<h4 id="c-cuda手搓一个简单的神经网络推理"><a class="markdownIt-Anchor" href="#c-cuda手搓一个简单的神经网络推理"></a> <a target="_blank" rel="noopener" href="https://blog.csdn.net/Bartender_VA11/article/details/135999169">C++ CUDA手搓一个简单的神经网络推理</a></h4>
<h3 id="trt"><a class="markdownIt-Anchor" href="#trt"></a> TRT</h3>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://shakewely.github.io/2024/02/12/2024%E5%B9%B42%E6%9C%8812%E6%97%A5-%E8%BD%AC%E8%BD%BD%E6%96%87%E7%AB%A0/" data-id="cltgreey1000aw8v4ahty4aie" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9D%82/" rel="tag">杂</a></li></ul>

    </footer>
  </div>
  
</article>




  


  <nav id="page-nav">
    <a class="extend prev" rel="prev" href="/page/4/">&amp;laquo; pre</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><a class="page-number" href="/page/7/">7</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/">10</a><a class="extend next" rel="next" href="/page/6/">next &amp;raquo;</a>
  </nav>
</section>
        
          <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title"></h3>
  <div class="widget">
    <h1 class="blog-title">Weakliy_Blog</h1>
    <h2 class="blog-subtitle"></h2>
    <ul class="blog-link">
     
          <a href="/" title="Home">
            <li>主页</li>
          </a>
        
          <a href="/archives" title="Archives">
            <li>归档</li>
          </a>
        
          <a href="/categories" title="Categories">
            <li>分类</li>
          </a>
        
          <a href="/tags" title="Tags">
            <li>标签</li>
          </a>
        
    </ul>
  </div>
</div>

  
    <div class="widget-wrap">
  <h3 class="widget-title"></h3>
  <div class="widget">
    <img class="avatar" src="https://raw.githubusercontent.com/ShakeWeLy/Weakliy.github.io/main/%E5%A4%B4%E5%83%8F/mmexport1683194148817.png">
    <h2 class="author">Weakliy</h2>
    <h3 class="description"></h3>
    <div class="count-box">
      <a href="/archives"><div><strong>92</strong><br>文章</div></a>
      <a href="/categories"><div><strong>0</strong><br>分类</div></a>
      <a href="/tags"><div><strong>29</strong><br>标签</div></a>
    </div>



    <div class="social-link">
      
        <a class="hvr-bounce-in" href="https://github.com/ShakeWeLy" target="_blank" title="Github">
          Github
        </a>
      
    </div>

    <div class="friend-link">
      <h2>联系我</h2>
      
        <a class="hvr-bounce-in" href="https://github.com/ShakeWeLy" target="_blank" title="ShanaMaid">
          ShanaMaid
        </a>
      
    </div>
  </div>
</div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy;2024 - 2025 Weakliy<br>
      由<a href="http://hexo.io/" target="_blank">Hexo</a>强力驱动 | 
      主题-<a target="_blank" rel="noopener" href="https://github.com/ShanaMaid/hexo-theme-shana">Shana</a>
      
    </div>
    
  </div>
</footer>
    </div>
    

<script src="//apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"></script>
<script src="//apps.bdimg.com/libs/wow/0.1.6/wow.min.js"></script>
<script>
new WOW().init();
</script>   


  
<link rel="stylesheet" href="/plugin/fancybox/jquery.fancybox.css">

  
<script src="/plugin/fancybox/jquery.fancybox.pack.js"></script>




  
<link rel="stylesheet" href="/plugin/galmenu/GalMenu.css">

  
<script src="/plugin/galmenu/GalMenu.js"></script>

  <div class="GalMenu GalDropDown">
      <div class="circle" id="gal">
        <div class="ring">
          
            <a href="/" title="" class="menuItem">首页</a>
          
            <a href="/tags" title="" class="menuItem">标签</a>
          
            <a href="/categories" title="" class="menuItem">分类</a>
          
            <a href="/archives" title="" class="menuItem">总览</a>
          
            <a href="/xxxxxxxxx" title="" class="menuItem">xxx</a>
          
            <a href="/xxxxxxx" title="" class="menuItem">xxxx</a>
          
        </div>
        
          <audio id="audio" src="#"></audio>
        
      </div> 
</div>
<div id="overlay" style="opacity: 1; cursor: pointer;"></div>
  <script type="text/javascript">var items = document.querySelectorAll('.menuItem');
    for (var i = 0,
    l = items.length; i < l; i++) {
      items[i].style.left = (50 - 35 * Math.cos( - 0.5 * Math.PI - 2 * (1 / l) * i * Math.PI)).toFixed(4) + "%";
      items[i].style.top = (50 + 35 * Math.sin( - 0.5 * Math.PI - 2 * (1 / l) * i * Math.PI)).toFixed(4) + "%"
    }</script>
<script type="text/javascript">
  $(document).ready(function() {
    $('body').GalMenu({
      'menu': 'GalDropDown'
    })
  });
</script>

  <section class="hidden-xs"> 
  <ul class="cb-slideshow"> 
    <li><span>苟利</span></li> 
    <li><span>国家</span></li> 
    <li><span>生死以</span></li> 
    <li><span>岂能</span></li> 
    <li><span>祸福</span></li> 
    <li><span>趋避之</span></li> 
  </ul>
</section>

<script src="/js/script.js"></script>




  </div>
</body>
</html>