<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Weakliy_Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Weakliy_Blog">
<meta property="og:url" content="https://shakewely.github.io/page/5/index.html">
<meta property="og:site_name" content="Weakliy_Blog">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Weakliy">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Weakliy_Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

  
<link rel="stylesheet" href="/plugin/bganimation/bg.css">

  

  <link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css" rel="stylesheet" type="text/css">
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <div class="outer">
        <div class="widget-wrap mobile-header">
  <h3 class="widget-title"></h3>
  <div class="widget">
    <img class="avatar" src="https://raw.githubusercontent.com/ShakeWeLy/Weakliy.github.io/main/%E5%A4%B4%E5%83%8F/mmexport1683194148817.png">
    <h2 class="author">Weakliy</h2>
    <h3 class="description"></h3>
    <div class="count-box">
      <a href="/archives"><div><strong>98</strong><br>文章</div></a>
      <a href="/categories"><div><strong>0</strong><br>分类</div></a>
      <a href="/tags"><div><strong>30</strong><br>标签</div></a>
    </div>
    <ul class="blog-link">
     
          <a href="/" title="Home">
            <li>主页</li>
          </a>
        
          <a href="/archives" title="Archives">
            <li>归档</li>
          </a>
        
          <a href="/categories" title="Categories">
            <li>分类</li>
          </a>
        
          <a href="/tags" title="Tags">
            <li>标签</li>
          </a>
        
    </ul>
  </div>
</div>

        <section id="main">
  
    <article id="post-2025年2月19日-CRNN模型" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2025/02/19/2025%E5%B9%B42%E6%9C%8819%E6%97%A5-CRNN%E6%A8%A1%E5%9E%8B/" class="article-date">
  <time class="post-time" datetime="2025-02-19T06:46:09.000Z" itemprop="datePublished">
    <span class="post-month">2月</span><br/>
    <span class="post-day">19</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/02/19/2025%E5%B9%B42%E6%9C%8819%E6%97%A5-CRNN%E6%A8%A1%E5%9E%8B/">2025年2月19日 CRNN模型</a>
    </h1>
  

        <div>
          
          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="ocr"><a class="markdownIt-Anchor" href="#ocr"></a> OCR</h1>
<h2 id="前言"><a class="markdownIt-Anchor" href="#前言"></a> 前言</h2>
<p>CRNN主要是<strong>CNN（卷积神经网络）+ RNN（循环神经网络）+ CTC（连接时序分类）</strong> 的组合，广泛用于 <strong>场景文本识别（STR, Scene Text Recognition）</strong></p>
<p><img src="/2025/02/19/2025%E5%B9%B42%E6%9C%8819%E6%97%A5-CRNN%E6%A8%A1%E5%9E%8B/image-20250219145436243.png" alt="image-20250219145436243"></p>
<p>在早期，OCR任务通常分为两个步骤：单字切割和分类任务。首先，使用投影法等技术将文本文件中的连续文字切割成单个字符，然后将这些字符输入到卷积神经网络（CNN）中进行分类。然而，这种方法现在已经有些过时。</p>
<p>目前更流行的方法是基于深度学习的端到端的文本识别，这种方法不需要显式地进行文字切割。而是将文字识别转化为序列学习问题。尽管输入的图像尺度和文本长度可能不同，但是通过深度卷积神经网络（DCNN）和循环神经网络（RNN）的处理，以及在输出阶段应用一定的连接时序分类（CTC）翻译转录后，就可以对整个文本图像进行识别。这意味着文字的切割过程已经被融入到深度学习模型中，从而简化了整个OCR流程。</p>
<p>深度学习在端到端OCR（光学字符识别）技术中的应用，特别是CRNN OCR和attention OCR两种技术。这两种技术都使用了CNN+RNN的网络结构进行特征学习，但在最后的输出层（翻译层）有所不同。CRNN OCR使用CTC（连接时序分类）算法进行序列对齐，而attention OCR则采用attention机制来进行序列对齐。文件中提到，CRNN算法应用更为广泛，因此主要介绍的是CRNN算法。</p>
<h2 id="crnn"><a class="markdownIt-Anchor" href="#crnn"></a> CRNN</h2>
<h3 id="网络结构"><a class="markdownIt-Anchor" href="#网络结构"></a> 网络结构</h3>
<img src="/2025/02/19/2025%E5%B9%B42%E6%9C%8819%E6%97%A5-CRNN%E6%A8%A1%E5%9E%8B/image-20250219150224189.png" style="zoom: 50%;">
<h4 id="1cnn"><a class="markdownIt-Anchor" href="#1cnn"></a> 1.CNN</h4>
<img src="/2025/02/19/2025%E5%B9%B42%E6%9C%8819%E6%97%A5-CRNN%E6%A8%A1%E5%9E%8B/image-20250219150328966.png" alt="image-20250219150328966" style="zoom: 33%;">
<h5 id="1x2-池化"><a class="markdownIt-Anchor" href="#1x2-池化"></a> 1x2 池化</h5>
<h4 id="2map2feature-sequence"><a class="markdownIt-Anchor" href="#2map2feature-sequence"></a> 2.Map2Feature sequence</h4>
<p><img src="/2025/02/19/2025%E5%B9%B42%E6%9C%8819%E6%97%A5-CRNN%E6%A8%A1%E5%9E%8B/image-20250219150833633.png" alt="image-20250219150833633"></p>
<img src="/2025/02/19/2025%E5%B9%B42%E6%9C%8819%E6%97%A5-CRNN%E6%A8%A1%E5%9E%8B/image-20250219150936041.png" alt="image-20250219150936041" style="zoom:50%;">
<h4 id="3rnn"><a class="markdownIt-Anchor" href="#3rnn"></a> 3.RNN</h4>
<h5 id="lstm"><a class="markdownIt-Anchor" href="#lstm"></a> LSTM</h5>
<img src="/2025/02/19/2025%E5%B9%B42%E6%9C%8819%E6%97%A5-CRNN%E6%A8%A1%E5%9E%8B/image-20250219150959428.png" alt="image-20250219150959428" style="zoom: 50%;">
<h5 id="双向lstm"><a class="markdownIt-Anchor" href="#双向lstm"></a> 双向LSTM</h5>
<h5 id><a class="markdownIt-Anchor" href="#"></a> </h5>
<h4 id="4ctcloss"><a class="markdownIt-Anchor" href="#4ctcloss"></a> 4.CTCLoss</h4>
<img src="/2025/02/19/2025%E5%B9%B42%E6%9C%8819%E6%97%A5-CRNN%E6%A8%A1%E5%9E%8B/image-20250219154341960.png" alt="image-20250219154341960" style="zoom: 80%;">
<p>如上图，对于最简单的时序为 2 的字符识别，有两个时间步长(t0，t1)和三个可能的字符为“ａ”，“ｂ”和“-”，我们得到两个概率分布向量，如果采取最大概率路径解码的方法，则“–”的概率最大，即真实字符为空的概率为0.6*0.6=0.36。</p>
<p>但是为字符“ａ”的情况有多种对齐组合，“aa”, “a-“和“-a”都是代表“ａ”，所以，输出“ａ”的概率应该为三种之和：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.4 * 0.4 + 0.4 * 0.6 + 0.6 * 0.4 = 0.16 + 0.24 + 0.24 = 0.64</span><br></pre></td></tr></table></figure>
<h4 id="5attention"><a class="markdownIt-Anchor" href="#5attention"></a> 5.Attention</h4>
<p>基于Attention的OCR解码算法，把OCR文字识别当成文字翻译任务，即通过Attention Decoder出文字序列。</p>
<h5 id="rnn-seq2seq"><a class="markdownIt-Anchor" href="#rnn-seq2seq"></a> <strong>RNN -&gt; Seq2Seq</strong></h5>
<p><img src="/2025/02/19/2025%E5%B9%B42%E6%9C%8819%E6%97%A5-CRNN%E6%A8%A1%E5%9E%8B/image-20250219155949098.png" alt="image-20250219155949098"></p>
<p>左图是经典的RNN结构，右图是Seq2Seq结构。RNN的输入序列和输出序列必须有相同的时间长度，而机器翻译以及文字识别任务都是输入输出不对齐的，不能直接使用RNN结构进行解码。于是在Seq2Seq结构中，将输入序列进行Encoder编码成一个统一的语义向量Context，然后送入Decoder中一个一个解码出输出序列。在Decoder解码过程中，第一个输入字符为<start>，然后不断将前一个时刻的输出作为下一个时刻的输入，循环解码，直到输出<stop></stop>字符为止</start></p>
<p><strong>Seq2Seq -&gt; Attention Decoder</strong></p>
<img src="/2025/02/19/2025%E5%B9%B42%E6%9C%8819%E6%97%A5-CRNN%E6%A8%A1%E5%9E%8B/image-20250219160050009.png" alt="image-20250219160050009" style="zoom: 50%;">
<p>Seq2Seq把所有的输入序列都编码成一个统一的语义向量Context，然后再由Decoder解码。由于context包含原始序列中的所有信息，它的长度就成了限制模型性能的瓶颈。</p>
<p>所以如果要改进Seq2Seq结构，最好的切入角度就是：**利用Encoder所有隐藏层状态解决Context长度限制问题。**于是Attention Decoder在Seq2Seq的基础上，增加了一个Attention Layer，如上图所示。</p>
<img src="/2025/02/19/2025%E5%B9%B42%E6%9C%8819%E6%97%A5-CRNN%E6%A8%A1%E5%9E%8B/image-20250219160149932.png" alt="image-20250219160149932" style="zoom: 50%;">
<p>在Decoder时，每个时刻的解码状态跟Encoder的所有隐藏层状态进行cross-attention计算，cross-attention将当前解码的隐藏层状态和encoder的所有隐藏层状态做相关性计算，然后对encoder的所有隐藏层加权求和，最后和当前解码的隐藏层状态concat得到最终的状态。</p>
<h4 id="6ace"><a class="markdownIt-Anchor" href="#6ace"></a> 6.ACE</h4>
<p>基于ACE的解码方法不同于CTC和Attention，ACE的监督信号实际上是一种弱监督(<strong>输入输出没有做形式上的对齐，没有先后顺序信息，倾向于学习表征</strong>)，并且可以用于多行文字识别。</p>
<p><img src="/2025/02/19/2025%E5%B9%B42%E6%9C%8819%E6%97%A5-CRNN%E6%A8%A1%E5%9E%8B/image-20250219160702221.png" alt="image-20250219160702221"></p>
<p><img src="/2025/02/19/2025%E5%B9%B42%E6%9C%8819%E6%97%A5-CRNN%E6%A8%A1%E5%9E%8B/image-20250219160636874.png" alt="image-20250219160636874"></p>
<p>从模型设计上来看，可以采用结合上面3种方法的**多任务文本识别模型。在训练时，以CTC为主，Attention Decoder和ACE辅助训练。在预测时，考虑到速度和性能，只采用CTC进行解码预测。**多任务可以提高模型的泛化性，同时如果对预测时间要求不高，多结果也可以提供更多的选择和对比。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Encoder+Decoder+Attention is implemented from:</span></span><br><span class="line"><span class="string">    https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CNN</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, channel_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(CNN, self).__init__()</span><br><span class="line">        self.cnn = nn.Sequential(</span><br><span class="line">                      nn.Conv2d(channel_size, <span class="number">64</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>), nn.ReLU(<span class="literal">True</span>), nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">                      nn.Conv2d(<span class="number">64</span>, <span class="number">128</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>), nn.ReLU(<span class="literal">True</span>), nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">                      nn.Conv2d(<span class="number">128</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>), nn.BatchNorm2d(<span class="number">256</span>), nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">                      nn.Conv2d(<span class="number">256</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>), nn.ReLU(<span class="literal">True</span>), nn.MaxPool2d((<span class="number">2</span>,<span class="number">2</span>), (<span class="number">2</span>,<span class="number">1</span>), (<span class="number">0</span>,<span class="number">1</span>)),</span><br><span class="line">                      nn.Conv2d(<span class="number">256</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>), nn.BatchNorm2d(<span class="number">512</span>), nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">                      nn.Conv2d(<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>), nn.ReLU(<span class="literal">True</span>), nn.MaxPool2d((<span class="number">2</span>,<span class="number">2</span>), (<span class="number">2</span>,<span class="number">1</span>), (<span class="number">0</span>,<span class="number">1</span>)),</span><br><span class="line">                      nn.Conv2d(<span class="number">512</span>, <span class="number">512</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>), nn.BatchNorm2d(<span class="number">512</span>), nn.ReLU(<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        <span class="comment"># [n, channel_size, 32, 280] -&gt; [n, 512, 1, 71]</span></span><br><span class="line">        conv = self.cnn(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> conv</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BidirectionalLSTM</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size, output_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(BidirectionalLSTM, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.rnn = nn.LSTM(input_size, hidden_size, bidirectional=<span class="literal">True</span>)</span><br><span class="line">        self.embedding = nn.Linear(hidden_size * <span class="number">2</span>, output_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        recurrent, _ = self.rnn(<span class="built_in">input</span>)</span><br><span class="line">        T, b, h = recurrent.size()</span><br><span class="line">        t_rec = recurrent.view(T * b, h)</span><br><span class="line"></span><br><span class="line">        output = self.embedding(t_rec)  <span class="comment"># [T * b, output_size]</span></span><br><span class="line">        output = output.view(T, b, -<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AttnDecoderRNN</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, hidden_size, output_size, dropout_p=<span class="number">0.1</span>, max_length=<span class="number">71</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(AttnDecoderRNN, self).__init__()</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.output_size = output_size</span><br><span class="line">        self.dropout_p = dropout_p</span><br><span class="line">        self.max_length = max_length</span><br><span class="line"></span><br><span class="line">        self.embedding = nn.Embedding(self.output_size, self.hidden_size)</span><br><span class="line">        self.attn = nn.Linear(self.hidden_size * <span class="number">2</span>, self.max_length)</span><br><span class="line">        self.attn_combine = nn.Linear(self.hidden_size * <span class="number">2</span>, self.hidden_size)</span><br><span class="line">        self.dropout = nn.Dropout(self.dropout_p)</span><br><span class="line">        self.gru = nn.GRU(self.hidden_size, self.hidden_size)</span><br><span class="line">        self.out = nn.Linear(self.hidden_size, self.output_size)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span>, hidden, encoder_outputs</span>):</span><br><span class="line">        embedded = self.embedding(<span class="built_in">input</span>)</span><br><span class="line">        embedded = self.dropout(embedded)</span><br><span class="line"></span><br><span class="line">        attn_weights = F.softmax(self.attn(torch.cat((embedded, hidden[<span class="number">0</span>]), <span class="number">1</span>)), dim=<span class="number">1</span>)</span><br><span class="line">        attn_applied = torch.bmm(attn_weights.unsqueeze(<span class="number">1</span>), encoder_outputs.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">        output = torch.cat((embedded, attn_applied.squeeze(<span class="number">1</span>)), <span class="number">1</span>)</span><br><span class="line">        output = self.attn_combine(output).unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        output = F.relu(output)</span><br><span class="line">        output, hidden = self.gru(output, hidden)</span><br><span class="line"></span><br><span class="line">        output = F.log_softmax(self.out(output[<span class="number">0</span>]), dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> output, hidden, attn_weights</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">initHidden</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.zeros(<span class="number">1</span>, <span class="number">1</span>, self.hidden_size, device=device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, channel_size, hidden_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line">        self.cnn = CNN(channel_size)</span><br><span class="line">        self.rnn = nn.Sequential(</span><br><span class="line">            BidirectionalLSTM(<span class="number">512</span>, hidden_size, hidden_size),</span><br><span class="line">            BidirectionalLSTM(hidden_size, hidden_size, hidden_size))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        <span class="comment"># conv features</span></span><br><span class="line">        conv = self.cnn(<span class="built_in">input</span>)</span><br><span class="line">        b, c, h, w = conv.size()</span><br><span class="line">        <span class="keyword">assert</span> h == <span class="number">1</span>, <span class="string">&quot;the height of conv must be 1&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># rnn feature</span></span><br><span class="line">        conv = conv.squeeze(<span class="number">2</span>)        <span class="comment"># [b, c, 1, w] -&gt; [b, c, w]</span></span><br><span class="line">        conv = conv.permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>)  <span class="comment"># [b, c, w] -&gt; [w, b, c]</span></span><br><span class="line">        output = self.rnn(conv)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, hidden_size, output_size, dropout_p=<span class="number">0.1</span>, max_length=<span class="number">71</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.decoder = AttnDecoderRNN(hidden_size, output_size, dropout_p, max_length)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span>, hidden, encoder_outputs</span>):</span><br><span class="line">        <span class="keyword">return</span> self.decoder(<span class="built_in">input</span>, hidden, encoder_outputs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">initHidden</span>(<span class="params">self, batch_size</span>):</span><br><span class="line">        result = torch.autograd.Variable(torch.zeros(<span class="number">1</span>, batch_size, self.hidden_size))</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="model改进"><a class="markdownIt-Anchor" href="#model改进"></a> Model改进</h2>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://shakewely.github.io/2025/02/19/2025%E5%B9%B42%E6%9C%8819%E6%97%A5-CRNN%E6%A8%A1%E5%9E%8B/" data-id="cm8q1tfhm000mpcv41rh6e84u" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/dl%E3%80%81model/" rel="tag">dl、model</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-2025年2月19日-pytorch2" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2025/02/19/2025%E5%B9%B42%E6%9C%8819%E6%97%A5-pytorch2/" class="article-date">
  <time class="post-time" datetime="2025-02-19T03:10:27.000Z" itemprop="datePublished">
    <span class="post-month">2月</span><br/>
    <span class="post-day">19</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/02/19/2025%E5%B9%B42%E6%9C%8819%E6%97%A5-pytorch2/">2025年2月19日 pytorch2</a>
    </h1>
  

        <div>
          
          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>TorchScript是PyTorch中实现**即时编译（JIT）**的核心工具，它通过将动态图模型转换为静态图形式，解决了PyTorch模型在部署、性能优化和多平台支持等方面的关键问题</p>
<p><strong>即时编译（JIT）</strong>：一种程序优化技术，通过将代码在运行时编译为机器码来提升性能。PyTorch JIT通过TorchScript实现这一机制</p>
<p>动态图是在运行时动态构建计算图，允许灵活修改，适合开发和调试；</p>
<p>而静态图是预先定义完整的计算图，然后进行优化和执行，适合部署和性能优化</p>
<p>例如，PyTorch的动态图允许在训练循环中修改模型结构，而TensorFlow的静态图一旦编译后无法更改。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://shakewely.github.io/2025/02/19/2025%E5%B9%B42%E6%9C%8819%E6%97%A5-pytorch2/" data-id="cm8q1tfhh000jpcv4fh4o5snj" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/dl%E3%80%81python/" rel="tag">dl、python</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-2025年2月14日-推理框架2" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2025/02/13/2025%E5%B9%B42%E6%9C%8814%E6%97%A5-%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B62/" class="article-date">
  <time class="post-time" datetime="2025-02-13T14:12:11.000Z" itemprop="datePublished">
    <span class="post-month">2月</span><br/>
    <span class="post-day">13</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/02/13/2025%E5%B9%B42%E6%9C%8814%E6%97%A5-%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B62/">2025年2月14日 推理框架2——MNN</a>
    </h1>
  

        <div>
          
          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="mnn"><a class="markdownIt-Anchor" href="#mnn"></a> MNN</h1>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://shakewely.github.io/2025/02/13/2025%E5%B9%B42%E6%9C%8814%E6%97%A5-%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B62/" data-id="cm8q1tfhm000npcv4gkf71mcg" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/dl%E3%80%81infer/" rel="tag">dl、infer</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-2025年2月13日-推理框架" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2025/02/13/2025%E5%B9%B42%E6%9C%8813%E6%97%A5-%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6/" class="article-date">
  <time class="post-time" datetime="2025-02-13T06:43:20.000Z" itemprop="datePublished">
    <span class="post-month">2月</span><br/>
    <span class="post-day">13</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/02/13/2025%E5%B9%B42%E6%9C%8813%E6%97%A5-%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6/">2025年2月13日 推理框架1-OpenVINO</a>
    </h1>
  

        <div>
          
          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="openvino"><a class="markdownIt-Anchor" href="#openvino"></a> OpenVINO</h2>
<h2 id="introduce"><a class="markdownIt-Anchor" href="#introduce"></a> Introduce</h2>
<h3 id="openvino-工具"><a class="markdownIt-Anchor" href="#openvino-工具"></a> OpenVINO 工具</h3>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/openvinotoolkit/nncf">神经网络压缩框架 （NNCF）</a> - 高级模型优化技术，包括量化、滤波器修剪、二值化和稀疏性。</li>
<li><a target="_blank" rel="noopener" href="https://github.com/openvinotoolkit/openvino.genai">GenAI 存储库</a>和 <a target="_blank" rel="noopener" href="https://github.com/openvinotoolkit/openvino_tokenizers">OpenVINO Tokenizers</a> - 用于开发和优化生成式 AI 应用程序的资源和工具。</li>
<li><a target="_blank" rel="noopener" href="https://github.com/openvinotoolkit/model_server">OpenVINO™ 模型服务器 （OVMS）</a> - 一种可扩展的高性能解决方案，用于为针对 Intel 架构优化的模型提供服务。</li>
<li><a target="_blank" rel="noopener" href="https://geti.intel.com/">Intel® Geti™</a> - 适用于计算机视觉用例的交互式视频和图像注释工具。</li>
</ul>
<h3 id="如何优化的"><a class="markdownIt-Anchor" href="#如何优化的"></a> 如何优化的</h3>
<img src="/2025/02/13/2025%E5%B9%B42%E6%9C%8813%E6%97%A5-%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6/image-20250213215109749.png" alt="image-20250213215109749"> 
<ol>
<li>模型压缩、层融合（Layer Fusion）、量化（Quantization）等技术优化模型</li>
<li>自动利用硬件加速特性</li>
</ol>
<h2 id="workflow"><a class="markdownIt-Anchor" href="#workflow"></a> Workflow</h2>
<p><img src="/2025/02/13/2025%E5%B9%B42%E6%9C%8813%E6%97%A5-%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6/image-20250213214620596.png" alt></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">训练模型 (TF/PyTorch) → 转换为 IR → 优化 → 部署到 CPU/GPU/VPU → 推理 → 性能调优</span><br></pre></td></tr></table></figure>
<p>Openvino整体框架为：<strong>Openvino前端→ Plugin中间层→ Backend后端</strong></p>
<img src="/2025/02/13/2025%E5%B9%B42%E6%9C%8813%E6%97%A5-%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6/image-20250213220750908.png" alt="image-20250213220750908" style="zoom:50%;">
<h2 id="code"><a class="markdownIt-Anchor" href="#code"></a> Code</h2>
<p>Python：<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_44613415/article/details/143507060">https://blog.csdn.net/weixin_44613415/article/details/143507060</a></p>
<p>​	<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_38739735/article/details/137099211=">https://blog.csdn.net/weixin_38739735/article/details/137099211=</a></p>
<p>C：<a target="_blank" rel="noopener" href="https://blog.csdn.net/bjbz_cxy/article/details/130318904">https://blog.csdn.net/bjbz_cxy/article/details/130318904</a></p>
<p>​	<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_44732054/article/details/140971622">https://blog.csdn.net/qq_44732054/article/details/140971622</a></p>
<h1 id="ncnn"><a class="markdownIt-Anchor" href="#ncnn"></a> ncnn</h1>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/PHILICS7/article/details/125430079">https://blog.csdn.net/PHILICS7/article/details/125430079</a></p>
<p>ncnn是腾讯优图推出的在手机端极致优化的高性能神经网络前向计架框架，适用于手机端的CPU计算且无需依赖第三方计算库，ncnn只用作推理而非边训练边推理。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://shakewely.github.io/2025/02/13/2025%E5%B9%B42%E6%9C%8813%E6%97%A5-%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6/" data-id="cm8q1tfhl000lpcv473w5empa" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/dl%E3%80%81infer/" rel="tag">dl、infer</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-2025年1月25日-Segmentation模型" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2025/02/10/2025%E5%B9%B41%E6%9C%8825%E6%97%A5-Segmentation%E6%A8%A1%E5%9E%8B/" class="article-date">
  <time class="post-time" datetime="2025-02-10T07:29:02.000Z" itemprop="datePublished">
    <span class="post-month">2月</span><br/>
    <span class="post-day">10</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/02/10/2025%E5%B9%B41%E6%9C%8825%E6%97%A5-Segmentation%E6%A8%A1%E5%9E%8B/">2025年1月25日 Segmentation模型</a>
    </h1>
  

        <div>
          
          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="segmentation"><a class="markdownIt-Anchor" href="#segmentation"></a> Segmentation</h1>
<h2 id="unet"><a class="markdownIt-Anchor" href="#unet"></a> UNET</h2>
<img src="/2025/02/10/2025%E5%B9%B41%E6%9C%8825%E6%97%A5-Segmentation%E6%A8%A1%E5%9E%8B/image-20250105155647634.png" alt="image-20250105155647634" style="zoom:33%;"> 
<h3 id="u2net"><a class="markdownIt-Anchor" href="#u2net"></a> U2NET</h3>
<p>SOD任务</p>
<p><img src="/2025/02/10/2025%E5%B9%B41%E6%9C%8825%E6%97%A5-Segmentation%E6%A8%A1%E5%9E%8B/image-20250105155624697.png" alt="image-20250105155624697" style="zoom:50%;"> 整体</p>
<p><img src="/2025/02/10/2025%E5%B9%B41%E6%9C%8825%E6%97%A5-Segmentation%E6%A8%A1%E5%9E%8B/image-20250105155734798.png" alt="image-20250105155734798" style="zoom: 33%;"> 小块</p>
<h2 id="sam"><a class="markdownIt-Anchor" href="#sam"></a> SAM</h2>
<p><img src="https://github.com/facebookresearch/segment-anything/raw/main/assets/model_diagram.png?raw=true" alt="SAM design"></p>
<p><img src="/2025/02/10/2025%E5%B9%B41%E6%9C%8825%E6%97%A5-Segmentation%E6%A8%A1%E5%9E%8B/image-20250107134550384.png" alt="image-20250107134550384"></p>
<img src="/2025/02/10/2025%E5%B9%B41%E6%9C%8825%E6%97%A5-Segmentation%E6%A8%A1%E5%9E%8B/image-20250107134129164.png" alt="image-20250107134129164" style="zoom: 50%;">

      
    </div>
    <footer class="article-footer">
      <a data-url="https://shakewely.github.io/2025/02/10/2025%E5%B9%B41%E6%9C%8825%E6%97%A5-Segmentation%E6%A8%A1%E5%9E%8B/" data-id="cm8q1tfhf000fpcv4fgiq10pj" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/model/" rel="tag">model</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-2025年1月15日-Vit模型" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2025/02/10/2025%E5%B9%B41%E6%9C%8815%E6%97%A5-Vit%E6%A8%A1%E5%9E%8B/" class="article-date">
  <time class="post-time" datetime="2025-02-10T07:28:02.000Z" itemprop="datePublished">
    <span class="post-month">2月</span><br/>
    <span class="post-day">10</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/02/10/2025%E5%B9%B41%E6%9C%8815%E6%97%A5-Vit%E6%A8%A1%E5%9E%8B/">2025年1月15日 Vit模型</a>
    </h1>
  

        <div>
          
          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="vit"><a class="markdownIt-Anchor" href="#vit"></a> VIT</h1>
<h2 id="原理"><a class="markdownIt-Anchor" href="#原理"></a> 原理</h2>
<p><strong>图像分块</strong>：</p>
<p>小块16*16</p>
<p><strong>线性嵌入</strong>：<br>
使用线性投影将每个图像块映射到固定长度的向量（称为 Patch Embedding），然后通过添加位置编码（Positional Encoding）表示块的位置信息。</p>
<p><strong>Transformer 编码器</strong>：<br>
利用标准的 Transformer 编码器（多头自注意力和前馈网络）对这些嵌入进行处理，学习全局的图像特征</p>
<p><strong>分类头</strong>：<br>
引入一个特殊的 [CLS] token，用于整合所有块的信息。最后通过 MLP（多层感知机）对 [CLS] token 进行分类。</p>
<img src="https://i-blog.csdnimg.cn/blog_migrate/3360fc9f03a84f99067859e2db20ffb5.gif" alt="img" style="zoom:60%;">
<p><img src="/2025/02/10/2025%E5%B9%B41%E6%9C%8815%E6%97%A5-Vit%E6%A8%A1%E5%9E%8B/image-20250210154505777.png" alt="image-20250210154505777"></p>
<h1 id="swin-transformer"><a class="markdownIt-Anchor" href="#swin-transformer"></a> Swin Transformer</h1>
<h2 id="核心原理"><a class="markdownIt-Anchor" href="#核心原理"></a> 核心原理：</h2>
<ol>
<li><strong>分层架构</strong>：<br>
Swin Transformer 的结构是分层的，类似于卷积神经网络（CNN）的金字塔设计。随着层数的增加，特征图的分辨率逐渐降低，通道数逐渐增加。</li>
<li><strong>滑动窗口注意力 (Shifted Window Attention)</strong>：
<ul>
<li>将图像划分为多个固定大小的窗口（如 7×77 \times 77×7），在每个窗口内计算自注意力，降低计算复杂度。</li>
<li>在不同层之间引入“滑动窗口”机制，使窗口之间的信息可以交互，增加全局建模能力。</li>
</ul>
</li>
<li><strong>线性复杂度</strong>：<br>
通过限制注意力计算在窗口内完成，避免了 ViT 的全局注意力计算带来的高时间和空间复杂度问题。</li>
<li><strong>Patch 合并</strong>：<br>
在分层过程中，通过 Patch 合并操作减少特征图分辨率，同时增加通道维度。</li>
</ol>
<h2 id="滑动窗口注意力"><a class="markdownIt-Anchor" href="#滑动窗口注意力"></a> 滑动窗口注意力</h2>
<img src="https://i-blog.csdnimg.cn/blog_migrate/4835528644ffcb483732678807ab00b6.png#pic_center" alt="在这里插入图片描述" style="zoom:33%;">
<h3 id="patch-merging"><a class="markdownIt-Anchor" href="#patch-merging"></a> <strong>Patch Merging</strong></h3>
<p><img src="https://i-blog.csdnimg.cn/blog_migrate/a3b29d89bad938bc2475dfca1df80501.png" alt="img"></p>
<h3 id="w-msa"><a class="markdownIt-Anchor" href="#w-msa"></a> W-MSA</h3>
<img src="https://i-blog.csdnimg.cn/blog_migrate/08dfc09f92ff3766608490fcbcd355b2.png#pic_center" alt="在这里插入图片描述" style="zoom:50%;">
<h3 id="sw-msa"><a class="markdownIt-Anchor" href="#sw-msa"></a> SW-MSA</h3>
<img src="https://i-blog.csdnimg.cn/blog_migrate/05a8fc43e0f99856f885fd489388136b.png#pic_center" alt="在这里插入图片描述" style="zoom:33%;">
<h1 id="mae"><a class="markdownIt-Anchor" href="#mae"></a> MAE</h1>
<h2 id="核心原理-2"><a class="markdownIt-Anchor" href="#核心原理-2"></a> 核心原理：</h2>
<ol>
<li><strong>随机掩码（Masking）</strong>：
<ul>
<li>将输入图像划分为一系列 Patch（例如 16×1616 \times 1616×16），并随机<strong>遮掩</strong>一定比例的 Patch（如 75%）。</li>
<li>遮掩的部分从输入中移除，剩余的 Patch 作为输入特征。</li>
</ul>
</li>
<li><strong>编码器（Encoder）</strong>：
<ul>
<li>使用 ViT（Vision Transformer）作为编码器，仅处理未被遮掩的 Patch，提取高效的全局特征。</li>
</ul>
</li>
<li><strong>解码器（Decoder）</strong>：
<ul>
<li>一个轻量级的解码器负责重建被遮掩的 Patch。</li>
<li>解码器接受编码器的输出和位置编码，并尝试还原完整的图像。</li>
</ul>
</li>
<li><strong>重建目标（Reconstruction Loss）</strong>：
<ul>
<li>模型通过最小化重建误差（通常是均方误差，MSE）来优化，从而学习有意义的图像特征。</li>
</ul>
</li>
</ol>
<img src="https://pic2.zhimg.com/v2-e6a970e23f0b03371047a6014a25a175_1440w.jpg" alt="img" style="zoom: 50%;">
<h1 id="igpt"><a class="markdownIt-Anchor" href="#igpt"></a> IGPT</h1>
<h2 id="核心原理-3"><a class="markdownIt-Anchor" href="#核心原理-3"></a> 核心原理：</h2>
<ol>
<li><strong>图像像素序列化</strong>：
<ul>
<li>将二维图像<strong>展平为一维像素序列</strong>，类似于自然语言处理中的文本序列。</li>
<li>每个像素值被表示为一个离散的类别（如颜色索引或灰度值）。</li>
</ul>
</li>
<li><strong>Transformer 架构</strong>：
<ul>
<li><strong>使用标准的 Transformer 模型，将图像像素序列作为输入</strong>，学习序列中各像素值之间的依赖关系。</li>
<li>通过<strong>自回归方式</strong>预测下一个像素值，实现图像生成任务。</li>
</ul>
</li>
<li><strong>无监督学习</strong>：
<ul>
<li>模型通过最大化训练数据的对数似然估计进行优化，学习如何生成与训练图像分布一致的新图像。</li>
</ul>
</li>
</ol>
<img src="https://pic1.zhimg.com/v2-56d15a5487853b70a504d7e330f9d074_1440w.jpg" alt="img" style="zoom:33%;">
<h1 id="mobilevit"><a class="markdownIt-Anchor" href="#mobilevit"></a> MobileViT</h1>
<h2 id="空间归纳偏置"><a class="markdownIt-Anchor" href="#空间归纳偏置"></a> 空间归纳偏置</h2>
<h1 id="simclr"><a class="markdownIt-Anchor" href="#simclr"></a> SimCLR</h1>
<h1 id="detr"><a class="markdownIt-Anchor" href="#detr"></a> DETR</h1>
<h2 id="概述"><a class="markdownIt-Anchor" href="#概述"></a> 概述</h2>
<ul>
<li>
<p>最大的特点就是：不需要预定义的先验anchor，也不需要NMS的后处理策略，就可以实现<strong>端到端</strong>的目标检测。</p>
</li>
<li>
<p>但是，DETR大目标检测上性能是最好的，而小目标上稍差，而且基于match的loss导致学习很难收敛（即难以学习到最优的情况）。</p>
</li>
<li>
<p>DETR的总体框架如下，先通过CNN提取图像的特征；再送入到transformer encoder-decoder中，该编码器解码器的结构基本与transformer相同，主要是在输入部分和输出部分的修改；最后得到类别和bbox的预测，并通过二分匹配计算损失来优化网络。</p>
</li>
<li>
<p>利用全局特征之间的关系，来消除冗余的框</p>
</li>
</ul>
<img src="/2025/02/10/2025%E5%B9%B41%E6%9C%8815%E6%97%A5-Vit%E6%A8%A1%E5%9E%8B/image-20250210162328722.png" alt="image-20250210162328722" style="zoom:67%;">
<ol>
<li>第一步，用卷积神经网络抽取图像特征。</li>
<li>第二步，通过transformer获取图片全特征。</li>
<li>第三步。用transformer decoder来生成很多个框。</li>
<li>第四步。将预测出的框与ground truth的框进行matching 匹配的框计算loss。 Or在推理阶段第四部将是设置一个阀值。高于阀值则为目标，低于则为背景。</li>
</ol>
<h2 id="目标函数"><a class="markdownIt-Anchor" href="#目标函数"></a> 目标函数</h2>
<p>n个输出</p>
<h3 id="二分图匹配"><a class="markdownIt-Anchor" href="#二分图匹配"></a> 二分图匹配</h3>
<p>输出的一百个框移Ground truth。进行匹配。而不需要nms</p>
<h4 id="匈牙利算法"><a class="markdownIt-Anchor" href="#匈牙利算法"></a> 匈牙利算法</h4>
<img src="/2025/02/10/2025%E5%B9%B41%E6%9C%8815%E6%97%A5-Vit%E6%A8%A1%E5%9E%8B/image-20250210174950488.png" alt="image-20250210174950488" style="zoom: 67%;">
<blockquote>
<p>对应最擅长的任务(框的准确度和分类准确度)</p>
</blockquote>
<img src="/2025/02/10/2025%E5%B9%B41%E6%9C%8815%E6%97%A5-Vit%E6%A8%A1%E5%9E%8B/image-20250210175222465.png" alt="image-20250210175222465" style="zoom: 50%;">
<h2 id="网络"><a class="markdownIt-Anchor" href="#网络"></a> 网络</h2>
<img src="/2025/02/10/2025%E5%B9%B41%E6%9C%8815%E6%97%A5-Vit%E6%A8%A1%E5%9E%8B/image-20250210175430711.png" alt="image-20250210175430711" style="zoom:50%;">
<h3 id="object-quiry"><a class="markdownIt-Anchor" href="#object-quiry"></a> object quiry</h3>
<p>embedding</p>
<h1 id="proposal-anchor"><a class="markdownIt-Anchor" href="#proposal-anchor"></a> proposal anchor</h1>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://shakewely.github.io/2025/02/10/2025%E5%B9%B41%E6%9C%8815%E6%97%A5-Vit%E6%A8%A1%E5%9E%8B/" data-id="cm8q1tfhe000dpcv4804mfj9l" class="article-share-link">分享</a>
      
      
    </footer>
  </div>
  
</article>




  
    <article id="post-2025年2月10日-轻量化神经网络3" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2025/02/10/2025%E5%B9%B42%E6%9C%8810%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C3/" class="article-date">
  <time class="post-time" datetime="2025-02-10T03:40:45.000Z" itemprop="datePublished">
    <span class="post-month">2月</span><br/>
    <span class="post-day">10</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/02/10/2025%E5%B9%B42%E6%9C%8810%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C3/">2025年2月10日 轻量化神经网络3</a>
    </h1>
  

        <div>
          
          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="introduction"><a class="markdownIt-Anchor" href="#introduction"></a> <strong>Introduction</strong></h1>
<p>近期卷积网络的设计除了注重准确率之外，还需要兼顾运行性能，特别是在移动设备上的运行性能，这使得卷积神经网络的设计变得更加难，主要有以下难点：</p>
<ul>
<li>Intractable design space，由于卷积网络参数很多，导致设计空间十分复杂，目前很多方法提出自动化搜索，能够简化人工设计的流程，但这种方法一般需要大量的算力。</li>
<li>Nontransferable optimality，卷积网络的性能取决于很多因素，比如输入分辨率和目标设备，不同的分辨率需要调整不同的网络参数，而相同block在不同的设备上的效率也可能大不相同，所以需要对网络在特定的条件下进行特定的调优。</li>
<li>Inconsistent efficiency metrics，大多数效率指标不仅与网络结构相关，也和目标设备上的软硬件设置有关。为了简化，很多研究都采用硬件无关的指标来表示卷积的效率，比如FLOPs，但FLOPs并不能总等同于性能，还跟block的实现方式相关，这使得网络的设计更加困难。</li>
</ul>
<h1 id="nas"><a class="markdownIt-Anchor" href="#nas"></a> NAS</h1>
<h2 id="总结nas-发展历程"><a class="markdownIt-Anchor" href="#总结nas-发展历程"></a> <strong>总结：NAS 发展历程</strong></h2>
<table>
<thead>
<tr>
<th>阶段</th>
<th>方法</th>
<th>主要创新点</th>
<th>计算成本</th>
<th>代表性模型</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>早期 NAS</strong></td>
<td>强化学习 NAS</td>
<td>RNN 控制器搜索网络结构</td>
<td>高</td>
<td>NASNet</td>
</tr>
<tr>
<td><strong>进化 NAS</strong></td>
<td>进化算法 NAS</td>
<td>变异 + 交叉进化优化网络</td>
<td>高</td>
<td>AmoebaNet</td>
</tr>
<tr>
<td><strong>梯度优化 NAS</strong></td>
<td>DARTS</td>
<td>采用梯度下降进行高效搜索</td>
<td>低</td>
<td>DARTS、ProxylessNAS</td>
</tr>
<tr>
<td><strong>高效 NAS</strong></td>
<td>Efficient NAS</td>
<td>共享参数 + 复合缩放 + 硬件感知搜索</td>
<td>低</td>
<td>EfficientNet、FBNet、OFA</td>
</tr>
</tbody>
</table>
<h1 id="fbnet"><a class="markdownIt-Anchor" href="#fbnet"></a> FBNet</h1>
<img src="/2025/02/10/2025%E5%B9%B42%E6%9C%8810%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C3/image-20250219134545540.png" style="zoom:67%;">
<p>将 <strong>硬件性能约束</strong>（如浮点运算量 FLOPs、内存使用量、计算延迟等）引入到 NAS 的搜索过程，使得网络架构不仅能够实现高精度，还能在指定硬件上具有良好的执行效率。</p>
<p>BNet 的工作流程可以分为以下几个步骤：</p>
<ol>
<li><strong>硬件感知的目标函数</strong>：<br>
在设计过程中，FBNet 将目标硬件（如移动设备、嵌入式设备等）的约束（如内存、计算量等）纳入考虑。FBNet 的目标是最小化 <strong>计算资源的消耗</strong>，同时保证较高的 <strong>分类精度</strong>。这个目标函数是硬件感知的，旨在寻找在硬件上性能最优的网络架构。</li>
<li><strong>可微分架构搜索</strong>：<br>
FBNet 使用了一种基于梯度优化的搜索方法，即将 NAS 搜索过程转化为可微分的优化问题。这使得 FBNet 可以在较少的计算资源下快速找到高效的网络架构。具体来说，FBNet 在 <strong>超网络（SuperNet）</strong> 中进行架构搜索，超网络包含了多种候选网络结构，通过训练和优化选择最优的架构。</li>
<li><strong>选择模块</strong>：<br>
在 FBNet 中，网络结构的选择是模块化的，每个模块（如卷积、深度可分离卷积等）都有多个候选结构，而这些候选结构的选择在搜索过程中是通过<strong>可微分的路径选择</strong>来实现的。最终，FBNet 在候选模块中选择一个最优的结构。</li>
<li><strong>搜索策略</strong>：<br>
为了加速搜索过程，FBNet 使用了 <strong>混合搜索策略</strong>，即结合了 <strong>强化学习</strong> 和 <strong>梯度优化</strong> 两种方法。强化学习用于生成架构的候选方案，而梯度优化则用于对这些方案进行优化。通过这种方式，FBNet 能够在复杂的搜索空间中找到最优的架构。</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://shakewely.github.io/2025/02/10/2025%E5%B9%B42%E6%9C%8810%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C3/" data-id="cm8q1tfhg000hpcv4f8qg1hic" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E3%80%81model/" rel="tag">轻量化神经网络、model</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-2025年2月9日-轻量化神经网络2" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/" class="article-date">
  <time class="post-time" datetime="2025-02-09T07:21:36.000Z" itemprop="datePublished">
    <span class="post-month">2月</span><br/>
    <span class="post-day">09</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/">2025年2月9日 轻量化神经网络2</a>
    </h1>
  

        <div>
          
          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="lcnet"><a class="markdownIt-Anchor" href="#lcnet"></a> LCNet</h1>
<blockquote>
<p>CPU端的最强轻量型架构</p>
</blockquote>
<img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250219133119899.png" alt="image-20250219133119899" style="zoom:33%;">
<h3 id="主要解决方案"><a class="markdownIt-Anchor" href="#主要解决方案"></a> 主要解决方案：</h3>
<ol>
<li>采用H-Swish作为激活函数，性能大幅提升，而推理速度几乎不变。</li>
<li>在网络合适的位置添加少量的SE模块可以进一步提升模型性能；实验表明：当把SE置于模型的尾部时，它具有更好的效果。因此，我们仅将SE模块添加到接近网络尾部的模块即可, 这种处理方式具有更好的精度-速度平衡。</li>
<li>根据MixNet的实验论证结果:在一定范围内大的卷积核可以提升模型的性能，但是超过这个范围会有损模型的性能。本文通过实验总结了一些更大的卷积核在不同位置的作用，类似SE模块的位置，更大的卷积核在网络的中后部作用更明显。比如5x5卷积。</li>
<li>GAP后使用更大的1x1卷积层；在GoogLeNet之后，GAP（Global-Average-Pooling）后往往直接接分类层，但是在轻量级网络中，这样会导致GAP后提取的特征没有得到进一步的融合和加工。如果在此后使用一个更大的1x1卷积层（等同于FC层），GAP后的特征便不会直接经过分类层，而是先进行了融合，并将融合的特征进行分类。这样可以在不影响模型推理速度的同时大大提升准确率。</li>
<li>使用dropout技术可以进一步提升模型的精度。</li>
</ol>
<p><strong>低秩卷积的引入</strong>：</p>
<ul>
<li>低秩卷积的引入是 LCNet 的最大创新之一。通过矩阵低秩分解，LCNet 能够将卷积操作的计算复杂度降到最低，从而显著减少了卷积神经网络的计算负担和内存消耗。</li>
</ul>
<p><strong>高效的通道剪枝</strong>：</p>
<ul>
<li>LCNet 提出了通道剪枝的概念，即对卷积层中的不重要通道进行剪枝，减少冗余计算。这个过程通过分析每个通道的贡献，剔除那些对最终输出没有显著影响的通道，从而大幅减小模型的规模。</li>
</ul>
<p><strong>集成深度可分离卷积</strong>：</p>
<ul>
<li>深度可分离卷积已经被证明是提升计算效率的一种有效手段，LCNet 将其应用到各个卷积层中，通过拆解卷积操作来减少计算量和参数量，提高了模型的运行效率。</li>
</ul>
<p><strong>硬件友好设计</strong>：</p>
<ul>
<li>LCNet 强调了硬件友好的设计理念，特别是在移动设备和嵌入式设备上。网络架构通过采用低计算量的卷积和模块化设计，使得 LCNet 在计算资源受限的设备上表现更好。</li>
</ul>
<p><strong>优化的注意力机制</strong>：</p>
<ul>
<li>LCNet 在设计中还结合了高效的注意力机制，通过优化网络对输入信息的加权方式，使得网络能够更好地关注到重要特征，从而提升分类性能，同时保持较低的计算开销。</li>
</ul>
<h1 id="squeezenet"><a class="markdownIt-Anchor" href="#squeezenet"></a> SqueezeNet</h1>
<h2 id="fire-module"><a class="markdownIt-Anchor" href="#fire-module"></a> Fire Module</h2>
<p><strong>Fire Module</strong>：每个 Fire Module 由两个主要部分组成：</p>
<ul>
<li><strong>Squeeze层</strong>：一个1x1的卷积层，用来减少特征图的深度。</li>
<li><strong>Expand层</strong>：两个分支，一个是1x1卷积，另一个是3x3卷积，用来增加特征图的深度。</li>
</ul>
<p>通过这种结构，SqueezeNet能够有效地减少参数量，而不牺牲太多的准确度。</p>
<p><strong>减少全连接层的参数</strong>：SqueezeNet中没有传统的全连接层，而是使用全局平均池化（Global Average Pooling）来替代。这样大大减少了参数数量，且仍然保留了模型的表现力。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cat([x1, x2], 1)</span><br></pre></td></tr></table></figure>
<h1 id="mobilenethttpsblogcsdnnetqq_37555071articledetails108393809"><a class="markdownIt-Anchor" href="#mobilenethttpsblogcsdnnetqq_37555071articledetails108393809"></a> [MobileNet][<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_37555071/article/details/108393809">https://blog.csdn.net/qq_37555071/article/details/108393809</a>]</h1>
<h2 id="深度可分离卷积depthwise-separable-convolutions"><a class="markdownIt-Anchor" href="#深度可分离卷积depthwise-separable-convolutions"></a> 深度可分离卷积(Depthwise Separable Convolutions)</h2>
<p>（Depthwise Separable Convolutions）</p>
<p><strong>深度卷积（Depthwise Convolution）</strong>：对每个输入通道独立进行卷积操作，而不是像传统卷积那样对所有通道进行卷积。这大大减少了计算量。</p>
<p><strong>逐点卷积（Pointwise Convolution）</strong>：即1x1卷积，用于将深度卷积的输出进行线性组合，融合通道信息。</p>
<p><strong>宽度和分辨率的可调性</strong>： MobileNet引入了两个超参数，分别是<strong>宽度系数（Width Multiplier）**和**分辨率系数（Resolution Multiplier）</strong>，使得网络的大小和计算量可以根据实际需求进行调整。</p>
<ul>
<li><strong>宽度系数（α）</strong>：用于控制网络每一层的通道数。通过减小α的值，可以降低网络的复杂度和参数数量。</li>
<li><strong>分辨率系数（ρ）</strong>：用于控制输入图像的分辨率，通过减小分辨率来减少计算量。</li>
</ul>
<p><img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209162740618.png" alt="image-20250209162740618"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=<span class="number">3</span>, stride=stride, padding=<span class="number">1</span>, groups=in_channels, bias=<span class="literal">False</span>)</span><br><span class="line">// groups=in_channels</span><br></pre></td></tr></table></figure>
<h1 id="shufflenet"><a class="markdownIt-Anchor" href="#shufflenet"></a> ShuffleNet</h1>
<p><strong>深度可分离卷积（Depthwise Separable Convolution） 和</strong></p>
<h2 id="通道混洗channel-shuffle"><a class="markdownIt-Anchor" href="#通道混洗channel-shuffle"></a> <strong>通道混洗（Channel Shuffle）</strong></h2>
<p>通道混洗技术会在<strong>每个卷积层的输出中打乱通道的顺序</strong>，使得不同通道的特征能够进行更好的融合，从而提高模型的表达能力和准确率。</p>
<h2 id="分组卷积grouped-convolution"><a class="markdownIt-Anchor" href="#分组卷积grouped-convolution"></a> <strong>分组卷积（Grouped Convolution）</strong></h2>
<p>为了进一步提高网络的计算效率，ShuffleNet 采用了 <strong>分组卷积（Grouped Convolution）</strong>。分组卷积将输入通道分成多个组，每个组内的通道与一个独立的卷积核进行卷积，从而减少了卷积运算的计算量。</p>
<h1 id="googlenet"><a class="markdownIt-Anchor" href="#googlenet"></a> GoogLeNet</h1>
<h2 id="inception块"><a class="markdownIt-Anchor" href="#inception块"></a> Inception块</h2>
<p>各种模块全都要</p>
<img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209172824970.png" alt="image-20250209172824970" style="zoom:50%;">
<img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209172918099.png" alt="image-20250209172918099" style="zoom:67%;">
<h2 id="xception-块"><a class="markdownIt-Anchor" href="#xception-块"></a> <strong>Xception 块</strong></h2>
<p>极限情况</p>
<h1 id="efficientnet"><a class="markdownIt-Anchor" href="#efficientnet"></a> EfficientNet</h1>
<p><img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209173848541.png" alt="image-20250209173848541" style="zoom: 80%;"><img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209174104028.png" alt="image-20250209174104028" style="zoom: 67%;"></p>
<h2 id="原理"><a class="markdownIt-Anchor" href="#原理"></a> 原理</h2>
<h3 id="网络架构搜索nas"><a class="markdownIt-Anchor" href="#网络架构搜索nas"></a> <strong>网络架构搜索（NAS）</strong>：</h3>
<ul>
<li>EfficientNet 使用了 <strong>神经架构搜索</strong>（NAS）来自动化地找到最适合的卷积神经网络架构。在这个过程中，作者通过 NAS 来寻找一个 <strong>基础架构</strong>，该架构在计算效率和性能之间取得了最佳的平衡。</li>
</ul>
<h3 id="优化网络的宽度-深度和分辨率"><a class="markdownIt-Anchor" href="#优化网络的宽度-深度和分辨率"></a> <strong>优化网络的宽度、深度和分辨率</strong>：</h3>
<ul>
<li>EfficientNet 在进行网络扩展时，并不是简单地增加网络的层数或通道数，而是采用了更加 <strong>均衡的增长策略</strong>。通过在深度、宽度和输入图像分辨率上都进行扩展，模型能够获得更强的表达能力，同时保持较低的计算成本。</li>
</ul>
<h3 id="高效的卷积操作"><a class="markdownIt-Anchor" href="#高效的卷积操作"></a> <strong>高效的卷积操作</strong>：</h3>
<ul>
<li>EfficientNet 采用了高效的卷积操作，如 <strong>Depthwise Separable Convolution</strong>，以进一步减少计算量。</li>
</ul>
<p>通过结构搜索（NAS, Neural Architecture Search）和优化策略，在精度和计算效率之间找到最好的平衡。</p>
<h2 id="mbconvefficientnet块"><a class="markdownIt-Anchor" href="#mbconvefficientnet块"></a> MBConv/EfficientNet块</h2>
<p><img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209180216944.png" alt="image-20250209180216944"></p>
<p><img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209174529781.png" alt="image-20250209174529781"></p>
<h3 id="倒残差结构inverted-residuals"><a class="markdownIt-Anchor" href="#倒残差结构inverted-residuals"></a> 倒残差结构（Inverted Residuals)</h3>
<p><img src="https://pic2.zhimg.com/v2-7c42cff2fa3c346d2e41be95848fc619_1440w.jpg" alt="img"></p>
<blockquote>
<p>使用逐通道卷积和逐点卷积来提高计算效率。</p>
</blockquote>
<h2 id="注意力机制"><a class="markdownIt-Anchor" href="#注意力机制"></a> 注意力机制</h2>
<h3 id="se模块squeeze-and-excitation-block"><a class="markdownIt-Anchor" href="#se模块squeeze-and-excitation-block"></a> SE模块（Squeeze-and-Excitation Block）</h3>
<p>来源于人类视觉系统中的注意机制：大脑会根据不同的视觉刺激，<strong>自动聚焦于最重要的信息，并忽略不相关的部分</strong>。SE模块通过类似的机制，<strong>自动为每个通道分配不同的重要性</strong>，从而增强模型对重要特征的敏感度。</p>
<ul>
<li>
<p><strong>queeze-and-Excitation</strong>（SE）模块，提升了特征通道之间的依赖关系.仅在 <strong>通道维度</strong> 上进行加权</p>
</li>
<li>
<p>通过一个 <strong>“Squeeze”</strong> 操作来压缩空间维度信息，再通过 <strong>“Excitation”</strong> 操作生成通道权重</p>
</li>
</ul>
<p><strong>Squeeze（压缩）</strong>：</p>
<ul>
<li>输入是一个 <strong>H×W×C</strong> 的特征图，其中 H和 W 分别是空间维度的高度和宽度，C 是通道数。</li>
<li><strong>通过全局平均池化</strong>（Global Average Pooling）对每个通道进行压缩。即对每个通道的空间维度（H×W）求平均，得到一个 <strong>C</strong> 维的向量，表示每个通道的“全局特征”。</li>
</ul>
<p><strong>Excitation（激励）</strong>：</p>
<ul>
<li>对通道的全局特征向量进行<strong>两层全连接层操作</strong>，其中第二层是激活函数（通常是 <strong>Sigmoid</strong>），生成每个通道的 <strong>注意力系数</strong>。</li>
<li>第一个全连接层是 <strong>瓶颈层（bottleneck layer）</strong>，通常通过降低维度来减少计算量。然后通过第二个全连接层，将输出恢复到原始通道数，得到每个通道的权重。</li>
</ul>
<img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209174719710.png" alt="image-20250209174719710" style="zoom:67%;">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_make_se_block</span>(<span class="params">self, channels, se_ratio</span>):</span><br><span class="line">        reduction = <span class="built_in">int</span>(channels * se_ratio)</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">            nn.AdaptiveAvgPool2d(<span class="number">1</span>),</span><br><span class="line">            nn.Conv2d(channels, reduction, kernel_size=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(reduction, channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">            nn.Sigmoid()</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<h4 id="空间注意力机制"><a class="markdownIt-Anchor" href="#空间注意力机制"></a> 空间注意力机制</h4>
<p>对特征图的<strong>每个位置</strong>分配一个权重来决定哪些空间区域应该被网络更多关注</p>
<blockquote>
<p>人类的视觉注意力更多的是<strong>空间注意力</strong>，也就是对图片上不同区域赋予不同的权重</p>
</blockquote>
<img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209183640934.png" alt="image-20250209183640934" style="zoom:67%;">
<p><img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250303211827687.png" alt="image-20250303211827687"></p>
<h4 id="通道注意力机制"><a class="markdownIt-Anchor" href="#通道注意力机制"></a> 通道注意力机制</h4>
<p><img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250303211752599.png" alt="image-20250303211752599"></p>
<h4 id="cbam注意力"><a class="markdownIt-Anchor" href="#cbam注意力"></a> CBAM注意力</h4>
<p>CBAM 通过融合 <strong>通道注意力机制</strong> 和 <strong>空间注意力机制</strong></p>
<h4 id="视觉自注意力non-local"><a class="markdownIt-Anchor" href="#视觉自注意力non-local"></a> 视觉自注意力(Non Local))</h4>
<p>用来在不引入过多计算量的基础上提高CNN网络的<strong>远程依赖</strong></p>
<p><img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209184403579.png" alt="image-20250209184403579"></p>
<blockquote>
<p>我可能先识别到篮球，然后在其周围找到人和篮筐，并根据他们的位置我们才能判断这个图是不是表达人在灌篮这个动作。其中，篮球位置可以理解为<strong>查询点</strong>，周边（可能离得较远）的人和篮筐就是<strong>查询点对应的关联区域</strong>。</p>
</blockquote>
<p>查询点到关联区域的对应可以加深CNN网络对场景<strong>从局部到整体</strong>的理解，因此可以有效提高CNN网络在视觉任务的效率。</p>
<ul>
<li>
<p>该模块建立了图像中<strong>每个像素/区域之间的关联</strong>，有效提升了CNN网络的感受野</p>
</li>
<li>
<p>对每个空间位置生成与所有其他位置的<strong>相似度</strong>。</p>
</li>
<li>
<p>基于<strong>相似度进行加权</strong>，使得每个位置可以融合其他位置的信息。</p>
</li>
</ul>
<p><img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209184638406.png" alt="image-20250209184638406"></p>
<blockquote>
<p>其大概的运算过程是：</p>
<p>（1）为了降低运算量，采用三个不同1x1卷积层进行维度减半，即图上的 x→ϕ(x),θ(x),g(x) ;</p>
<p>（2）为 ϕ(x),θ(x),g(x) 按照w,h维度进行铺平，即图上三个flatten；</p>
<p>（3）利用铺平的 ϕ(x),θ(x) 进行矩阵乘法运算，并通过softmax获得各空间位置之间的<strong>关联图</strong>（即图中的 vc ），很明显这个关联图的大小为 (w×h,w×h) ，表征着各个像素点（区域）之间的联系；</p>
<p>（4）将铺平转置的 g(x) 与vc进行矩阵乘法，获得 y ；</p>
<p>（5）对y进行展开与特征提取（Conv4），获得注意力（refined）;</p>
<p>（6）利用注意力调整原始输入的分布，Over!!!</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/350760243">https://zhuanlan.zhihu.com/p/350760243</a></p>
</blockquote>
<h4 id="non-local改进版-gcnet"><a class="markdownIt-Anchor" href="#non-local改进版-gcnet"></a> Non Local改进版 — GCNet</h4>
<p>原因:<strong>不同查询点（区域）居然对应相同的attention map</strong></p>
<img src="https://pic4.zhimg.com/v2-31d05dfac01c6ae64bd5bf4e52ffc069_1440w.jpg" alt="img" style="zoom: 33%;">
<p>与查询（点）无关的依赖（query-independent dependency）。那么这是否意味着原始Non Local中<strong>query分支可以剪去不要</strong>呢</p>
<img src="https://pic4.zhimg.com/v2-dc2339ac78de452e286317dd259070f5_1440w.jpg" alt="img" style="zoom: 50%;">
<img src="https://pic3.zhimg.com/v2-104058d724746316b12298ec27fafe26_1440w.jpg" alt="img" style="zoom:50%;">
<h3 id="dfc-注意力模块"><a class="markdownIt-Anchor" href="#dfc-注意力模块"></a> DFC 注意力模块</h3>
<p>利用固定权重的全连接层来创建具有全局感受野的注意力图，以捕 获长距离空间位置的依赖关系</p>
<h1 id="efficientdet"><a class="markdownIt-Anchor" href="#efficientdet"></a> EfficientDet</h1>
<h2 id="结构概述"><a class="markdownIt-Anchor" href="#结构概述"></a> <strong>结构概述</strong></h2>
<h3 id="1-backboneefficientnet"><a class="markdownIt-Anchor" href="#1-backboneefficientnet"></a> 1 Backbone（EfficientNet）</h3>
<ul>
<li>EfficientDet 的 backbone 使用了 EfficientNet 作为特征提取器。EfficientNet 是一种使用<strong>复合缩放</strong>策略的高效网络，在目标检测任务中，EfficientDet 对 EfficientNet 进行调整和优化，使得其能更好地适应目标检测的要求。</li>
</ul>
<p>BiFPN（双向特征金字塔网络）**</p>
<ul>
<li>BiFPN 通过对低层和高层特征的加权融合，使得低层和高层的特征能够互相补充，提升了多尺度特征的利用率。</li>
</ul>
<h3 id="2-head"><a class="markdownIt-Anchor" href="#2-head"></a> 2 Head</h3>
<ul>
<li><strong>分类头</strong>：用于对目标进行分类，预测每个框的类别。</li>
<li><strong>回归头</strong>：用于回归目标的边界框坐标。</li>
</ul>
<h3 id="3-复合缩放"><a class="markdownIt-Anchor" href="#3-复合缩放"></a> 3 复合缩放</h3>
<ul>
<li>EfficientDet 通过复合缩放策略调整模型的深度、宽度和输入分辨率，使得模型可以在不同的硬件环境下进行灵活的调整，同时提升了精度和效率。</li>
</ul>
<h2 id="fpn到bifpn"><a class="markdownIt-Anchor" href="#fpn到bifpn"></a> FPN.到BiFPN</h2>
<img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209190732084.png" style="zoom: 80%;">
<img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209224653761.png" style="zoom: 80%;">
<blockquote>
<p>a. 网络只是针对某一特点的分辨率进行训练, 如果只是在测试和推理阶段使用图像金字塔的话, 可能导致训练和测试推理过程不匹配</p>
<p>b. 利用单个高层特征图(主干网络产生)进行物体的分类和bounding box的回归</p>
<p>c. 在 <strong>多个不同尺度的特征图（如 38×38、19×19、10×10）</strong> 上直接预测目标框。</p>
<p>d 尽管在SSD中我们已经使用了特征金字塔, 但该金字塔中的所有要素都处于不同的比例, 并且由于网络中层的深度不同而存在巨大的语义鸿沟. 高分辨率地图具有低级语义特征, 而低分辨率地图具有较高的语义特征, 这会损害其对象识别的表示能力.</p>
</blockquote>
<img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209223353104.png" alt="image-20250209223353104" style="zoom:50%;">
<blockquote>
<p>先对高阶特征进行上采样, 然后使用横向连接将其与低阶特征进行组合, 该横向连接基本上是1x1卷积, 然后进行求和,</p>
</blockquote>
<h3 id="fully-connected-fpn-and-nas-fpn"><a class="markdownIt-Anchor" href="#fully-connected-fpn-and-nas-fpn"></a> Fully connected FPN and NAS-FPN</h3>
<p><img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209224903212.png" alt="image-20250209224903212"></p>
<blockquote>
<p>a. 传统fpn</p>
<p>b. 全连接网络</p>
<p>c. NAS</p>
</blockquote>
<h3 id="panet"><a class="markdownIt-Anchor" href="#panet"></a> PANet</h3>
<p><img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209224555819.png" alt="image-20250209224555819"></p>
<blockquote>
<p>添加<strong>自下而上的路径</strong>以增强FPN中的自上而下的路径</p>
</blockquote>
<h3 id="simplified-fpn-and-bifpn"><a class="markdownIt-Anchor" href="#simplified-fpn-and-bifpn"></a> Simplified FPN and BiFPN</h3>
<p><img src="https://pic4.zhimg.com/v2-faec285a27615d8829af43172d9d1385_1440w.jpg" alt="img"></p>
<blockquote>
<p>a. 如果一个节点只有一个输入边并且没有特征融合, 那么它对特征网络的融合贡献较小, 这个节点可以删除(Simplified PANET)</p>
<p>b. 添加一条额外的连接路径,融合更多功能(BiFPN). 这点其实跟skip connection很相似</p>
<p>c. 重复叠加相同的特征网络层 复合缩放</p>
</blockquote>
<h2 id="权重计算"><a class="markdownIt-Anchor" href="#权重计算"></a> 权重计算</h2>
<p>快速归一化融合特征网络.计算路径,计算出不同特征节点的输入最合适的权值</p>
<p><img src="https://pic4.zhimg.com/v2-a0f1319eae0571829c79b1a70d4fc1b9_1440w.jpg" alt="img"></p>
<h2 id="compound-scaling"><a class="markdownIt-Anchor" href="#compound-scaling"></a> Compound Scaling</h2>
<p>复合缩放的目标是在任何给定的资源约束下最大化模型精度, 因此可以表述为优化问题.</p>
<p><strong>对网络深度、宽度和分辨率中的任何尺度进行缩放都可以提高精度, 但是当模型足够大时, 这种放大的收益会减弱。</strong></p>
<p><strong>FLOPS</strong>是floating point operations per second的缩写, 意指每秒浮点运算次数, 理解为计算速度. 是一个衡量硬件性能的指标. 我们假设我们能使用的FLOPS是2.</p>
<p><img src="https://pic2.zhimg.com/v2-032dfea7e52345c3f26a543e3dbcc6fd_1440w.jpg" alt="img"></p>
<blockquote>
<p>a. 对于网络模型depth来说, 加倍深度会使得FLOPS加倍.</p>
</blockquote>
<p><img src="https://pic1.zhimg.com/v2-4f1414b59ea6700387f0dea2dac0d878_1440w.jpg" alt="img"></p>
<blockquote>
<p>b. 对于网络模型width来说, 由于width(#channel)的增加导致卷积计算的路径平方级增加, 因此加倍宽度会使得FLOPS加4倍.</p>
</blockquote>
<p><img src="https://pica.zhimg.com/v2-646a538d15c17d28145024953ff90eda_1440w.jpg" alt="img"></p>
<blockquote>
<p>c. 对于网络模型resolution来说, 和width的情况一样, 由于resolution的增加会导致feather map呈现平方级扩张, 因此加倍图像分辨率也会使得FLOPS加4倍.</p>
</blockquote>
<p><strong>复合缩放公式:</strong></p>
<p><img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209230157789.png" alt="image-20250209230157789"></p>
<p><strong>网格搜索</strong></p>
<p><img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209230342459.png" alt></p>
<p><strong>复合缩放的总结图:</strong></p>
<p><img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250209231222947.png" alt="image-20250209231222947"></p>
<h1 id="ghostnet"><a class="markdownIt-Anchor" href="#ghostnet"></a> GhostNet</h1>
<p>特征图存在<strong>冗余性</strong> 廉价操作 生成 Ghost 特征图</p>
<h2 id="原理-2"><a class="markdownIt-Anchor" href="#原理-2"></a> 原理</h2>
<h2 id="ghost-卷积"><a class="markdownIt-Anchor" href="#ghost-卷积"></a> Ghost 卷积</h2>
<p>利用计算量较低的线性操作来增加特征图的数量， 从而在保持输出特征图不变的同时，显著地降低了计算复杂度</p>
<ol>
<li>使用1×1卷积对输入特征进行压缩，以获取少量特征图</li>
<li>这部分特征图通过线性变换生成另一部分特征图</li>
<li>将两部分特征图进行拼接，形 成最终的输出特征图</li>
</ol>
<img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250303184803366.png" alt="image-20250303184803366" style="zoom:67%;">
<h1 id="mobilevit"><a class="markdownIt-Anchor" href="#mobilevit"></a> MobileViT</h1>
<p><strong>融合 CNN 和 Transformer</strong>：通过将卷积操作与 Transformer 结合，模型能够同时捕捉局部和全局特征，提高了特征表达能力。</p>
<p><strong>高效的特征处理</strong>：通过<strong>展开和折叠操作</strong>(unfold )，将特征映射到序列空间进行处理，然后再映射回原始空间，实现了高效的特征处理。</p>
<p><strong>模型图：</strong></p>
<img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250210155707027.png" alt="image-20250210155707027" style="zoom:50%;">
<h2 id="mobile-vit-块"><a class="markdownIt-Anchor" href="#mobile-vit-块"></a> Mobile ViT 块</h2>
<img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250210155825206.png" alt="image-20250210155825206" style="zoom: 50%;">
<blockquote>
<p>…</p>
</blockquote>
<p>减少self-attention</p>
<img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250210160135377.png" alt="image-20250210160135377" style="zoom:50%;"> 
<blockquote>
<p>冗余信息多,相邻token信息差异小</p>
</blockquote>
<h3 id="fold"><a class="markdownIt-Anchor" href="#fold"></a> fold</h3>
<img src="/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/image-20250210160342687.png" alt="image-20250210160342687" style="zoom:33%;">
<h1 id="emo结合-cnn-和-transformerhttpsdeveloperaliyuncomarticle1210407"><a class="markdownIt-Anchor" href="#emo结合-cnn-和-transformerhttpsdeveloperaliyuncomarticle1210407"></a> [EMO：结合 CNN 和 Transformer][<a target="_blank" rel="noopener" href="https://developer.aliyun.com/article/1210407">https://developer.aliyun.com/article/1210407</a>]</h1>
<h3 id="1-特征提取网络sd-ghostnet"><a class="markdownIt-Anchor" href="#1-特征提取网络sd-ghostnet"></a> 1. 特征提取网络（SD-Ghostnet）</h3>
<ul>
<li><strong>输入图像</strong>：原始图像作为输入。</li>
<li><strong>Conv(64,6,2,2)</strong>：首先通过一个卷积层，输出通道数为64，核大小为6x6，步长为2，填充为2。</li>
<li><strong>S-GhostConv (128,3,2)</strong> 和 <strong>SD-Ghost(128)</strong>：经过一系列的S-GhostConv和SD-Ghost模块，逐步增加通道数到128。</li>
<li><strong>S-GhostConv (256,3,2)</strong> 和 <strong>SD-Ghost(256)</strong>：进一步通过S-GhostConv和SD-Ghost模块，通道数增加到256。</li>
<li><strong>S-GhostConv (512,3,2)</strong> 和 <strong>SD-Ghost(512)</strong>：继续通过S-GhostConv和SD-Ghost模块，通道数增加到512。</li>
<li><strong>S-GhostConv (1024,3,2)</strong> 和 <strong>SD-Ghost(1024)</strong>：最后通过S-GhostConv和SD-Ghost模块，通道数增加到1024。</li>
<li><strong>SPPF(1024,5)</strong>：空间金字塔池化模块，进一步增强特征表示。</li>
</ul>
<h3 id="2-特征融合网络ffn"><a class="markdownIt-Anchor" href="#2-特征融合网络ffn"></a> 2. 特征融合网络（FFN）</h3>
<ul>
<li><strong>Concat(512)</strong>：将特征提取网络中不同层次的特征进行拼接，形成512通道的特征图。</li>
<li><strong>Upsample(256)</strong>：对特征图进行上采样，通道数减少到256。</li>
<li><strong>GSCovn (256,1,1)</strong>：通过一个1x1的卷积层调整通道数。</li>
<li><strong>VOVGSCSP (256)</strong>：通过VOVGSCSP模块进一步处理特征。</li>
<li><strong>GSCovn (256,3,2)</strong>：通过一个1x1的卷积层调整通道数。</li>
<li><strong>Concat(512)</strong>：再次拼接特征，形成512通道的特征图。</li>
<li><strong>VOVGSCSP (512)</strong>：通过VOVGSCSP模块进一步处理特征。</li>
<li><strong>GSCovn (512,3,2)</strong>：通过一个1x1的卷积层调整通道数。</li>
<li><strong>Concat(1024)</strong>：再次拼接特征，形成1024通道的特征图。</li>
<li><strong>VOVGSCSP (1024)</strong>：通过VOVGSCSP模块进一步处理特征。</li>
</ul>
<h3 id="3-多尺度检测网络mdn"><a class="markdownIt-Anchor" href="#3-多尺度检测网络mdn"></a> 3. 多尺度检测网络（MDN）</h3>
<ul>
<li><strong>CBAM (256)</strong>：通过CBAM注意力机制模块，增强特征表示。</li>
<li><strong>Conv2d (256)</strong>：通过一个卷积层，输出通道数为256。</li>
<li><strong>CBAM (512)</strong>：通过CBAM注意力机制模块，增强特征表示。</li>
<li><strong>Conv2d (512)</strong>：通过一个卷积层，输出通道数为512。</li>
<li><strong>CBAM (1024)</strong>：通过CBAM注意力机制模块，增强特征表示。</li>
<li><strong>Conv2d (1024)</strong>：通过一个卷积层，输出通道数为1024。</li>
<li><strong>输出图像</strong>：最终生成输出图像。</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://shakewely.github.io/2025/02/09/2025%E5%B9%B42%E6%9C%889%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2/" data-id="cm6yfcd2b0000mcv4hi073hzf" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E3%80%81model/" rel="tag">轻量化神经网络、model</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-2025年2月5日-轻量化神经网络" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2025/02/05/2025%E5%B9%B42%E6%9C%885%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="article-date">
  <time class="post-time" datetime="2025-02-05T08:28:10.000Z" itemprop="datePublished">
    <span class="post-month">2月</span><br/>
    <span class="post-day">05</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/02/05/2025%E5%B9%B42%E6%9C%885%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">2025年2月5日 轻量化神经网络1</a>
    </h1>
  

        <div>
          
          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="简介"><a class="markdownIt-Anchor" href="#简介"></a> 简介</h1>
<p><img src="/2025/02/05/2025%E5%B9%B42%E6%9C%885%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20250205162820154.png" alt="image-20250205162820154"></p>
<h1 id="知识蒸馏"><a class="markdownIt-Anchor" href="#知识蒸馏"></a> 知识蒸馏</h1>
<h2 id="soft-target"><a class="markdownIt-Anchor" href="#soft-target"></a> soft target</h2>
<img src="/2025/02/05/2025%E5%B9%B42%E6%9C%885%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20250205163735260.png" alt="image-20250205163735260" style="zoom:33%;">
<blockquote>
<p>更多信息+</p>
</blockquote>
<h4 id="蒸馏温度"><a class="markdownIt-Anchor" href="#蒸馏温度"></a> 蒸馏温度</h4>
<img src="/2025/02/05/2025%E5%B9%B42%E6%9C%885%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20250205164432943.png" alt="image-20250205164432943" style="zoom:50%;">
<img src="/2025/02/05/2025%E5%B9%B42%E6%9C%885%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20250205164308480.png" alt="image-20250205164308480" style="zoom:67%;">
<h4 id="训练"><a class="markdownIt-Anchor" href="#训练"></a> 训练</h4>
<img src="/2025/02/05/2025%E5%B9%B42%E6%9C%885%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20250205164454289.png" alt="image-20250205164454289" style="zoom:33%;">
<h1 id="量化"><a class="markdownIt-Anchor" href="#量化"></a> 量化</h1>
<h1 id="剪枝"><a class="markdownIt-Anchor" href="#剪枝"></a> 剪枝</h1>
<h2 id="定义"><a class="markdownIt-Anchor" href="#定义"></a> 定义</h2>
<h3 id="剪枝方法总结表"><a class="markdownIt-Anchor" href="#剪枝方法总结表"></a> <strong>剪枝方法总结表</strong></h3>
<table>
<thead>
<tr>
<th>剪枝方法</th>
<th>主要剪枝对象</th>
<th>优点</th>
<th>缺点</th>
<th>示例方法/应用</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>权重剪枝（Weight Pruning）</strong></td>
<td>单个权重</td>
<td>剪枝率高，可应用于各种网络</td>
<td>需要稀疏矩阵优化，不适合硬件加速</td>
<td>TensorFlow <code>tf.sparsity</code>, PyTorch <code>torch.nn.utils.prune</code></td>
</tr>
<tr>
<td><strong>结构化剪枝（Structured Pruning）</strong></td>
<td>神经元、通道、卷积核</td>
<td>适合硬件加速</td>
<td>剪枝率受限，可能影响模型结构</td>
<td><code>Channel Pruning</code>, <code>Filter Pruning</code></td>
</tr>
<tr>
<td><strong>低秩分解剪枝（Low-Rank Approximation）</strong></td>
<td>整体权重矩阵</td>
<td>计算加速明显</td>
<td>需要额外的分解计算</td>
<td>SVD 分解, CP 分解, Tensor Train 分解</td>
</tr>
<tr>
<td><strong>剪枝 + 训练（Prune and Fine-tune）</strong></td>
<td>结合剪枝和微调</td>
<td>可恢复精度</td>
<td>训练时间增加</td>
<td>迭代剪枝（Iterative Pruning）, 一次性剪枝（One-shot Pruning）</td>
</tr>
<tr>
<td><strong>软剪枝（Soft Pruning）</strong></td>
<td>权重</td>
<td>更温和的剪枝方式</td>
<td>需要更多训练步骤</td>
<td>逐步缩小权重（Weight Decay），平滑剪枝（Gradual Magnitude Pruning）</td>
</tr>
<tr>
<td><strong>剪枝 + 蒸馏（Pruning with Distillation）</strong></td>
<td>剪枝后蒸馏</td>
<td>精度损失小</td>
<td>需要额外教师模型</td>
<td><code>Knowledge Distillation (KD)</code>, MobileBERT, TinyBERT</td>
</tr>
<tr>
<td><strong>幸运票剪枝（Lottery Ticket Hypothesis）</strong></td>
<td>子网络</td>
<td>保留重要子结构</td>
<td>训练步骤复杂</td>
<td>训练大模型后剪枝，重新初始化训练</td>
</tr>
<tr>
<td><strong>正则化剪枝（Regularization-based Pruning）</strong></td>
<td>L1/L2 约束</td>
<td>无需额外剪枝步骤</td>
<td>训练需额外超参数</td>
<td><code>L1 Regularization</code>, <code>Group Lasso Pruning</code></td>
</tr>
<tr>
<td><strong>动态剪枝（Movement Pruning）</strong></td>
<td>Transformer 模型</td>
<td>适合 NLP</td>
<td>计算复杂度高</td>
<td><code>BERT Pruning</code>, <code>MobileBERT</code></td>
</tr>
<tr>
<td><strong>自动剪枝（AutoML Pruning）</strong></td>
<td>NAS/强化学习</td>
<td>自动优化</td>
<td>计算成本高</td>
<td><code>AMC (AutoML for Model Compression)</code>, <code>Meta-Pruning</code></td>
</tr>
</tbody>
</table>
<img src="https://ucc.alicdn.com/yysinyik4knec/developer-article1644450/20241207/de25bb1591524da481677d9008ecc078.png?x-oss-process=image/resize,w_1400/format,webp" alt="image" style="zoom: 67%;">
<ul>
<li><strong>非结构化剪枝（Unstructured Pruning）</strong>：直接删除模型中的某些参数，通常基于参数的绝对值大小。这种方法可以实现较高的压缩比，但可能会破坏模型的整体结构。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 应用权重剪枝</span><br><span class="line">def apply_pruning(model, amount=0.2):</span><br><span class="line">    for name, module in model.named_modules():</span><br><span class="line">        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):</span><br><span class="line">            prune.l1_unstructured(module, name=&#x27;weight&#x27;, amount=amount)</span><br><span class="line">            print(f&quot;Applied pruning on &#123;name&#125;&quot;)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>结构化剪枝（Structured Pruning）</strong>：删除模型中的特定结构单元，如滤波器、通道或层。这种方法不会破坏模型的整体结构，更适合硬件加速。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">threshold = 0.01</span><br><span class="line">for name, module in model.named_modules():</span><br><span class="line">    if isinstance(module, nn.Conv2d):</span><br><span class="line">        # 计算每个卷积核的L2范数</span><br><span class="line">        kernel_norms = torch.norm(module.weight, dim=(1, 2, 3))</span><br><span class="line">        # 找到小于阈值的卷积核索引</span><br><span class="line">        prune_indices = torch.where(kernel_norms &lt; threshold)[0]</span><br><span class="line">        # 将这些卷积核的权重置零</span><br><span class="line">        module.weight[prune_indices] = 0</span><br></pre></td></tr></table></figure>
<ul>
<li>
<h4 id="基于梯度的剪枝"><a class="markdownIt-Anchor" href="#基于梯度的剪枝"></a> <strong>基于梯度的剪枝</strong></h4>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line">def gradient_magnitude_pruning(weight, gradient, percentile=0.5):</span><br><span class="line">    &quot;&quot;&quot;基于梯度幅值剪枝权重&quot;&quot;&quot;</span><br><span class="line">    num_zeros = round(weight.numel() * percentile)  # 计算剪枝元素数量</span><br><span class="line">    threshold = gradient.abs().view(-1).kthvalue(num_zeros).values  # 计算剪枝阈值</span><br><span class="line">    mask = gradient.abs() &gt; threshold  # 生成掩码</span><br><span class="line">    weight.mul_(mask.to(weight.device))  # 应用掩码剪枝</span><br><span class="line">    return weight</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="注意力迁移"><a class="markdownIt-Anchor" href="#注意力迁移"></a> 注意力迁移</h1>
<p><a target="_blank" rel="noopener" href="https://github.com/szagoruyko/attention-transfer?tab=readme-ov-file">https://github.com/szagoruyko/attention-transfer?tab=readme-ov-file</a></p>
<h2 id="定义-2"><a class="markdownIt-Anchor" href="#定义-2"></a> 定义</h2>
<ul>
<li>注意力迁移的思想来源于知识蒸馏，但与<strong>传统知识蒸馏主要关注最后层的知识不同，注意力迁移关注训练过程中特征图中的知识</strong>。</li>
<li>其目的是通过将教师网络的注意力图迁移到学生网络，提升学生网络的性能，同时实现模型的轻量化。</li>
</ul>
<h2 id="注意力机制"><a class="markdownIt-Anchor" href="#注意力机制"></a> 注意力机制</h2>
<ul>
<li><strong>空间域（Spatial Domain）</strong>：关注特征空间信息，决定空间中哪些区域重要。例如，通过动态注意力机制来选择性地关注图像中的特定区域。</li>
<li><strong>通道域（Channel Domain）</strong>：关注通道信息，如Squeeze-and-Excitation Networks（SENet）。SENet通过全局平均池化、降维再升维的方式为通道分配权重，增强重要通道的特征。</li>
<li><strong>混合域（Mixed Domain）</strong>：同时关注空间域和通道域，如CBAM等注意力机制，综合考虑特征空间和通道信息来生成注意力图。</li>
</ul>
<h2 id="算法部分"><a class="markdownIt-Anchor" href="#算法部分"></a> 算法部分</h2>
<ul>
<li><strong>基于激活的注意力迁移（Activation-based Attention Transfer）</strong>：
<ul>
<li>在前馈过程中，通过教师网络的激活特征图来引导学生网络的学习。</li>
<li>教师网络的激活特征图反映了输入数据在不同区域的重要性，<strong>学生网络通过模仿这些激活特征图</strong>来学习关注重要的区域。</li>
</ul>
</li>
<li><strong>基于梯度的注意力迁移（Gradient-based Attention Transfer）</strong>：
<ul>
<li>在反馈过程中，对教师网络和学生网络的交叉熵损失函数分别求梯度，<strong>将教师网络的梯度作为注意力图转移到学生网络</strong>。</li>
<li>关注那些对输出影响大的区域，通过构造损失函数，使得学生网络的梯度注意力图与教师网络的梯度注意力图接近，从而实现知识的迁移。</li>
</ul>
</li>
</ul>
<h1 id="低秩分解"><a class="markdownIt-Anchor" href="#低秩分解"></a> 低秩分解</h1>
<h1 id="轻量化网络结构"><a class="markdownIt-Anchor" href="#轻量化网络结构"></a> 轻量化网络结构</h1>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://shakewely.github.io/2025/02/05/2025%E5%B9%B42%E6%9C%885%E6%97%A5-%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" data-id="cm6rncay50000xgv42v0x1l8p" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E8%BD%BB%E9%87%8F%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E3%80%81model/" rel="tag">轻量化神经网络、model</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-2025年1月9日-语言模型" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2025/01/08/2025%E5%B9%B41%E6%9C%889%E6%97%A5-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" class="article-date">
  <time class="post-time" datetime="2025-01-07T16:22:05.000Z" itemprop="datePublished">
    <span class="post-month">1月</span><br/>
    <span class="post-day">08</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/01/08/2025%E5%B9%B41%E6%9C%889%E6%97%A5-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/">2025年1月9日 语言模型</a>
    </h1>
  

        <div>
          
          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ol>
<li>预训练 浅层特征不变 高层改变
<ol>
<li>冻结</li>
<li>fine-tuning</li>
</ol>
</li>
</ol>
<h2 id="统计语言模型"><a class="markdownIt-Anchor" href="#统计语言模型"></a> 统计语言模型</h2>
<p><img src="/2025/01/08/2025%E5%B9%B41%E6%9C%889%E6%97%A5-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/image-20250103124916040.png" alt="image-20250103124916040"></p>
<ol>
<li>马尔科夫链</li>
<li></li>
</ol>
<h2 id="神经网络语言模型"><a class="markdownIt-Anchor" href="#神经网络语言模型"></a> 神经网络语言模型</h2>
<p>word embedding 例子</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
<p>word embedding 就是 预训练的frozen</p>
<p><img src="/2025/01/08/2025%E5%B9%B41%E6%9C%889%E6%97%A5-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/image-20250108002222562.png" alt></p>
<p><img src="/2025/01/08/2025%E5%B9%B41%E6%9C%889%E6%97%A5-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/image-20250108002349457.png" alt="image-20250108002349457" style="zoom:50%;"><img src="/2025/01/08/2025%E5%B9%B41%E6%9C%889%E6%97%A5-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/image-20250108002355983.png" alt="image-20250108002355983"><img src="/2025/01/08/2025%E5%B9%B41%E6%9C%889%E6%97%A5-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/image-20250108002359696.png" alt="image-20250108002359696" style="zoom:50%;"><img src="/2025/01/08/2025%E5%B9%B41%E6%9C%889%E6%97%A5-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/image-20250108002402739.png" alt="image-20250108002402739" style="zoom:50%;"></p>
<h2 id="改进的注意力机制"><a class="markdownIt-Anchor" href="#改进的注意力机制"></a> 改进的注意力机制</h2>
<h3 id><a class="markdownIt-Anchor" href="#"></a> </h3>
<h3 id="1-lora-低秩近似在-transformer-中的应用"><a class="markdownIt-Anchor" href="#1-lora-低秩近似在-transformer-中的应用"></a> 1 LORA 低秩近似在 Transformer 中的应用</h3>
<p>在 Transformer 中，低秩近似主要应用于自注意力计算，特别是通过优化注意力矩阵的存储和计算来提高效率。以下是两种常见的实现方式：</p>
<ol>
<li>
<p><strong>Linformer</strong>：Linformer 是一种基于低秩近似的优化方法。Linformer 的核心思想是将标准自注意力中的注意力矩阵近似为低秩矩阵。Linformer 假设注意力矩阵可以用低秩矩阵来近似，因此将其从一个全连接矩阵（n×n）降为一个n×k  的矩阵（其中 kkk 是较小的值，通常比 nnn 小得多）。通过这种低秩近似，计算复杂度从</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1141079999999999em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p>
<p>降低为</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mo>⋅</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n \cdot k)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mclose">)</span></span></span></span></span></p>
<p>大大减少了计算量，特别是在长序列时效果显著。</p>
</li>
<li>
<p><strong>Performer</strong>：Performer 是另一种采用低秩近似的 Transformer 变种，它通过引入一种叫做 <strong>线性注意力</strong> 的方法，采用<strong>随机特征来近似注意力矩阵</strong>的乘积。这种方法能够将原本的</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1141079999999999em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p>
<p>计算复杂度降低为</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal">n</span><span class="mclose">)</span></span></span></span></span></p>
<p>，使得 Transformer 可以处理非常长的序列。</p>
</li>
</ol>
<h4 id="低秩近似的优点"><a class="markdownIt-Anchor" href="#低秩近似的优点"></a> 低秩近似的优点</h4>
<ul>
<li><strong>减少计算复杂度</strong>：通过将高秩矩阵分解为低秩矩阵，降低了计算复杂度，从而加快了训练和推理速度。</li>
<li><strong>节省内存</strong>：低秩矩阵的存储通常比原始矩阵所需的内存少，因此节省了计算过程中的内存消耗。</li>
<li><strong>适用于长序列</strong>：对于长序列，低秩近似可以大大提高 Transformer 的效率，使其能够处理更长的输入序列而不遇到内存瓶颈。</li>
</ul>
<h4 id="低秩近似的缺点"><a class="markdownIt-Anchor" href="#低秩近似的缺点"></a> 低秩近似的缺点</h4>
<ul>
<li><strong>性能损失</strong>：低秩近似虽然可以减少计算复杂度，但可能会带来一定的性能损失，尤其是在近似效果不佳时。较低的秩可能无法捕捉到原矩阵中的复杂关系，因此可能影响模型的精度。</li>
<li><strong>近似精度问题</strong>：低秩近似依赖于如何选择低秩矩阵的秩（rank）。如果选择的秩过小，可能会导致较大的误差，从而影响模型的效果。</li>
</ul>
<p>、</p>
<h3 id="lora-的应用步骤"><a class="markdownIt-Anchor" href="#lora-的应用步骤"></a> LoRA 的应用步骤</h3>
<h4 id="1-选择适当的层"><a class="markdownIt-Anchor" href="#1-选择适当的层"></a> 1. <strong>选择适当的层</strong></h4>
<ul>
<li>通常，LoRA 应用在 Transformer 中的关键层，如注意力层和前馈层。选择这些层是因为它们通常是最有影响力的层，影响模型的表达能力。</li>
</ul>
<h4 id="2-添加低秩矩阵"><a class="markdownIt-Anchor" href="#2-添加低秩矩阵"></a> 2. <strong>添加低秩矩阵</strong></h4>
<ul>
<li>对于每个需要修改的层（如注意力层中的权重矩阵），LoRA 在其上加上低秩矩阵。通常，这些低秩矩阵的秩较小，因此不会显著增加模型的复杂度。</li>
</ul>
<h4 id="3-训练低秩矩阵"><a class="markdownIt-Anchor" href="#3-训练低秩矩阵"></a> 3. <strong>训练低秩矩阵</strong></h4>
<ul>
<li>在微调过程中，仅训练低秩矩阵的参数，而保持原始的预训练权重不变。这意味着只有少量参数需要更新，从而加速训练并降低存储需求。</li>
</ul>
<h4 id="4-合成模型"><a class="markdownIt-Anchor" href="#4-合成模型"></a> 4. <strong>合成模型</strong></h4>
<ul>
<li>在微调结束后，最终的模型将包括原始的预训练权重和经过 LoRA 微调的低秩矩阵</li>
</ul>
<h3 id="代码实现"><a class="markdownIt-Anchor" href="#代码实现"></a> 代码实现</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="adapter"><a class="markdownIt-Anchor" href="#adapter"></a> Adapter</h2>
<p>适</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://shakewely.github.io/2025/01/08/2025%E5%B9%B41%E6%9C%889%E6%97%A5-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" data-id="cm6rncay70001xgv40x8a6tc6" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/llm/" rel="tag">llm</a></li></ul>

    </footer>
  </div>
  
</article>




  


  <nav id="page-nav">
    <a class="extend prev" rel="prev" href="/page/4/">&amp;laquo; pre</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><a class="page-number" href="/page/7/">7</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/">10</a><a class="extend next" rel="next" href="/page/6/">next &amp;raquo;</a>
  </nav>
</section>
        
          <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title"></h3>
  <div class="widget">
    <h1 class="blog-title">Weakliy_Blog</h1>
    <h2 class="blog-subtitle"></h2>
    <ul class="blog-link">
     
          <a href="/" title="Home">
            <li>主页</li>
          </a>
        
          <a href="/archives" title="Archives">
            <li>归档</li>
          </a>
        
          <a href="/categories" title="Categories">
            <li>分类</li>
          </a>
        
          <a href="/tags" title="Tags">
            <li>标签</li>
          </a>
        
    </ul>
  </div>
</div>

  
    <div class="widget-wrap">
  <h3 class="widget-title"></h3>
  <div class="widget">
    <img class="avatar" src="https://raw.githubusercontent.com/ShakeWeLy/Weakliy.github.io/main/%E5%A4%B4%E5%83%8F/mmexport1683194148817.png">
    <h2 class="author">Weakliy</h2>
    <h3 class="description"></h3>
    <div class="count-box">
      <a href="/archives"><div><strong>98</strong><br>文章</div></a>
      <a href="/categories"><div><strong>0</strong><br>分类</div></a>
      <a href="/tags"><div><strong>30</strong><br>标签</div></a>
    </div>



    <div class="social-link">
      
        <a class="hvr-bounce-in" href="https://github.com/ShakeWeLy" target="_blank" title="Github">
          Github
        </a>
      
    </div>

    <div class="friend-link">
      <h2>联系我</h2>
      
        <a class="hvr-bounce-in" href="https://github.com/ShakeWeLy" target="_blank" title="ShanaMaid">
          ShanaMaid
        </a>
      
    </div>
  </div>
</div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy;2024 - 2025 Weakliy<br>
      由<a href="http://hexo.io/" target="_blank">Hexo</a>强力驱动 | 
      主题-<a target="_blank" rel="noopener" href="https://github.com/ShanaMaid/hexo-theme-shana">Shana</a>
      
    </div>
    
  </div>
</footer>
    </div>
    

<script src="//apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"></script>
<script src="//apps.bdimg.com/libs/wow/0.1.6/wow.min.js"></script>
<script>
new WOW().init();
</script>   


  
<link rel="stylesheet" href="/plugin/fancybox/jquery.fancybox.css">

  
<script src="/plugin/fancybox/jquery.fancybox.pack.js"></script>




  
<link rel="stylesheet" href="/plugin/galmenu/GalMenu.css">

  
<script src="/plugin/galmenu/GalMenu.js"></script>

  <div class="GalMenu GalDropDown">
      <div class="circle" id="gal">
        <div class="ring">
          
            <a href="/" title="" class="menuItem">首页</a>
          
            <a href="/tags" title="" class="menuItem">标签</a>
          
            <a href="/categories" title="" class="menuItem">分类</a>
          
            <a href="/archives" title="" class="menuItem">总览</a>
          
            <a href="/xxxxxxxxx" title="" class="menuItem">xxx</a>
          
            <a href="/xxxxxxx" title="" class="menuItem">xxxx</a>
          
        </div>
        
          <audio id="audio" src="#"></audio>
        
      </div> 
</div>
<div id="overlay" style="opacity: 1; cursor: pointer;"></div>
  <script type="text/javascript">var items = document.querySelectorAll('.menuItem');
    for (var i = 0,
    l = items.length; i < l; i++) {
      items[i].style.left = (50 - 35 * Math.cos( - 0.5 * Math.PI - 2 * (1 / l) * i * Math.PI)).toFixed(4) + "%";
      items[i].style.top = (50 + 35 * Math.sin( - 0.5 * Math.PI - 2 * (1 / l) * i * Math.PI)).toFixed(4) + "%"
    }</script>
<script type="text/javascript">
  $(document).ready(function() {
    $('body').GalMenu({
      'menu': 'GalDropDown'
    })
  });
</script>

  <section class="hidden-xs"> 
  <ul class="cb-slideshow"> 
    <li><span>苟利</span></li> 
    <li><span>国家</span></li> 
    <li><span>生死以</span></li> 
    <li><span>岂能</span></li> 
    <li><span>祸福</span></li> 
    <li><span>趋避之</span></li> 
  </ul>
</section>

<script src="/js/script.js"></script>




  </div>
</body>
</html>